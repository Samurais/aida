{"data":{"allFile":{"edges":[{"node":{"childMarkdownRemark":{"html":"<p align=\"center\">\n  <a href=\"https://aida.dor.ai\">\n    \n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; ; max-width: 180px; margin-left: auto; margin-right: auto;\">\n    <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 100%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAABjqAAAY6gG/LxVHAAACXElEQVQ4y52UTYsTQRCGJ5kFF0RPiuLdu0dhD172KEyymxVRf8CCK+hFEfEDxQ/0FwgGvU12N+tHBEHBqyCov8Cj4M2TmO6ZJONT09Wd3hWDa+BNVXdXv/1WdU0nyYxfq2e9bWS5bfi5Vm5nbSqSrGcCgYMJm1hr+Fjvy5ogyw0IcTVXrEKIUtDEb6qdU4ITYFHj5tyai8vqPVP121KbkfpxsPgv5YkVzmc9ex0lz0jlCeMu6XRR8hh/HWyp323lNYgxT7GP2HfQ17pOUwdnT7+rqs6rqloZYAeRj+28xH+t/sDNLzN35n1Vsfea44CLE5pKeEECsEMWCuy43WecmxF+CbCmbG9OJGaiMcOO2/PA1drUCvcwcUAI5fR683op9if4RNqW25OxEtkv4Ed7YyTjQpRi7zoOs1dYjzB4C66IQupRLL+QIPMZsn3Mf1dVlSo9xJ7nHUdkpAzYO8QO2HNM+mc/Exm4pAqtI7QftRTflvpVTEjb2FyJhmpF4UkIDydRD52XUzmpWNqS2tmvzK249MaeUOp5DnzQQ42mfF8vJYlveS3UUGpFHd248LUT1HPtzbGfK5Xwnn4tqfvbRmhKH4xv9IASxWUmQJVchsYNVcTDadv0bBqnrMEjuSDfi3+DxEjvSodECgPhmutDM9be6vPFXEXZLfybO8G+G6zfxl8lZj58dUFhbldPvamJJqR2MdnlL/qWQw0vg19gQeebEKeokNfkT8iae2nS8ITlNonbZoGgo/6J8kHTN3InTPD1s4tU5tNH1JcgnLjr3PPwWjemao1cVvI/v9/U/81QXj2DwAAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;\">\n      <img class=\"gatsby-resp-image-image\" style=\"width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;\" alt=\"Aida\" title=\"\" src=\"/static/icon-2c3e064d27ddf667cdc016d5c5c99793-180a2.png\" srcset=\"/static/icon-2c3e064d27ddf667cdc016d5c5c99793-bd62e.png 148w,\n/static/icon-2c3e064d27ddf667cdc016d5c5c99793-180a2.png 180w\" sizes=\"(max-width: 180px) 100vw, 180px\">\n    </span>\n  </span>\n  \n  </a>\n</p>\n<h1 align=\"center\">\n  Aida\n</h1>\n<p align=\"center\">\n  <a href=\"https://aida.dor.ai\" title=\"License\">\n    <img alt=\"License\" src=\"https://img.shields.io/github/license/rodrigopivi/aida.svg\" width=\"100\">\n  </a>\n  <br>\n  <strong>Your application can understand natural language in house.</strong><br>\n  Use open source AI models that can train from the browser using javascript or python and can run everywhere.\n</p>\n<h3 align=\"center\">\n  <a href=\"https://aida.dor.ai\">\n    Try it online!\n  </a>\n</h3>\n<hr>\n<br>\n<p align=\"center\">\n<a href=\"https://www.patreon.com/bePatron?u=13643440\" title=\"Become a Patron!\">\n<img alt=\"Become a Patron!\" src=\"https://c5.patreon.com/external/logo/become_a_patron_button.png\" width=\"170\">\n</a>\n<br><br>\nDesigning and maintaining chatito takes time and effort, if it was usefull for you, please consider making a donation and share the abundance! :)\n</p>\n<hr />\n<br/>\n<h2>Setup and training</h2>\n<ol>\n<li>\n<p>Clone the GH proejct and install dependencies:</p>\n<ul>\n<li>Run <code>npm install</code> from the <code>./typescript</code> directory</li>\n<li>Run <code>pip3 install -r requirements.txt</code> from the <code>./python</code> directory</li>\n</ul>\n</li>\n<li>\n<p>Edit or create the chatito files inside <code>./typescript/examples/en/intents</code> to customize the dataset generation as you need.</p>\n</li>\n<li>\n<p>From <code>./typescript</code> run <code>npm run dataset:en:process</code>. This will generate many files at the <code>./typescript/public/models</code> directory. The dataset, the dataset parameters, the testing parameters and the embeddings dictionary. You can further inspect those generated files to make sence of their content. (Note: Aida also supports spanish language, if you need other language you can add if you first download the fastText embeddings for that language).</p>\n</li>\n<li>\n<p>You can start training from 3 local environments:</p>\n<ul>\n<li>\n<p>From python: just open <code>./python/main.ipynb</code> with jupyter notebook or jupyter lab. Python will load your custom settings generated at step 3. After running the notebook, convert the models to tensorflow.js web format running <code>npm run python:convert</code> from the <code>./typescript</code> directory.</p>\n</li>\n<li>\n<p>From web browsers: from <code>./typescript</code> run <code>npm run web:start</code>. Then navigate to <code>http://localhost:8000/train</code> for the training web UI. After training, downloading the model to the <code>./typescript/public/pretrained/web</code> directory.</p>\n</li>\n<li>\n<p>From node.js: from <code>./typescript</code> run <code>npm run node:start</code>.</p>\n</li>\n</ul>\n<p>NOTE: After training (and converting for python), the models should be available at <code>./typescript/public/pretrained</code> with a custom directory for each platform:</p>\n</li>\n</ol>\n<h2>Technical overview</h2>\n<p>Aida's NLP pipeline is composed of 3 models. The <code>Embeddings model</code> takes text sentences and encodes them to high dimensional vector representations. The <code>text classification model</code> takes the encoded sentences and predicts the intent of the sentence. And finally the <code>Named entity recognition model</code> takes the sentence embeddings by bigrams and the text classification output (the intent tags), and returns the sentence slots. Here is a more detailed description of the models.</p>\n<h3>Embeddings model</h3>\n<p>Plot:</p>\n<p>\n  <span\n    class=\"gatsby-resp-image-wrapper\"\n    style=\"position: relative; display: block; ; max-width: 590px; margin-left: auto; margin-right: auto;\"\n  >\n    <span\n      class=\"gatsby-resp-image-background-image\"\n      style=\"padding-bottom: 36.31578947368421%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAAAsSAAALEgHS3X78AAAA6klEQVQoz4VQ2QqEMBDr/3+bbyoq+OAt3veJdNYMtizLLltIJ02PSSroz9j3ndq2pa7rmP8Y8qmXWJaFtm2jdV25vgPaeZ76QXBo8zzTOI5cp2miYRjkDawvkWUZFUVBcRxzxVpxpUNLkoR5FEUUBAEZhkFhGJJlWeS6rrRtm3zfvwS6fLpTa0QE+r5nR+BqH/eQ7jkrH9eXQPDjOOixrIHDisNpWZZ8uWka/oK6rvkbwO99iUb8h5jRPU1THRPABQB6nues4THTNMnzPI7uOA5iokqk0A+iM4RvgHNV4QhO0Ui5RJOqqrTDF7GuFceDrjjuAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n    >\n      <img\n        class=\"gatsby-resp-image-image\"\n        style=\"width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;\"\n        alt=\"Alt text\"\n        title=\"embeddings model\"\n        src=\"/static/embedding-81e08416e944a31c4b4d00c9aa998734-fb8a0.png\"\n        srcset=\"/static/embedding-81e08416e944a31c4b4d00c9aa998734-1a291.png 148w,\n/static/embedding-81e08416e944a31c4b4d00c9aa998734-2bc4a.png 295w,\n/static/embedding-81e08416e944a31c4b4d00c9aa998734-fb8a0.png 590w,\n/static/embedding-81e08416e944a31c4b4d00c9aa998734-c3143.png 760w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n      />\n    </span>\n  </span>\n  </p>\n<p>The embeddings model uses a pre-trained fastText dictionary of bigrams to form word representations. For text classification, the embeddings model first takes a sentence, then breaks the sentence into words and then splits the words by bigrams. Here is an example of this process (sentence => words => word bigrams):</p>\n<pre><code>\"hi friend\" => [\"hi\", \"friend\"] => [[\"hi\"],[\"fr\", \"ri\", \"ie\", \"en\", \"nd\"]]\n</code></pre>\n<p> Then each word bigram is replaced by its 300 dimensional representation provided by fastText. Then we obtain a single 300 dimensional vector for each word of the sentence by the sum and average of the bigrams.</p>\n<p>The dimension each sentence tensor is: <code>21 (length in words of the dataset longest sentence)</code> by <code>300 (embedding dimensions)</code>. Here is a visualization of the embeddings tensor given this sentence: <code>please remind to me watch real madrid match tomorrow at 9pm</code></p>\n<p>\n  <span\n    class=\"gatsby-resp-image-wrapper\"\n    style=\"position: relative; display: block; ; max-width: 432px; margin-left: auto; margin-right: auto;\"\n  >\n    <span\n      class=\"gatsby-resp-image-background-image\"\n      style=\"padding-bottom: 66.66666666666666%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsSAAALEgHS3X78AAABAUlEQVQ4y9WTu0oDQRSG806WEtYUvlEKw27ew8IiiWzwgoW3oJggBBujiYEFLxCDVnYWFqJz+zy7cV2NIgpb6MDP+eafmcOZM0yBnEfhfyR0zuWiJGEKeYw411uFSim0NiKNEVmTRWcnnK4pnc1TL6vQOqxAd3GXpZkyy16VpuezOlcRBayVfDbnhSXWihXCUkDNCwiLvuyrUp/1aXgLPNzdZz18Norx4IJxa8Dldo/hepebg4iTZofzrWOudk7phW1u2xH9lUOijSOu94aJN2r1Ge2foR6fftdD9wPvQw+TqxuDM1bYTli8eG5FxOv6C++VP73y+zjN33nT5//+T3kB54XmTOgtRBcAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n    >\n      <img\n        class=\"gatsby-resp-image-image\"\n        style=\"width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;\"\n        alt=\"Alt text\"\n        title=\"embedded senttence\"\n        src=\"/static/embedded_sentence-8fc133017578e38faf938e8162e13241-bd193.png\"\n        srcset=\"/static/embedded_sentence-8fc133017578e38faf938e8162e13241-4b199.png 148w,\n/static/embedded_sentence-8fc133017578e38faf938e8162e13241-d3891.png 295w,\n/static/embedded_sentence-8fc133017578e38faf938e8162e13241-bd193.png 432w\"\n        sizes=\"(max-width: 432px) 100vw, 432px\"\n      />\n    </span>\n  </span>\n  </p>\n<h3>Text classification model</h3>\n<p>Plot:</p>\n<p>\n  <span\n    class=\"gatsby-resp-image-wrapper\"\n    style=\"position: relative; display: block; ; max-width: 590px; margin-left: auto; margin-right: auto;\"\n  >\n    <span\n      class=\"gatsby-resp-image-background-image\"\n      style=\"padding-bottom: 53.45830639948287%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsSAAALEgHS3X78AAABR0lEQVQoz32TDa+CMAxF+f9/kkTlQwb4AQIqW+1pmA+MzyULo7Wn93aYiIj/tpum8XVd+7ZtfVEUfrfb+TRN/fl89v/UBF1VMo6jdF0n1+tVFCBaLAqT2+0ml8tFFGAx55ztYRgsT402s1hVVTLPszLFJZoIWhg0GMqyDMfj0d61kT1Pp1PI89zyCgrTNFmcOm30ziHs+Xy6BAVsumhS9vu9nVFBXIH2jsosy9454uRVoUGpVUEukWV57xmCWaGAHW0yClUuh8OBIhvHajG7sJxdwlltSN/31pVZMjuKAAGIKok9Hg+sRZIssC0wQpCNLVTFi8IqUIC4WMl6A6NCffxZjovbQjEwbpEmbBqsQfG8qNsqZN3vd7PEZ4HCqIrZAf6wuZnhVyDXjm2UAMQqM+RMbqXoN3D9AwYOFGX6r7BPCOWfsF/AFwfOUYc0YAgGAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n    >\n      <img\n        class=\"gatsby-resp-image-image\"\n        style=\"width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;\"\n        alt=\"Alt text\"\n        title=\"text classification model\"\n        src=\"/static/classification-eec884040142f3a5e0d0a495df6a6d36-fb8a0.png\"\n        srcset=\"/static/classification-eec884040142f3a5e0d0a495df6a6d36-1a291.png 148w,\n/static/classification-eec884040142f3a5e0d0a495df6a6d36-2bc4a.png 295w,\n/static/classification-eec884040142f3a5e0d0a495df6a6d36-fb8a0.png 590w,\n/static/classification-eec884040142f3a5e0d0a495df6a6d36-526de.png 885w,\n/static/classification-eec884040142f3a5e0d0a495df6a6d36-fa2eb.png 1180w,\n/static/classification-eec884040142f3a5e0d0a495df6a6d36-85560.png 1547w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n      />\n    </span>\n  </span>\n  </p>\n<p>The text classification model is composed of 3 CNN layers concated and with a skip-layer connection and a dense layer output. The input is the output of the embeddings model for a given sentence using bigram embeddings and the ouput is the list of probabilities this sentence belongs to each of the classification classes.</p>\n<p>Here is an animation of the 3 convolutional layers activations during training, we see the convolutional layer filters learn features from the 300 dimensional vector representations and also the output layer visualization. We pass the same sentence (<code>please remind to me watch real madrid match tomorrow at 9pm</code>) to the model at the end of each training batch (~50 batches) and plot the activations to see how they evolve as the model learns to classify. (note: frame 0 is empty for 1 second, and the last frame also pauses for 1 second).</p>\n<p>First convolutional layer activations animation:</p>\n<p><img src=\"/classConv1-522769f921e260822996b3fab30224ce.gif\" alt=\"text classification Conv 1\" title=\"text classification Conv 1\"></p>\n<p>Second convolutional layer activations animation:</p>\n<p><img src=\"/classConv2-63bd4a41e761615d1331f1ae45715290.gif\" alt=\"text classification Conv 2\" title=\"text classification Conv 2\"></p>\n<p>Third convolutional layer activations animation:</p>\n<p><img src=\"/classConv3-2f26c63ff26438a79fb7b76c35ee90f0.gif\" alt=\"text classification Conv 3\" title=\"text classification Conv 3\"></p>\n<p>Output layer visualization:</p>\n<p><img src=\"/classOutput-5573009cebc47d93e16667fd5e8938cf.gif\" alt=\"text classification output\" title=\"text classification output\"></p>\n<p>NOTE: The model for text classification performs good and fast enough so there was no need to add a simple self-attention mechanism but it can be explored.</p>\n<h3>Named entity recognition model</h3>\n<p>Plot:</p>\n<p>\n  <span\n    class=\"gatsby-resp-image-wrapper\"\n    style=\"position: relative; display: block; ; max-width: 590px; margin-left: auto; margin-right: auto;\"\n  >\n    <span\n      class=\"gatsby-resp-image-background-image\"\n      style=\"padding-bottom: 90.48140043763676%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAASCAYAAABb0P4QAAAACXBIWXMAAAsSAAALEgHS3X78AAACCklEQVQ4y4WTh67sMAhE/f8fuEXbe++92FwO0qyy0j69SAQc28MwkGRm+Xw+5+FwmMfjcZ5Op3kymUTcbrdzp9PJ9Xo9N5vN3O12M08p5Z+WHNDe77ddr1e73+92u93scDjEerVa2Xw+t+VyGfF+v7f/PWm325XT6VScZTkej2WxWBRnW9brdXGQ8H6uPJ/P2PdkH+Ouzs1ms8BJHkR23zAv1bxM6/V64b1EY//1egVr9j1hMIa5y2MuSZxrtVrmCSy5JOa1B13KBdjZRIwEipGFczr7eDxCAnz1TPLyIjNM+v2+ufjWaDTMGxEx37kEG4w1d7xpcb5WqwVD7oxGI0vKADqx62Cup202G9tut9Ec1y4YsIcRI4MaeLlc4g7rVC0VAC7jWcMQFjCAHSxgxxoPWzRFtk+XeaGVGKELMYCUhScJTGmGmggjjD3Y8qBvksBqwi8/GAwiGdJIDoyYM8j1BYgOjAH0NRZ42PAdQMYI0SmVmHGhAtYQ+gBCl8x8lIkJpVEqomOwghGacU+mUQpANOKidIMdHmYMq0aJRjDwAP56BJrIjLAaCWKZ/iAk4S8gruolkOr6MzYAAK45pAmwQj+A0BPG/AQaGaphnL40JACEEtU9mNEUyoUZRkLOAEhzqiP2BVilrUdjhKdcWOJlKvunhtX69RFGYgsb/mvKpDk0Bml+3cP/AbqvXnyZk5xcAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n    >\n      <img\n        class=\"gatsby-resp-image-image\"\n        style=\"width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;\"\n        alt=\"Alt text\"\n        title=\"ner model\"\n        src=\"/static/ner-6d01e2d635e50c618fa5a1248e3d1ccf-fb8a0.png\"\n        srcset=\"/static/ner-6d01e2d635e50c618fa5a1248e3d1ccf-1a291.png 148w,\n/static/ner-6d01e2d635e50c618fa5a1248e3d1ccf-2bc4a.png 295w,\n/static/ner-6d01e2d635e50c618fa5a1248e3d1ccf-fb8a0.png 590w,\n/static/ner-6d01e2d635e50c618fa5a1248e3d1ccf-526de.png 885w,\n/static/ner-6d01e2d635e50c618fa5a1248e3d1ccf-24d09.png 914w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n      />\n    </span>\n  </span>\n  </p>\n<p>The named entity recognition (NER) model uses 2 inputs. The sentence embeddings at the word bigrams level and the one hot encoded classification tag for the sentence (the text classification model output).</p>\n<p>The training data is one-hot encoded with inside-outside (IO) tagging format. The model architecture uses a CNN at the sentence-word-bigram level concatenated with the classification tags repeated to match the length of words, then a bidirectional LSTM with no merge mode, with optional time-series attention mechanism applied only to the forward lstm, and a final dense output layer.</p>\n<p>Given a sentence, the model will return its tags and the class of the tag. e.g.:</p>\n<pre><code class=\"language-json\">{\n  \"sentence\": \"please remind to me watch real madrid match tomorrow at 9pm\",\n  \"slots\": {\n    \"dateTime\": [{ \"value\": \"tomorrow at 9am\" }],\n    \"calendarEvent\": [{ \"value\": \"real madrid match\" } ]\n  }\n}\n</code></pre>\n<p>Here is an animation of the 2 deep convolutional layers activations during training, we see the convolutional layer filters learn features from the 300 dimensional vector representations and also the output layer visualization. We pass the same sentence (<code>please remind to me watch real madrid match tomorrow at 9pm</code>) to the model at the end of each training batch (~50 batches) and plot the activations to see how they evolve as the model learns to perform NER. (note: frame 0 is empty for 1 second, and the last frame also pauses for 1 second).</p>\n<p>First convolutional layer activations animation:</p>\n<p><img src=\"/nerConv1-1c57da82951a5b224f44572677d36674.gif\" alt=\"NER Conv 1\" title=\"NER Conv 1\"></p>\n<p>Second convolutional layer activations animation:</p>\n<p><img src=\"/nerConv2-5938b9ed8a3d3c6a409d7105ebb7563f.gif\" alt=\"NER Conv 2\" title=\"NER Conv 2\"></p>\n<p>Output layer visualization:</p>\n<p><img src=\"/nerOutput-b7eb4295c6861f0e53cfbb03c7fe75a4.gif\" alt=\"NER output\" title=\"NER output\"></p>\n<h2>Visualization code snippets for python</h2>\n<p>There is code at <code>classification.py</code> and <code>ner.py</code> marked inside <code># === Visualization code block ===</code> comments that can be uncommented to generate images and then gif's of the activation progress for a given phrase. Also at the jupyter notebook, there is code for plotting the model diagrams.</p>\n<h1>Resources</h1>\n<ul>\n<li>\n<p><a href=\"https://arxiv.org/abs/1707.05928\">Deep Active Learning for Named Entity Recognition</a> - NER model architecture based on CNN word level, CNN char level, and LSTM + attention decoder</p>\n</li>\n<li>\n<p><a href=\"https://arxiv.org/abs/1511.08308\">Named Entity Recognition with Bidirectional LSTM-CNNs</a> - NER model architecture based on CNN word level, CNN Char level, extra features and a biLSTM decoder</p>\n</li>\n<li>\n<p><a href=\"https://arxiv.org/abs/1708.07120\">Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates</a> - Inspiring work on how to boost the learning speed of NN models.</p>\n</li>\n<li>\n<p><a href=\"https://github.com/twgr/thesis/blob/master/main.pdf\">Automatic Inference, Learning, and Design using Probabilistic Programming</a> - Inspiring work on probabilistic programming to simulate or generate datasets</p>\n</li>\n<li>\n<p><a href=\"https://www.safaribooksonline.com/library/view/tensorflow-solutions-for/9781788399180/\">Tensorflow Solutions for Text by Will Ballard</a> - Excellent course on NLP (goes in depth on how to use fastText trigrams and implements attention mechanisms)</p>\n</li>\n<li>\n<p><a href=\"https://fasttext.cc/\">fastText</a> - Pretrained embeddings based on ngram analysis for hundreds of languages</p>\n</li>\n<li>\n<p><a href=\"https://keras.io/\">Keras</a> - High-level neural networks API in Python with almost the same api as Tensorflow.js</p>\n</li>\n<li>\n<p><a href=\"https://js.tensorflow.org/\">Tensorflow.js</a> - High-level neural networks API in JavaScript with almost the same api as Keras</p>\n</li>\n<li>\n<p><a href=\"https://github.com/rodrigopivi/Chatito\">Chatito</a> - Dataset generation DSL for text classification and NER tasks</p>\n</li>\n</ul>\n<h3>TODO</h3>\n<ul>\n<li><a href=\"https://arxiv.org/abs/1801.06146\">Universal Language Model Fine-tuning for Text Classification</a> <a href=\"http://nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html\">(blog post)</a>- ULMFiT is the current state of the art for NLP tasks.</li>\n</ul>\n<h1>Author and maintainer</h1>\n<p>Rodrigo Pimentel</p>\n<h1>License</h1>\n<p>The code is open sourced under the BSD-3-Clause license, please contact me if you want to use the code under a less restrictive license.</p>\n<p>Copyright 2018 Rodrigo Pimentel</p>\n<p>Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:</p>\n<ol>\n<li>\n<p>Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.</p>\n</li>\n<li>\n<p>Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.</p>\n</li>\n<li>\n<p>Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.</p>\n</li>\n</ol>\n<p>THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p>"}}}]}},"pageContext":{}}