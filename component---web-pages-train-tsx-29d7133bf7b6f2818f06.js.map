{"version":3,"sources":["webpack:///./web/components/Logo.tsx","webpack:///./.cache/public-page-renderer.js","webpack:///./.cache/public-page-renderer-prod.js","webpack:///./web/components/Layout.tsx","webpack:///./.cache/gatsby-browser-entry.js","webpack:///./src/languages/en/EnglishTokenizer.ts","webpack:///./src/pipelines/zebraWings/pipeline.ts","webpack:///./src/languages/es/SpanishTokenizer.ts","webpack:///./src/pipelines/zebraWings/embeddings/EmbeddingsModel.ts","webpack:///./src/pipelines/zebraWings/embeddings/CombineNgramsLayer.ts","webpack:///./src/pipelines/zebraWings/embeddings/PreSavedEmbeddingsInitializer.ts","webpack:///./src/pipelines/zebraWings/models/classification.ts","webpack:///./src/pipelines/zebraWings/models/ner.ts","webpack:///./src/pipelines/zebraWings/TimeSeriesAttention.ts","webpack:///./web/components/TrainedPipelineTestInput.tsx","webpack:///./web/pages/train.tsx","webpack:///./web/components/Editor/Editor.tsx","webpack:///./web/components/Editor/editorConfig.ts","webpack:///./web/components/Editor/editorStyles.tsx","webpack:///./src/utils/dictionaryUtils.ts","webpack:///./web/components/TrainingDashboard.tsx","webpack:///./web/components/LineChart.tsx"],"names":["__webpack_require__","ColoredText","styled","div","withConfig","displayName","componentId","Logo","extend","_templateObject","m","module","exports","require","default","_react","_interopRequireDefault","_propTypes","_pageRenderer","_loader","ProdPageRenderer","_ref","location","pageResources","loader","getResourcesForPathnameSync","pathname","React","createElement","InternalPageRenderer","_objectSpread","json","propTypes","PropTypes","shape","string","isRequired","_gatsbyLink","_interopRequireWildcard","_styledComponents","_Logo","Content","Footer","overviewRouteRE","demoRouteRE","trainRouteRE","InnerContent","InnerPaddedContent","MainLayout","Component","defaultSelectedKeys","test","this","props","IC","addPadding","_layout","style","minHeight","flexDirection","Sider","width","breakpoint","collapsedWidth","theme","backgroundColor","textAlign","padding","className","to","_menu","mode","background","Item","key","_icon","type","href","title","fontSize","children","Error","_waitForRouteChange","_publicPageRenderer","_parsePath","StaticQueryContext","createContext","StaticQuery","Consumer","staticQueryData","data","query","render","object","func","zero","one","two","three","four","five","six","seven","eight","nine","sentence","trim","toLowerCase","replace","_this","FILTER_CHARS_REGEXP","sanitizeSentence","split","WORD_SEPARATORS_REGEXP","map","w","filter","word","grams","index","length","substr","words","join","_EnglishTokenizer","_SpanishTokenizer","_EmbeddingsModel","_classification","_ner","defaultPipelineDefinition","config","classification","epochs","filterSizes","lowConfidenceThreshold","numFilters","batchSize","drop","embeddingDimensions","lossThresholdToStopTraining","maxNgrams","trainingValidationSplit","ner","addAttention","rnnUnits","AidaPipeline","cfg","_classCallCheck","_defineProperty","_asyncToGenerator","regeneratorRuntime","mark","_callee","trainDataset","wrap","_context","prev","next","logger","log","classificationModel","train","Object","keys","datasetParams","slotsToId","nerModel","stop","_x","apply","arguments","_ref2","_callee2","testDataset","classificationStats","nerStats","_context2","sent","t0","correct","wrong","abrupt","_x2","sentences","predict","_ref3","_callee3","_context3","tfModel","save","classificationPath","nerPath","embeddingsModel","embeddingPath","_x3","pipelineDefinition","tokenizer","language","lang","englishTokenizer","spanishTokenizer","getTokenizer","EmbeddingsModel","ngramToIdDictionary","maxWordsPerSentence","pretrainedEmbedding","pretrainedNGramVectors","classificationTrainStatsHandler","nerTrainStatsHandler","classificationCfg","assign","trainStatsHandler","ClassificationModel","pretrainedClassifier","nerCfg","NerModel","pretrainedNer","cero","uno","dos","tres","cuatro","cinco","seis","siete","ocho","nueve","tf","_CombineNgramsLayer","_PreSavedEmbeddingsInitializer","maxWords","pretrainedEmbeddingModel","model","inputModel","input","layers","dtype","embedded","inputs","outputs","tidy","sentencesTensor","sentencesToWordIds","output","modelInput","predictOnBatch","dispose","sentencesSplittedByWords","s","splitSentenceToWords","buffer","forEach","sentenceIndex","wordIndex","undefined","set","generateWordIdsFromNGrams","gram","gramIndex","console","warn","toTensor","vecIds","addToVecsIfNotUndefined","k","push","allNgramsFound","splitWordToBiGrams","wt","setupModel","Map","sequential","embedLayer","embedding","embeddingsInitializer","PreSavedEmbeddingsInitializer","inputDim","size","inputLength","maskZero","outputDim","trainable","add","timeDistributed","layer","inputShape","CombineNgramsLayer","_losses","Layer","kwargs","_this2","invokeCallHook","combined","sum","l2Normalize","serialization","SerializationMap","register","initializers","_lodash","_getPrototypeOf","call","_assertThisInitialized","Initializer","flatMat","flatMapDeep","_toConsumableArray","values","tensor2d","zeros","types","pretrainedModel","prediction","embeddedSentences","embed","d","dataSync","intents","i","preds","slice","sentencePreds","p","idx","confidence","intent","sort","a","b","trainYChunks","trainXChunks","enoughAccuracyReached","_iteratorNormalCompletion","_didIteratorError","_iteratorError","_iterator","_step","_step$value","xChunk","dataLabels","hotEncodedLabels","h","c","chunk","trainY","trainX","entries","Symbol","iterator","done","_slicedToArray","value","tensor1d","oneHot","fit","callbacks","onBatchEnd","nextFrame","shuffle","validationSplit","history","val_loss","batch","batchEpochs","epoch","currentBatchSize","tensorsInMemory","memory","numTensors","totalBatches","trainingAccuracy","acc","trainingLoss","loss","validationAccuracy","val_acc","validationLoss","concat","return","finish","testExamples","resultsHandler","handler","stats","x","y","_iteratorNormalCompletion2","_didIteratorError2","_iteratorError2","_iterator2","_step2","_step2$value","predictions","defaultResultsLogger","lowConfidence","testX","testY","o","debug","error","setup","PipelineModel","numClasses","optimizer","adam","name","convLayer1","conv1d","activation","filters","kernelInitializer","kernelSize","maxpool1","maxPooling1d","poolSize","convLayer2","maxpool2","convLayer3","maxpool3","concatLayer","concatenate","axis","flat","flatten","dropOut","dropout","rate","flatPool1","concatForClassification","outClassification","dense","units","compile","metrics","_TimeSeriesAttention","classificationPred","_this$datasetParams","slotTypesLength","encodedIntent","intentEncoded","Array","fill","indexOf","intentsFlat","classLabel","flattenedPredictions","sp","wordTagPredictions","highestIndex","tp","ti","wordPredictionsChunk","rawPrediction","sentenceWordPredictionIds","sentenceWords","reduce","accumulator","currentIndex","current","currentSlotKey","find","slotKey","slots","trainY2Chunks","_this$config","slotsLength","_loop","_ret","trainY2","intentLabels","embeddedSentenceWords","slotTags","v","asType","y2sentences","wordsSlotId","slotIds","pad","ohe","err","stack","delegateYield","_typeof","t1","testY2","_iteratorNormalCompletion3","_didIteratorError3","_iteratorError3","_loop2","_iterator3","_step3","_context4","_step3$value","classifications","y2","sentenceIdx","expectedTags","predictedTags","tag","numSlotTypes","embeddedSentencesInput","classLabelInput","classLabelRepeated","repeatVector","n","concated","biLstm","bidirectional","lstm","returnSequences","finalHidden","timeAttention","TimeSeriesAttention","_topology","inputSpec","InputSpec","ndim","supportsMasking","dimensions","timed","build","trainableWeights","nonTrainableWeights","built","encoded","permuted","permute","dims","unstackedInput","unstack","unstackedPermuted","dotProds","ui","dot","selfAttend","attention","softmax","attentionPermuted","unstackedOutput","ua","SearchInput","_input","Search","TrainedPipelineTestInput","disabled","outTextContent","pipeline","setState","JSON","stringify","inputSearch","document","getElementById","_row","justify","_col","span","sm","_card","placeholder","enterButton","onSearch","handleSubmit","id","state","marginTop","_Editor","_Layout","_TrainingDashboard","AidaTrain","_this$props","chatitoFiles","allFiles","allFile","edges","edge","node","extension","fields","chatitoDSL","tabs","tdp","_axios","chatito","webAdapter","utils","_editorConfig","es","_dictionaryUtils","CodeFlask","ReactJson","window","Editor","warning","activeTabIndex","trainingDataset","testingDataset","showDrawer","generating","isDownloading","downloadProgress","createRef","debounce","codeInputValue","validation","getDSLValidation","newState","saveToLocalStorage","_progress","percent","marginBottom","marginLeft","_button","onClick","trainTestAndSaveModels","StrokeText","BlockWrapper","BlockWrapperTitle","src","iconStyle","enableClipboard","displayDataTypes","collapsed","t","onCloseTab","closerTab","TabButton","active","changeTab","filename","prompt","tabsContainer","scrollTo","left","scrollWidth","behavior","validChatitoFiles","validateChatitoFiles","e","setTimeout","generateDataset","alert","localStorage","setItem","parseAsJSON","item","getItem","parse","localTabs","loadFromLocalIfPresent","cb","codeflask","updateCode","setLineNumber","stopPropagation","confirm","ati","newActiveTabIndex","dsl","intentsWithoutLimit","astFromString","entity","args","constructor","toString","message","start","line","column","some","tab","_training","_testing","adapter","training","testing","mergeDeep","files","total","progress","downloads","Promise","all","file","axios","get","onDownloadProgress","progressEvent","totalLength","lengthComputable","target","getResponseHeader","Math","round","loaded","jsonFiles","_dictionariesFromData","_dictionariesFromData2","intentsWithSlots","trainingParams","testingParams","withPrefix","downloadFiles","dictionariesFromDataset","dictionary","embeddingsAndTrainingDatasetLoaded","datasetStats","loadFromLocalStorage","flask","lineNumbers","addLanguage","chatitoPrism","onUpdate","code","editorUpdatesSetupCount","debouncedTabDSLValidation","alertState","loading","EditorWrapper","EditorHeader","display","onAddFile","marginRight","onToggleDrawer","TabsArea","innerRef","renderTabButton","CodeStyles","AlertNotification","EditorOverlay","onCloseDrawer","Drawer","renderDatasetPreviewer","comments","pattern","greedy","intentDefinition","inside","intentArguments","slotDefinition","slotArguments","slot","alias","_ref4","ret","O","intentTrainingStats","intentTestingStats","processedTrainingSentences","processedTestingSentences","intentsWithSlotsSet","Set","containsSlots","getProcessedSentence","sentenceTokens","y2Tags","token","encodedSentenceFrag","slotName","internalKey","tagsForSentence","intentId","findIndex","intentProcessedSentences","intentTestingProcessedSentences","intentKey","dictJson","dictionaryCache","NGRAM_TO_ID_MAP","PRETRAINED","_pipeline","_LineChart","_TrainedPipelineTestInput","globalLog","LoggerFeed","TrainingDashboard","currentStep","logLinesCounter","pipelineFinishedTraining","plot","valuesClassification","valuesNer","Fragment","_alert","batchInfo","lineChart","dataValues","extra","trainStatsClassification","trainStatsNer","logHandler","disposeVariables","_steps","Step","renderPipelineManualTestInput","renderChart","AidaLineChart"],"mappings":"0FACA,8SADAA,EAAA,KAIO,IAAMC,MAHbD,EAAA,kCAG2BE,QAAOC,IAAVC,WAAA,CAAAC,YAAA,oBAAAC,YAAA,gBAAGJ,CAAH,kdAuBjB,IAAMK,EAAON,EAAYO,OAAfC,mCC3BjB,IAAsBC,EAKpBC,EAAOC,SALaF,EAKWG,EAAQ,OALRH,EAAEI,SAAYJ,sGCA/C,IAAAK,EAAAC,EAAAhB,EAAA,IACAiB,EAAAD,EAAAhB,EAAA,IAEAkB,EAAAF,EAAAhB,EAAA,KACAmB,EAAAH,EAAAhB,EAAA,+KAEA,IAAMoB,EAAmB,SAAAC,GAAkB,IAAfC,EAAeD,EAAfC,SACpBC,EAAgBC,UAAOC,4BAA4BH,EAASI,UAClE,OAAOC,UAAMC,cAAcC,4UAApBC,CAAA,CACLR,WACAC,iBACGA,EAAcQ,QAIrBX,EAAiBY,UAAY,CAC3BV,SAAUW,UAAUC,MAAM,CACxBR,SAAUO,UAAUE,OAAOC,aAC1BA,kBAGUhB,+bCrBfpB,EAAA,KAEA,IAAAqC,EAAArB,EAAAhB,EAAA,KACA2B,ySAAAW,CAAAtC,EAAA,IACAuC,EAAAvB,EAAAhB,EAAA,KACAwC,EAAAxC,EAAA,8nBAEQyC,oBAASC,mBAEXC,EAAkB,uBAClBC,EAAc,mBACdC,EAAe,oBAERC,GAAe,EAAAP,EAAAzB,SAAO2B,GAAVrC,WAAA,CAAAC,YAAA,uBAAAC,YAAA,gBAAG,CAAH,oFAQlB,IAAMyC,GAAqB,EAAAR,EAAAzB,SAAOgC,GAAV1C,WAAA,CAAAC,YAAA,6BAAAC,YAAA,gBAAG,CAAH,6DAIV0C,gaAAmBrB,EAAMsB,kDAEtC,IAAIC,EAAsB,KACtBP,EAAgBQ,KAAKC,KAAKC,MAAM/B,SAASI,UACzCwB,EAAsB,IACfL,EAAaM,KAAKC,KAAKC,MAAM/B,SAASI,UAC7CwB,EAAsB,IACfN,EAAYO,KAAKC,KAAKC,MAAM/B,SAASI,YAC5CwB,EAAsB,KAE1B,IAAMI,EAAKF,KAAKC,MAAME,WAAaR,EAAqBD,EACxD,OACInB,EAAAC,cAAA4B,EAAA1C,QAAA,CAAQ2C,MAAO,CAAEC,UAAW,UACxB/B,EAAAC,cAAA4B,EAAA1C,QAAA,CAAQ2C,MAAO,CAAEE,cAAe,QAC5BhC,EAAAC,cAAA4B,EAAA1C,QAAQ8C,MAAR,CAAcC,MAAO,IAAKC,WAAW,KAAKC,eAAe,IAAIC,MAAM,QAAQP,MAAO,CAAEQ,gBAAiB,YACjGtC,EAAAC,cAACY,EAAAjC,KAAD,CAAMkD,MAAO,CAAES,UAAW,SAAUL,MAAO,IAAKM,QAAS,IAAMC,UAAU,UACrEzC,EAAAC,cAACS,EAAAvB,QAAD,CAAMuD,GAAG,KAAT,cAEJ1C,EAAAC,cAAA0C,EAAAxD,QAAA,CAAMkD,MAAM,QAAQO,KAAK,SAASrB,oBAAqB,CAACA,GAAsBO,MAAO,CAAEe,WAAY,YAC/F7C,EAAAC,cAAA0C,EAAAxD,QAAM2D,KAAN,CAAWC,IAAI,KACX/C,EAAAC,cAACS,EAAAvB,QAAD,CAAMuD,GAAG,aACL1C,EAAAC,cAAA+C,EAAA7D,QAAA,CAAM8D,KAAK,mBADf,aAKJjD,EAAAC,cAAA0C,EAAAxD,QAAM2D,KAAN,CAAWC,IAAI,KACX/C,EAAAC,cAACS,EAAAvB,QAAD,CAAMuD,GAAG,UACL1C,EAAAC,cAAA+C,EAAA7D,QAAA,CAAM8D,KAAK,mBADf,oBAKJjD,EAAAC,cAAA0C,EAAAxD,QAAM2D,KAAN,CAAWC,IAAI,KACX/C,EAAAC,cAACS,EAAAvB,QAAD,CAAMuD,GAAG,SACL1C,EAAAC,cAAA+C,EAAA7D,QAAA,CAAM8D,KAAK,mBADf,UAMRjD,EAAAC,cAAA,OAAK6B,MAAO,CAAEU,QAAS,OAAQD,UAAW,WACtCvC,EAAAC,cAAA,KAAGiD,KAAK,sCAAsCC,MAAM,OAAOrB,MAAO,CAAEsB,SAAU,KAC1EpD,EAAAC,cAAA+C,EAAA7D,QAAA,CAAM8D,KAAK,cAIvBjD,EAAAC,cAAA4B,EAAA1C,QAAA,CAAQ2C,MAAO,CAAEU,QAAS,kBACtBxC,EAAAC,cAAC0B,EAAD,KAAKF,KAAKC,MAAM2B,UAChBrD,EAAAC,cAACc,EAAD,CAAQe,MAAO,CAAES,UAAW,WAA5B,iLChCxB,WACE,MAAM,IAAIe,MACR,ipCAzCJ,IAAAlE,EAAAC,EAAAhB,EAAA,IACAiB,EAAAD,EAAAhB,EAAA,IACAqC,ySAAAC,CAAAtC,EAAA,KAOAkF,EAAAlF,EAAA,IACAmF,EAAAnE,EAAAhB,EAAA,MACAoF,EAAApE,EAAAhB,EAAA,wDAEA,IAAMqF,EAAqB1D,UAAM2D,cAAc,2BAE/C,IAAMC,EAAc,SAAAlC,GAAK,OACvBtC,EAAAD,QAAAc,cAACyD,EAAmBG,SAApB,KACG,SAAAC,GACC,OACEpC,EAAMqC,MACLD,EAAgBpC,EAAMsC,QAAUF,EAAgBpC,EAAMsC,OAAOD,MAEtDrC,EAAMuC,QAAUvC,EAAM2B,UAC5B3B,EAAMqC,KAAOrC,EAAMqC,KAAKA,KAAOD,EAAgBpC,EAAMsC,OAAOD,MAGvD3E,EAAAD,QAAAc,cAAA,uDAMf2D,EAAYvD,UAAY,CACtB0D,KAAMzD,UAAU4D,OAChBF,MAAO1D,UAAUE,OAAOC,WACxBwD,OAAQ3D,UAAU6D,KAClBd,SAAU/C,UAAU6D,sdCoBP,+JAnDkB,gGAEG,kEAEC,0CAEN,2BAEqB,CAE5CC,KAAM,IACNC,IAAK,IACLC,IAAK,IACLC,MAAO,IACPC,KAAM,IACNC,KAAM,IACNC,IAAK,IACLC,MAAO,IACPC,MAAO,IACPC,KAAM,gCAIgB,SAACC,GACvB,OAAOA,EACFC,OACAC,cACAC,QAAQC,EAAKC,oBAAqB,oCAGb,SAACL,GAC3B,OAAOI,EAAKE,iBAAiBN,GACxBO,MAAMH,EAAKI,wBACXC,IAAI,SAAAC,GAAC,OAAIA,EAAET,SACXU,OAAO,SAAAD,GAAC,QAAMA,kCAGK,SAACE,GACzB,IACMC,EAAkB,GACpBC,EAAQF,EAAKG,OAFH,EAEoB,EAClC,GAAID,EAAQ,EACR,OAAOD,EAEX,KAAOC,KACHD,EAAMC,GAASF,EAAKI,OAAOF,EAPjB,GASd,OAAOD,iCAEkB,SAACI,GAAD,OAA6BA,EAAMC,KAAK,yOCrDzE,IAAAC,EAAA5G,EAAAhB,EAAA,MACA6H,EAAA7G,EAAAhB,EAAA,MAEA8H,EAAA9H,EAAA,KACA+H,EAAA/G,EAAAhB,EAAA,MACAgI,EAAAhH,EAAAhB,EAAA,2eAYO,IAAMiI,EAAuD,CAChEC,OAAQ,CACJC,eAAgB,CACZC,OAAQ,EACRC,YAAa,CAAC,EAAG,EAAG,GACpBC,uBAAwB,GACxBC,WAAY,KAEhBzH,QAAS,CAIL0H,UAAW,IACXC,KAAM,GACNC,oBAAqB,IACrBC,4BAA6B,KAC7BC,UAAW,GACXC,wBAAyB,IAE7BC,IAAK,CACDC,cAAc,EACdX,OAAQ,EACRE,uBAAwB,GACxBC,WAAY,CAAC,IAAK,KAClBS,SAAU,oDAalB,SAAAC,EAAYC,GAUT,IAAArC,EAAAzD,kGAAA+F,CAAA/F,KAAA6F,GAAAG,EAAAhG,KAAA,qBAlBqD6E,GAkBrDmB,EAAAhG,KAAA,wBAAAgG,EAAAhG,KAAA,0BAAAgG,EAAAhG,KAAA,8BAAAgG,EAAAhG,KAAA,mBAAAgG,EAAAhG,KAAA,iBAAAgG,EAAAhG,KAAA,oBAAAgG,EAAAhG,KAAA,SAAA/B,EAAAgI,EAAAC,mBAAAC,KA0CY,SAAAC,EAAOC,GAAP,OAAAH,mBAAAI,KAAA,SAAAC,GAAA,cAAAA,EAAAC,KAAAD,EAAAE,MAAA,cACXhD,EAAKiD,OAAOC,IAAI,mCAChBlD,EAAKiD,OAAOC,IAAI,sGAFLJ,EAAAE,KAAA,EAGLhD,EAAKmD,oBAAoBC,MAAMR,GAH1B,YAISS,OAAOC,KAAKtD,EAAKuD,cAAcC,WAAW7C,QAE3C,GANR,CAAAmC,EAAAE,KAAA,gBAAAF,EAAAE,KAAA,EAODhD,EAAKyD,SAASL,MAAMR,GAPnB,OAAAE,EAAAE,KAAA,iBASPhD,EAAKiD,OAAOC,IAAI,wCAChBlD,EAAKiD,OAAOC,IAAI,sGAVT,QAYXlD,EAAKiD,OAAOC,IAAI,sCAChBlD,EAAKiD,OAAOC,IAAI,sGAbL,yBAAAJ,EAAAY,SAAAf,EAAApG,SA1CZ,SAAAoH,GAAA,OAAAnJ,EAAAoJ,MAAArH,KAAAsH,cAAAtB,EAAAhG,KAAA,QAAAuH,EAAAtB,EAAAC,mBAAAC,KA0DW,SAAAqB,EAAOC,GAAP,IAAAC,EAAAC,EAAA,OAAAzB,mBAAAI,KAAA,SAAAsB,GAAA,cAAAA,EAAApB,KAAAoB,EAAAnB,MAAA,cACVhD,EAAKiD,OAAOC,IAAI,kCAChBlD,EAAKiD,OAAOC,IAAI,sGAFNiB,EAAAnB,KAAA,EAGwBhD,EAAKmD,oBAAoB7G,KAAK0H,GAHtD,UAGJC,EAHIE,EAAAC,OAKUf,OAAOC,KAAKtD,EAAKuD,cAAcC,WAAW7C,QAC9B,GANtB,CAAAwD,EAAAnB,KAAA,gBAAAmB,EAAAnB,KAAA,EAMgChD,EAAKyD,SAASnH,KAAK0H,GANnD,OAAAG,EAAAE,GAAAF,EAAAC,KAAAD,EAAAnB,KAAA,iBAAAmB,EAAAE,GAMkE,CAAEC,QAAS,EAAGC,MAAO,GANvF,eAMJL,EANIC,EAAAE,GAAAF,EAAAK,OAAA,SAOH,CAAEP,sBAAqBC,aAPpB,yBAAAC,EAAAT,SAAAK,EAAAxH,SA1DX,SAAAkI,GAAA,OAAAX,EAAAF,MAAArH,KAAAsH,cAAAtB,EAAAhG,KAAA,UAoEc,SAACmI,GACd,IAAMpD,EAAiBtB,EAAKmD,oBAAoBwB,QAAQD,GAIxD,MAAO,CAAEpD,iBAAgBW,IAFLoB,OAAOC,KAAKtD,EAAKuD,cAAcC,WAAW7C,QACb,EAAIX,EAAKyD,SAASkB,QAAQD,EAAWpD,GAAkB,MAxEzGiB,EAAAhG,KAAA,QAAAqI,EAAApC,EAAAC,mBAAAC,KA4EW,SAAAmC,EAAOxC,GAAP,OAAAI,mBAAAI,KAAA,SAAAiC,GAAA,cAAAA,EAAA/B,KAAA+B,EAAA9B,MAAA,cACVhD,EAAKiD,OAAOC,IAAI,2BAChBlD,EAAKiD,OAAOC,IAAI,sGAFN4B,EAAA9B,KAAA,EAGJhD,EAAKmD,oBAAoB4B,UAAUC,KAAK3C,EAAI4C,oBAHxC,YAIU5B,OAAOC,KAAKtD,EAAKuD,cAAcC,WAAW7C,QAE3C,GANT,CAAAmE,EAAA9B,KAAA,eAAA8B,EAAA9B,KAAA,EAOAhD,EAAKyD,SAASsB,UAAUC,KAAK3C,EAAI6C,SAPjC,cAAAJ,EAAA9B,KAAA,GASJhD,EAAKmF,gBAAgBJ,UAAUC,KAAK3C,EAAI+C,eATpC,yBAAAN,EAAApB,SAAAmB,EAAAtI,SA5EX,SAAA8I,GAAA,OAAAT,EAAAhB,MAAArH,KAAAsH,cACKxB,EAAIiD,qBACJ/I,KAAK+I,mBAAqBjD,EAAIiD,oBAElC/I,KAAKgH,cAAgBlB,EAAIkB,cACzBhH,KAAK0G,OAASZ,EAAIY,OAClB1G,KAAKgJ,UA/Db,SAAsBC,GAClB,IAAMC,EAAOD,EAAWA,EAAS1F,cAAgB0F,EACjD,GAAa,OAATC,EACA,OAAOC,UACJ,GAAa,OAATD,EACP,OAAOE,UAEX,MAAM,IAAIvH,MAAM,8DAwDKwH,CAAarJ,KAAKgH,cAAciC,UACjDjJ,KAAK4I,gBAAkB,IAAIU,kBACvBxD,EAAIyD,oBACJzD,EAAIkB,cAAcwC,oBAClBxJ,KAAK+I,mBAAmBjE,OAAOpH,QAAQ8H,UACvCxF,KAAK+I,mBAAmBjE,OAAOpH,QAAQ4H,oBACvCtF,KAAKgJ,UACLlD,EAAI2D,oBACJ3D,EAAI4D,wBAdT,IAAArB,EAAA,IAAAd,EAAA,IAAAtJ,EAgBC,IACI0L,EACAC,EAFEC,EAAoB/C,OAAOgD,OAAO,GAAI9J,KAAK+I,mBAAmBjE,OAAOpH,QAASsC,KAAK+I,mBAAmBjE,OAAOC,gBAG/Ge,EAAIiE,oBACJJ,EAAkC7D,EAAIiE,kBAAkBhF,eACxD6E,EAAuB9D,EAAIiE,kBAAkBrE,KAEjD1F,KAAK4G,oBAAsB,IAAIoD,UAC3BH,EACA7J,KAAKgH,cACLhH,KAAK4I,gBACL5I,KAAK0G,OACLZ,EAAImE,qBACJN,GAEJ,IAAMO,EAASpD,OAAOgD,OAAO,GAAI9J,KAAK+I,mBAAmBjE,OAAOpH,QAASsC,KAAK+I,mBAAmBjE,OAAOY,KACxG1F,KAAKkH,SAAW,IAAIiD,UAChBD,EACAlK,KAAKgH,cACLhH,KAAK4I,gBACL5I,KAAK0G,OACLZ,EAAIsE,cACJR,sOC7CG,+JArDkB,4JAEG,kEAEC,sGAEN,2BAEqB,CAE5CS,KAAM,IACNC,IAAK,IACLC,IAAK,IACLC,KAAM,IACNC,OAAQ,IACRC,MAAO,IACPC,KAAM,IACNC,MAAO,IACPC,KAAM,IACNC,MAAO,gCAIe,SAACzH,GACvB,OAAOA,EACFC,OACAC,cACAC,QAAQC,EAAKC,oBAAqB,oCAGb,SAACL,GAC3B,OAAOI,EAAKE,iBAAiBN,GACxBO,MAAMH,EAAKI,wBACXC,IAAI,SAAAC,GAAC,OAAIA,EAAET,SACXU,OAAO,SAAAD,GAAC,QAAMA,kCAGK,SAACE,GACzB,IACMC,EAAkB,GACpBC,EAAQF,EAAKG,OAFH,EAEoB,EAClC,GAAID,EAAQ,EACR,OAAOD,EAEX,KAAOC,KACHD,EAAMC,GAASF,EAAKI,OAAOF,EAPjB,GASd,OAAOD,iCAGkB,SAACI,GAAD,OAA6BA,EAAMC,KAAK,gICvDzE,IAAAwG,ySAAA7L,CAAAtC,EAAA,KAEAoO,EAAApO,EAAA,KACAqO,EAAArO,EAAA,uSAEa0M,uBAiCT,SAAAA,EACIC,EACA2B,EACA1F,EACAF,EACA0D,EACAmC,EACAzB,GACF,IAAAjG,EAAAzD,kGAAA+F,CAAA/F,KAAAsJ,GAAAtD,EAAAhG,KAAA,oBAAAgG,EAAAhG,KAAA,8BAAAgG,EAAAhG,KAAA,mBAAAgG,EAAAhG,KAAA,oBAAAgG,EAAAhG,KAAA,8BAAAgG,EAAAhG,KAAA,gBAAAgG,EAAAhG,KAAA,aAVoC,MAUpCgG,EAAAhG,KAAA,UAWe,kBAAMyD,EAAK2H,QAX1BpF,EAAAhG,KAAA,aAakB,WAChB,IAAKyD,EAAK4H,WAAY,CAClB,IAAMC,EAAQP,EAAGQ,OAAOD,MAAM,CAAExM,MAAO,CAAC2E,EAAKyH,SAAUzH,EAAK+B,WAAYgG,MAAO,UACzEC,EAAWhI,EAAK2H,MAAM/D,MAAMiE,GAClC7H,EAAK4H,WAAaN,EAAGK,MAAM,CAAEM,OAAQJ,EAAOK,QAASF,IAEzD,OAAOhI,EAAK4H,aAnBdrF,EAAAhG,KAAA,QAuBa,SAACmI,GACZ,OAAO4C,EAAGa,KAAK,WACX,IAAMC,EAAkBpI,EAAKqI,mBAAmB3D,GAC1C4D,EAAStI,EAAKuI,aAAaC,eAAeJ,GAEhD,OADAA,EAAgBK,UACTH,MA5Bb/F,EAAAhG,KAAA,aAgCkB,kBAAMyD,EAAK8F,sBAhC7BvD,EAAAhG,KAAA,qBAkC2B,SAACmI,GAC1B,OAAO4C,EAAGa,KAAK,WACX,IAAMO,EAA2BhE,EAAUrE,IAAI,SAAAsI,GAAC,OAAI3I,EAAKuF,UAAUqD,qBAAqBD,KAClFE,EAASvB,EAAGuB,OAAO,CAACnE,EAAU/D,OAAQX,EAAKyH,SAAUzH,EAAK+B,WAAY,SAkB5E,OAjBA2G,EAAyBI,QAAQ,SAACH,EAAGI,GACjCJ,EAAEG,QAAQ,SAACxI,EAAW0I,QACkBC,IAAhCjJ,EAAK8F,oBAAoBxF,GAEzBuI,EAAOK,IAAIlJ,EAAK8F,oBAAoBxF,GAAIyI,EAAeC,EAAW,GAC3D1I,EAAEK,QACTX,EAAKmJ,0BAA0B7I,GAAGwI,QAAQ,SAACM,EAAMC,GACzCA,EAAYrJ,EAAK+B,UAEjBuH,QAAQC,KAAK,wCAAyCjJ,GAG1DuI,EAAOK,IAAIE,EAAML,EAAeC,EAAWK,SAKpDR,EAAOW,eAvDpBjH,EAAAhG,KAAA,4BA2DkC,SAACiE,GACjC,IAAIiJ,EAAmB,GACjBC,EAA0B,SAACC,GAC7B,YAAoCV,IAAhCjJ,EAAK8F,oBAAoB6D,KAG7BF,EAAOG,KAAK5J,EAAK8F,oBAAoB6D,KAC9B,IAGX,GAAInJ,EAAKG,OAAS,EAAG,CACjB,IAAIkJ,GAAiB,EAOrB,GANmB7J,EAAKuF,UAAUuE,mBAAmBtJ,GAC1CsI,QAAQ,SAAAiB,IACVL,EAAwBK,IAAOF,IAChCA,GAAiB,KAGrBA,EACA,OAAOJ,EAOf,OAHAA,EAAS,GAETjJ,EAAKL,MAAM,IAAI2I,QAAQY,GAChBD,IApFPlN,KAAKuJ,oBAAsBA,EAC3BvJ,KAAKkL,SAAWA,EAChBlL,KAAKwF,UAAYA,EACjBxF,KAAKsF,oBAAsBA,EAC3BtF,KAAKoL,MAAQD,GAEP7B,EAAgBmE,WAAW/D,GAA0B,IAAIgE,IAAO1N,KAAKkL,SAAUlL,KAAKwF,UAAWxF,KAAKsF,qBAC1GtF,KAAKgJ,UAAYA,iDA/CjBU,EACAwB,EACA1F,EACAF,GAEA,IAAM8F,EAAQL,EAAG4C,aACXC,EAAa7C,EAAGQ,OAAOsC,UAAU,CACnCC,sBAAuB,IAAIC,gCAA8B,CACrDzI,sBACAoE,2BAEJsE,SAAUtE,EAAuBuE,KACjCC,YAAa,CAAC1I,GACd2I,UAAU,EACVC,UAAW9I,EACX+I,WAAW,IAIf,OAFAjD,EAAMkD,IAAIvD,EAAGQ,OAAOgD,gBAAgB,CAAEC,MAAOZ,EAAYa,WAAY,CAACvD,EAAU1F,MAChF4F,EAAMkD,IAAI,IAAII,qBAAmB,KAC1BtD,kLC1Bf,IAAAL,ySAAA7L,CAAAtC,EAAA,KACA+R,EAAA/R,EAAA,o2BAIa8R,sVAEUA,EAAmB1N,mQAFF+J,EAAGQ,OAAOqD,wDAIpBH,GACtB,MAAO,CAACA,EAAW,GAAIA,EAAW,GAAIA,EAAWA,EAAWrK,OAAS,iCAE7DsH,EAAmBmD,GAAa,IAAAC,EAAA9O,KACxC,OAAO+K,EAAGa,KAAK,WACXkD,EAAKC,eAAerD,EAAQmD,GAC5B,IAAMG,EAAWjE,EAAGkE,IAAIvD,EAAQ,GAC1BK,GAAS,EAAA4C,EAAAO,aAAYF,EAAU,GAErC,OADAA,EAAS9C,UACFH,kEAbN2C,cACiB,sBAiB9B3D,EAAGoE,cAAcC,iBAAiBC,SAASX,6HCvB3C,IAAA3D,EAAA7L,EAAAtC,EAAA,KACA0S,EAAApQ,EAAAtC,EAAA,MACA2S,EAAA3S,EAAA,k+CAOamR,cAIT,SAAAA,EAAYjJ,GAAgC,IAAArB,MAAA,mGAAAsC,CAAA/F,KAAA+N,KACxC/N,KAAAyD,MAAA+L,EAAAzB,GAAA0B,KAAAzP,qDADwCgG,EAAA0J,IAAAjM,IAAA,iBAAAuC,EAAA0J,IAAAjM,IAAA,YADzBsK,EAA8B/M,WAG7CyC,EAAKqB,OAASA,EAF0BrB,wPAJG6L,EAAaK,iDAQ/C7Q,EAAiB0M,GAA+B,IAAAsD,EAAA9O,KACzD,OAAKA,KAAK8E,QAAW9E,KAAK8E,OAAO4E,uBAG1BqB,EAAGa,KAAK,WACX,IAAMgE,GAAU,EAAAL,EAAAM,aAAAC,EAAgBhB,EAAKhK,OAAO4E,uBAAuBqG,WACnE,OAAOhF,EAAGiF,SAASJ,EAAS,CAACd,EAAKhK,OAAO4E,uBAAuBuE,KAAMa,EAAKhK,OAAOQ,qBAAsB,aAJjGyF,EAAGkF,MAAMnR,EAAO0M,uCAS3B,OAAOxL,KAAK8E,OAAO4E,gGAnBdqE,cACiB,iCAqB9BhD,EAAGoE,cAAcC,iBAAiBC,SAAStB,uGC/B3C,IAAAhD,EAAA7L,EAAAtC,EAAA,KACA2S,EAAA3S,EAAA,KACAsT,EAAAhR,EAAAtC,EAAA,01DAGqBoN,wBAgGjB,SAAAA,EACIlF,EACAkC,EACA4B,EACAlC,EACAyJ,EACAxG,GACF,IAAAlG,MAAA,mGAAAsC,CAAA/F,KAAAgK,KACEhK,OAAAwP,EAAAxF,GAAAyF,KAAAzP,MAAAyD,mDADFuC,EAAA0J,IAAAjM,IAAA,iBAAAuC,EAAA0J,IAAAjM,IAAA,wBAAAuC,EAAA0J,IAAAjM,IAAA,gBAAAuC,EAAA0J,IAAAjM,IAAA,0BAAAuC,EAAA0J,IAAAjM,IAAA,iBAAAuC,EAAA0J,IAAAjM,IAAA,0CAAAuC,EAAA0J,IAAAjM,IAAA,UAUe,kBAAMA,EAAK2H,QAV1BpF,EAAA0J,IAAAjM,IAAA,UAYe,SAAC0E,GACd,IAAMiI,EAAa,GAsBnB,OArBArF,EAAGa,KAAK,WACJ,IAAMyE,EAAoB5M,EAAKmF,gBAAgB0H,MAAMnI,GAC/C4D,EAAStI,EAAK2H,MAAMhD,QAAQiI,GAC5BE,EAAIxE,EAAOyE,WACjBzE,EAAOG,UACPmE,EAAkBnE,UAClB,IAAMuE,EAAUhN,EAAKuD,cAAcyJ,QACnCtI,EAAUoE,QAAQ,SAACH,EAAGsE,GAClB,IAAMC,EAAQJ,EAAEK,MAAMF,EAAID,EAAQrM,OAAQsM,EAAID,EAAQrM,OAASqM,EAAQrM,QACjEyM,EAA6C,GACnDF,EAAMpE,QAAQ,SAACuE,EAAGC,GAAJ,OACVF,EAAcxD,KAAK,CACf2D,WAAYF,EACZG,OAAQR,EAAQM,GAChB1N,SAAU+I,MAGlByE,EAAcK,KAAK,SAACC,EAAGC,GAAJ,OAAWD,EAAEH,WAAaI,EAAEJ,YAAc,EAAI,IACjEZ,EAAW/C,KAAKwD,EAAc,QAG/BT,IAnCTpK,EAAA0J,IAAAjM,IAAA,uBAAA8D,EAAAtB,EAAAC,mBAAAC,KAsCa,SAAAC,EAAOC,GAAP,IAAAgL,EAAAC,EAAAhU,EAAAiU,EAAAC,EAAAC,EAAAC,EAAAC,EAAAC,EAAAC,EAAA1N,EAAA2N,EAAAzB,EAAA0B,EAAAC,EAAAC,EAAAC,EAAA,OAAAhM,mBAAAI,KAAA,SAAAC,GAAA,cAAAA,EAAAC,KAAAD,EAAAE,MAAA,OACL4K,GAAe,EAAA9B,EAAA4C,OAAM9L,EAAa+L,OAAQ3O,EAAKqB,OAAOM,WACtDkM,GAAe,EAAA/B,EAAA4C,OAAM9L,EAAagM,OAAQ5O,EAAKqB,OAAOM,WAC5D3B,EAAKiD,OAAOC,IAAI,wCACVrJ,EAAImG,EAAK2H,MACXmG,GAAwB,EALjBC,GAAA,EAAAC,GAAA,EAAAC,OAAAhF,EAAAnG,EAAAC,KAAA,EAAAmL,EAMmBL,EAAagB,UANhCC,OAAAC,YAAA,WAAAhB,GAAAI,EAAAD,EAAAlL,QAAAgM,KAAA,CAAAlM,EAAAE,KAAA,YAAAoL,EAAAa,EAAAd,EAAAe,MAAA,GAMCxO,EAND0N,EAAA,GAMQC,EANRD,EAAA,IAOHN,EAPG,CAAAhL,EAAAE,KAAA,gBAAAF,EAAA0B,OAAA,yBAUDoI,EAAoB5M,EAAKmF,gBAAgB0H,MAAMwB,GAC/CC,EAAahH,EAAG6H,SAASvB,EAAalN,GAAQ,SAC9C6N,EAAmBjH,EAAG8H,OAAOd,EAAYtO,EAAKuD,cAAcyJ,QAAQrM,QAZnEmC,EAAAE,KAAA,GAaDnJ,EAAEwV,IAAIzC,EAAmB2B,EAAkB,CAE7Ce,UAAW,CAAEC,WAAYjI,EAAGkI,WAC5BjO,OAAQvB,EAAKqB,OAAOE,OACpBkO,SAAS,EACTC,gBAAiB1P,EAAKqB,OAAOW,0BAlB1B,QAoBPsM,EAAW7F,UACXmE,EAAkBnE,UAClB8F,EAAiB9F,UACX+F,EAAI3U,EAAE8V,QAAQA,QACdlB,EAAID,EAAEoB,SAASjP,OAAS,EAC1BX,EAAKkG,iCACLlG,EAAKkG,gCAAgC,CACjC2J,MAAOnP,EAAQ,EACfoP,YAAajW,EAAE8V,QAAQI,MAAMpP,OAC7BqP,iBAAkBnC,EAAanN,GAAOC,OACtCsP,gBAAiB3I,EAAG4I,SAASC,WAC7BC,aAAcvC,EAAalN,OAC3B0P,iBAAkB7B,EAAE8B,IAAI7B,GACxB8B,aAAc/B,EAAEgC,KAAK/B,GACrBgC,mBAAoBjC,EAAEkC,QAAQjC,GAC9BkC,eAAgBnC,EAAEoB,SAASnB,KAGnCzO,EAAKiD,OAAOC,IAAZ,WAAA0N,OAA2B/W,EAAE8V,QAAQI,MAAMpP,OAA3C,qBAAAiQ,OAAqElQ,EAAQ,EAA7E,QAAAkQ,OAAqF/C,EAAalN,SAClGX,EAAKiD,OAAOC,IAAZ,kBAAA0N,OAAkCpC,EAAEgC,KAAK/B,GAAzC,0BAAAmC,OAAoEpC,EAAE8B,IAAI7B,KAC1EzO,EAAKiD,OAAOC,IAAZ,oBAAA0N,OAAoCpC,EAAEoB,SAASnB,GAA/C,4BAAAmC,OAA4EpC,EAAEkC,QAAQjC,KACtFzO,EAAKiD,OAAOsG,KAAZ,6DAAAqH,OAA8EtJ,EAAG4I,SAASC,aAC1FnQ,EAAKiD,OAAOC,IAAI,sGAEZlD,EAAKqB,OAAOS,6BACZ0M,EAAEgC,KAAK/B,GAAKzO,EAAKqB,OAAOS,6BACxB0M,EAAEoB,SAASnB,GAAKzO,EAAKqB,OAAOS,8BAE5BgM,GAAwB,EACxB9N,EAAKiD,OAAOsG,KAAZ,wDAAAqH,OAAyElQ,EAAQ,EAAjF,QAAAkQ,OAAyF/C,EAAalN,SACtGX,EAAKiD,OAAOC,IAAI,uGAlDb,QAAA6K,GAAA,EAAAjL,EAAAE,KAAA,iBAAAF,EAAAE,KAAA,iBAAAF,EAAAC,KAAA,GAAAD,EAAAuB,GAAAvB,EAAA,SAAAkL,GAAA,EAAAC,EAAAnL,EAAAuB,GAAA,QAAAvB,EAAAC,KAAA,GAAAD,EAAAC,KAAA,GAAAgL,GAAA,MAAAG,EAAA2C,QAAA3C,EAAA2C,SAAA,WAAA/N,EAAAC,KAAA,IAAAiL,EAAA,CAAAlL,EAAAE,KAAA,eAAAiL,EAAA,eAAAnL,EAAAgO,OAAA,mBAAAhO,EAAAgO,OAAA,6BAAAhO,EAAAY,SAAAf,EAAApG,KAAA,+BAtCb,gBAAAoH,GAAA,OAAAG,EAAAF,MAAArH,KAAAsH,YAAA,IAAAtB,EAAA0J,IAAAjM,IAAA,sBAAA4E,EAAApC,EAAAC,mBAAAC,KA6FY,SAAAqB,EACVgN,EACAC,GAFU,IAAAC,EAAAC,EAAAC,EAAAC,EAAAC,EAAAC,EAAAC,EAAAC,EAAAC,EAAAC,EAAAzE,EAAAvI,EAAAiN,EAAA,OAAAlP,mBAAAI,KAAA,SAAAsB,GAAA,cAAAA,EAAApB,KAAAoB,EAAAnB,MAAA,OAQV,IAJMiO,EAAUD,GAAkChR,EAAK4R,qBACjDV,EAAgC,CAAE5M,QAAS,EAAGC,MAAO,EAAGsN,cAAe,GACvEV,GAAI,EAAArF,EAAA4C,OAAMqC,EAAae,MAAO9R,EAAKqB,OAAOM,WAC1CyP,GAAI,EAAAtF,EAAA4C,OAAMqC,EAAagB,MAAO/R,EAAKqB,OAAOM,WAPtC0P,GAAA,EAAAC,GAAA,EAAAC,OAAAtI,EAAA9E,EAAApB,KAAA,EAQVyO,EAA6BL,EAAEtC,UAA/BC,OAAAC,cAAAsC,GAAAI,EAAAD,EAAAxO,QAAAgM,MAAAqC,GAAA,EAA0CK,EAAAzC,EAAAwC,EAAAvC,MAAA,GAA9BjC,EAA8ByE,EAAA,GAA3BhN,EAA2BgN,EAAA,GAChCC,EAAc3R,EAAK2E,QAAQD,GACjCuM,EAAQvM,EAAW0M,EAAEnE,GAAI0E,EAAaT,GAVhC/M,EAAAnB,KAAA,iBAAAmB,EAAApB,KAAA,GAAAoB,EAAAE,GAAAF,EAAA,SAAAmN,GAAA,EAAAC,EAAApN,EAAAE,GAAA,QAAAF,EAAApB,KAAA,GAAAoB,EAAApB,KAAA,GAAAsO,GAAA,MAAAG,EAAAX,QAAAW,EAAAX,SAAA,WAAA1M,EAAApB,KAAA,IAAAuO,EAAA,CAAAnN,EAAAnB,KAAA,eAAAuO,EAAA,eAAApN,EAAA2M,OAAA,mBAAA3M,EAAA2M,OAAA,mBAAA3M,EAAAK,OAAA,SAYH0M,GAZG,yBAAA/M,EAAAT,SAAAK,EAAAxH,KAAA,+BA7FZ,gBAAAkI,EAAAY,GAAA,OAAAT,EAAAhB,MAAArH,KAAAsH,YAAA,IAAAtB,EAAA0J,IAAAjM,IAAA,uBA4G6B,SAC3BmR,EACAC,EACAY,EACAd,GAmBA,OAjBAC,EAAErI,QAAQ,SAACH,EAAGsE,GACV,IAAMO,EAASxN,EAAKuD,cAAcyJ,QAAQoE,EAAEnE,IACtC3I,EAAU0N,EAAE/E,GAAGO,SAAWA,EAChC,GAAIwE,EAAE/E,GAAGM,WAAavN,EAAKqB,OAAOI,uBAAwB,CACtD,QAA4BwH,IAAxBiI,EAAMW,cACN,OAEJX,EAAMW,gBACN7R,EAAKiD,OAAOsG,KAAZ,2BAAAqH,OAA4CoB,EAAE/E,GAAGO,OAAjD,kBAAAoD,OAAwEoB,EAAE/E,GAAGM,WAA7E,QAAAqD,OAA8FjI,SACvFrE,GACP4M,EAAM5M,UACNtE,EAAKiD,OAAOgP,MAAZ,oBAAArB,OAAsCoB,EAAE/E,GAAGO,OAA3C,kBAAAoD,OAAkEoB,EAAE/E,GAAGM,WAAvE,QAAAqD,OAAwFjI,MAExFuI,EAAM3M,QACNvE,EAAKiD,OAAOiP,MAAZ,kBAAAtB,OAAoCoB,EAAE/E,GAAGO,OAAzC,kBAAAoD,OAAgEoB,EAAE/E,GAAGM,WAArE,QAAAqD,OAAsFjI,OAGvFuI,IAjIPlR,EAAKqB,OAASA,EACdrB,EAAKuD,cAAgBA,EACrBvD,EAAKmF,gBAAkBA,EACvBnF,EAAK2H,MAAQ+E,GAAoCnG,EAAoB4L,MAAMnS,EAAKqB,OAAQrB,EAAKuD,eAC7FvD,EAAKiD,OAASA,EACdjD,EAAKkG,gCAAkCA,EAPzClG,8OAvG2CyM,EAAM2F,kDAE/C/Q,KAEF,IADyBoG,EACzBjN,EADIuL,oBAEIsM,EADR7X,EADmCwS,QAENrM,OAIrB2R,EAAYhL,EAAGlE,MAAMmP,KAHL,MACF,MACA,IAGd1K,EAAQP,EAAGO,MAAM,CACnBE,MAAO,UACP1M,MAAO,CAACoM,EAAUpG,EAAOQ,qBACzB2Q,KAAM,mBAEJC,EAAanL,EAAGQ,OACjB4K,OAAO,CACJC,WAAY,OACZC,QAASvR,EAAOK,WAChBsJ,WAAY,CAACvD,EAAUpG,EAAOQ,qBAC9BgR,kBAAmB,eACnBC,WAAY,CAACzR,EAAOG,YAAY,IAChCgR,KAAM,aACNlV,QAAS,UAEZsG,MAAMiE,GACLkL,EAAWzL,EAAGQ,OACfkL,aAAa,CACV1V,QAAS,QACT2V,SAAUxL,EAAWpG,EAAOG,YAAY,GAAK,IAEhDoC,MAAM6O,GAELS,EAAa5L,EAAGQ,OACjB4K,OAAO,CACJC,WAAY,OACZC,QAASvR,EAAOK,WAChBsJ,WAAY,CAACvD,EAAUpG,EAAOQ,qBAC9BgR,kBAAmB,eACnBC,WAAY,CAACzR,EAAOG,YAAY,IAChCgR,KAAM,aACNlV,QAAS,UAEZsG,MAAMiE,GACLsL,EAAW7L,EAAGQ,OACfkL,aAAa,CACV1V,QAAS,QACT2V,SAAUxL,EAAWpG,EAAOG,YAAY,GAAK,IAEhDoC,MAAMsP,GAELE,EAAa9L,EAAGQ,OACjB4K,OAAO,CACJC,WAAY,OACZC,QAASvR,EAAOK,WAChBsJ,WAAY,CAACvD,EAAUpG,EAAOQ,qBAC9BgR,kBAAmB,eACnBC,WAAY,CAACzR,EAAOG,YAAY,IAChCgR,KAAM,aACNlV,QAAS,UAEZsG,MAAMiE,GACLwL,EAAW/L,EAAGQ,OACfkL,aAAa,CACV1V,QAAS,QACT2V,SAAUxL,EAAWpG,EAAOG,YAAY,GAAK,IAEhDoC,MAAMwP,GAELE,EAAchM,EAAGQ,OAAOyL,YAAY,CAAEC,KAAM,IAAK5P,MAAM,CAACmP,EAAUI,EAAUE,IAC5EI,EAAOnM,EAAGQ,OAAO4L,UAAU9P,MAAM0P,GACjCK,EAAUrM,EAAGQ,OAAO8L,QAAQ,CAAEC,KAAMxS,EAAOO,OAAQgC,MAAM6P,GACzDK,EAAYxM,EAAGQ,OAAO4L,UAAU9P,MAAMmP,GACtCgB,EAA0BzM,EAAGQ,OAAOyL,YAAY,CAAEC,KAAM,IAAK5P,MAAM,CAAC+P,EAASG,IAC7EE,EAAoB1M,EAAGQ,OACxBmM,MAAM,CACHtB,WAAY,UACZuB,MAAO7B,IAEVzO,MAAMmQ,GACLpM,EAAQL,EAAGK,MAAM,CAAEM,OAAQJ,EAAOK,QAAS8L,IAMjD,OALArM,EAAMwM,QAAQ,CACV3D,KAAM,0BACN4D,QAAS,CAAC,YACV9B,cAEG3K,+JC5Ff,IAAAL,EAAA7L,EAAAtC,EAAA,KACA2S,EAAA3S,EAAA,KACAsT,EAAAhR,EAAAtC,EAAA,MAEAkb,EAAAlb,EAAA,y1DAEqBuN,wBAqEjB,SAAAA,EACIrF,EACAkC,EACA4B,EACAlC,EACAyJ,EACAvG,GACF,IAAAnG,MAAA,mGAAAsC,CAAA/F,KAAAmK,KACEnK,OAAAwP,EAAArF,GAAAsF,KAAAzP,MAAAyD,mDADFuC,EAAA0J,IAAAjM,IAAA,iBAAAuC,EAAA0J,IAAAjM,IAAA,wBAAAuC,EAAA0J,IAAAjM,IAAA,gBAAAuC,EAAA0J,IAAAjM,IAAA,0BAAAuC,EAAA0J,IAAAjM,IAAA,iBAAAuC,EAAA0J,IAAAjM,IAAA,+BAAAuC,EAAA0J,IAAAjM,IAAA,UAUe,kBAAMA,EAAK2H,QAV1BpF,EAAA0J,IAAAjM,IAAA,gBAYqB,SAAC0E,EAAqB4P,GACzC,OAAOhN,EAAGa,KAAK,WAAM,IAAAoM,EACoCvU,EAAKuD,cAA7BkE,EADZ8M,EACTxO,oBAA+BvC,EADtB+Q,EACsB/Q,UACjCgR,EAAkBnR,OAAOC,KAAKE,GAAW7C,OACzCiM,EAAoB5M,EAAKmF,gBAAgB0H,MAAMnI,GAC/C+P,EAAgBH,EAAmBjU,IAAI,SAAAgN,GACzC,IAAMqH,EAAgB,IAAIC,MAAM3U,EAAKuD,cAAcyJ,QAAQrM,QAAQiU,KAAK,GAClEtH,EAAMtN,EAAKuD,cAAcyJ,QAAQ6H,QAAQxH,EAAEG,QAIjD,OAHa,IAATF,IACAoH,EAAcpH,GAAO,GAElBoH,IAELI,GAAc,EAAAhJ,EAAAM,aAAYqI,GAC1BM,EAAazN,EAAGiF,SAASuI,EAAa,CAACL,EAAc9T,OAAQX,EAAKuD,cAAcyJ,QAAQrM,SACxF2H,EAAStI,EAAK2H,MAAMhD,QAAQ,CAACoQ,EAAYnI,IACzCoI,EAAuB1M,EAAOyE,WAMpC,OALAzE,EAAOG,UACPsM,EAAWtM,UACXmE,EAAkBnE,WAEH,EAAAqD,EAAA4C,OAAMsG,EAAsBvN,EAAW+M,GAAiBnU,IAAI,SAAA4U,GAAE,OAAI,EAAAnJ,EAAA4C,OAAMuG,EAAIT,KAC7EnU,IAAI,SAAA+M,GACd,OAAOA,EAAc/M,IAAI,SAAA6U,GACrB,IAAIC,EAAe,EACf5H,EAAa2H,EAAmBvU,OAASuU,EAAmBC,GAAgB,EAOhF,OANAD,EAAmBpM,QAAQ,SAACsM,EAAIC,GACxBH,EAAmBC,GAAgBC,IACnCD,EAAeE,EACf9H,EAAa6H,KAGd,CAAED,eAAc5H,sBA5CrChL,EAAA0J,IAAAjM,IAAA,UAkDe,SAAC0E,EAAqB4P,GAAoD,IAC/E7S,EAA2BzB,EAAKqB,OAAhCI,uBACA+B,EAAcxD,EAAKuD,cAAnBC,UACF8R,EAAuBtV,EAAKuV,cAAc7Q,EAAW4P,GAC3D,OAAO5P,EAAUrE,IAAI,SAACsI,EAAGsE,GACrB,IAAMuI,EAA4BF,EAAqBrI,GACjDwI,EAAgBzV,EAAKmF,gBAAgBI,UAAUqD,qBAAqBD,GAC1E,OAAO8M,EAAcC,OACjB,SAACC,EAAiCrV,EAAWsV,GACrCD,EAAYE,SAA8C,IAAnCF,EAAYE,QAAQtI,aAC3CoI,EAAYE,QAAQtI,WAAaiI,EAA0BI,GAAcrI,YAE7E,IAAMuI,EAAiBzS,OAAOC,KAAKE,GAAWuS,KAC1C,SAAAC,GAAO,OACHR,EAA0BI,IAC1BpS,EAAUwS,KAAaR,EAA0BI,GAAcT,eAEvE,OAAKW,GAAmBH,EAAYE,SAGhCF,EAAYE,QAAQhY,MAAQiY,GAExBH,EAAYE,QAAQhY,KACQ,MAA5B8X,EAAYE,QAAQhY,KACpB8X,EAAYE,QAAQtI,YAAc9L,IAE7BkU,EAAYM,MAAMN,EAAYE,QAAQhY,OACvC8X,EAAYM,MAAMN,EAAYE,QAAQhY,KAAO,IAEjD8X,EAAYM,MAAMN,EAAYE,QAAQhY,KAAK+L,KAAK,CAC5C2D,WAAYoI,EAAYE,QAAQtI,WAChC2B,MAAOyG,EAAYE,QAAQ3G,SAGnCyG,EAAYE,QAAU,CAClBtI,WAAYiI,EAA0BI,GAAcrI,WACpD1P,IAAKiY,EACL5G,MAAO5O,KAIXqV,EAAYE,QAAQ3G,OAApB,IAAA0B,OAAiCtQ,GACjCqV,EAAYE,QAAQtI,YACfiI,EAA0BI,GAAcrI,WAAaoI,EAAYE,QAAQtI,YAAc,GAE5FqI,EAAe,IAAMH,EAAc9U,QACH,MAA5BgV,EAAYE,QAAQhY,KAAe8X,EAAYE,QAAQtI,YAAc9L,IAChEkU,EAAYM,MAAMN,EAAYE,QAAQhY,OACvC8X,EAAYM,MAAMN,EAAYE,QAAQhY,KAAO,IAEjD8X,EAAYM,MAAMN,EAAYE,QAAQhY,KAAK+L,KAAK,CAC5C2D,WAAYoI,EAAYE,QAAQtI,WAChC2B,MAAOyG,EAAYE,QAAQ3G,SAG5B,CAAEtP,SAAU+I,EAAGsN,MAAON,EAAYM,QAEtCN,GAvCIA,GAyCf,CAAEE,QAAS,CAAEhY,IAAK,GAAIqR,MAAO,GAAI3B,WAAY,GAAK0I,MAAO,GAAIrW,SAAU,SA7GjF2C,EAAA0J,IAAAjM,IAAA,uBAAAxF,EAAAgI,EAAAC,mBAAAC,KAkHa,SAAAC,EAAOC,GAAP,IAAAsT,EAAAtI,EAAAC,EAAAsI,EAAA5U,EAAAmO,EAAA0G,EAAAtI,EAAAC,EAAAC,EAAAC,EAAAoI,EAAAnI,EAAAC,EAAAmI,EAAA,OAAA7T,mBAAAI,KAAA,SAAAsB,GAAA,cAAAA,EAAApB,KAAAoB,EAAAnB,MAAA,OACLkT,GAAgB,EAAApK,EAAA4C,OAAM9L,EAAa2T,QAASvW,EAAKqB,OAAOM,WACxDiM,GAAe,EAAA9B,EAAA4C,OAAM9L,EAAa+L,OAAQ3O,EAAKqB,OAAOM,WACtDkM,GAAe,EAAA/B,EAAA4C,OAAM9L,EAAagM,OAAQ5O,EAAKqB,OAAOM,WAHjDwU,EAIkDnW,EAAKqB,OAA1DE,EAJG4U,EAIH5U,OAAiCmO,EAJ9ByG,EAIKnU,wBACVoU,EAAc/S,OAAOC,KAAKtD,EAAKuD,cAAcC,WAAW7C,OAC9DX,EAAKiD,OAAOC,IAAI,6BACZ4K,GAAwB,EAPjBC,GAAA,EAAAC,GAAA,EAAAC,OAAAhF,EAAA9E,EAAApB,KAAA,GAAAsT,EAAA5T,mBAAAC,KAAA,SAAA2T,IAAA,IAAAjI,EAAA1N,EAAA2N,EAAAmI,EAAAC,EAAAC,EAAAlI,EAAAC,EAAAsB,EAAA,OAAAtN,mBAAAI,KAAA,SAAAC,GAAA,cAAAA,EAAAC,KAAAD,EAAAE,MAAA,UAAAoL,EAAAa,EAAAd,EAAAe,MAAA,GAQCxO,EARD0N,EAAA,GAQQC,EARRD,EAAA,IASHN,EATG,CAAAhL,EAAAE,KAAA,eAAAF,EAAA0B,OAAA,UAAAmS,OAAA,kBAaDH,EAAelP,EAAGa,KAAK,kBACzBb,EAAG8H,OAAO9H,EAAG6H,SAASvB,EAAalN,GAAQ,SAAUV,EAAKuD,cAAcyJ,QAAQrM,QAAQiW,OAAO,aAE7FH,EAAwBzW,EAAKmF,gBAAgB0H,MAAMwB,GAGnDqI,EAAwBpP,EAAGa,KAAK,WAClC,IAAM0O,EAA6B,GADKxF,GAAA,EAAAC,GAAA,EAAAC,OAAAtI,EAAA,IAExC,QAAAwI,EAAAD,EAA0B0E,EAAcxV,GAAxCoO,OAAAC,cAAAsC,GAAAI,EAAAD,EAAAxO,QAAAgM,MAAAqC,GAAA,EAAgD,KAArCyF,EAAqCrF,EAAAvC,MACtC6H,EAAUzP,EACX6H,SAAS2H,EAAa,SACtBE,IAAI,CAAC,CAAC,EAAGhX,EAAKuD,cAAcwC,oBAAsB+Q,EAAYnW,UAC7DsW,EAAM3P,EAAG8H,OAAO2H,EAASX,GAAaQ,OAAO,WACnDG,EAAQtO,UACRoO,EAAYjN,KAAKqN,IARmB,MAAAC,GAAA5F,GAAA,EAAAC,EAAA2F,EAAA,YAAA7F,GAAA,MAAAG,EAAAX,QAAAW,EAAAX,SAAA,WAAAS,EAAA,MAAAC,GAUxC,IAAM4F,EAAQ7P,EAAG6P,MAAMN,GAEvB,OADAA,EAAY/N,QAAQ,SAAAH,GAAC,OAAIA,EAAEF,YACpB0O,IA/BJrU,EAAAE,KAAA,EAiCDhD,EAAK2H,MAAM0H,IAAI,CAACmH,EAAcC,GAAwBC,EAAU,CAElEpH,UAAW,CAAEC,WAAYjI,EAAGkI,WAC5BjO,SACAkO,SAAS,EACTC,oBAtCG,cAwCP8G,EAAa/N,UACbgO,EAAsBhO,UAzCf3F,EAAAE,KAAA,GA0CDsE,EAAGkI,YA1CF,QA2CDhB,EAAIxO,EAAK2H,MAAMgI,QAAQA,QACvBlB,EAAID,EAAEoB,SAASjP,OAAS,EACxBoP,EAAQ/P,EAAK2H,MAAMgI,QAAQI,MAC7B/P,EAAKmG,sBACLnG,EAAKmG,qBAAqB,CACtB0J,MAAOnP,EAAQ,EACfoP,YAAaC,EAAMpP,OACnBqP,iBAAkBnC,EAAanN,GAAOC,OACtCsP,gBAAiB3I,EAAG4I,SAASC,WAC7BC,aAAcvC,EAAalN,OAC3B0P,iBAAkB7B,EAAE8B,IAAI7B,GACxB8B,aAAc/B,EAAEgC,KAAK/B,GACrBgC,mBAAoBjC,EAAEkC,QAAQjC,GAC9BkC,eAAgBnC,EAAEoB,SAASnB,KAGnCzO,EAAKiD,OAAOC,IAAZ,WAAA0N,OAA2Bb,EAAMpP,OAAjC,qBAAAiQ,OAA2DlQ,EAAQ,EAAnE,QAAAkQ,OAA2E/C,EAAalN,SACxFX,EAAKiD,OAAOC,IAAZ,kBAAA0N,OAAkCpC,EAAEgC,KAAK/B,GAAzC,0BAAAmC,OAAoEpC,EAAE8B,IAAI7B,KAC1EzO,EAAKiD,OAAOC,IAAZ,oBAAA0N,OAAoCpC,EAAEoB,SAASnB,GAA/C,4BAAAmC,OAA4EpC,EAAEkC,QAAQjC,KACtFzO,EAAKiD,OAAOsG,KAAZ,6DAAAqH,OAA8EtJ,EAAG4I,SAASC,aAC1FnQ,EAAKiD,OAAOC,IAAI,sGAChBwT,EAASjO,UAELzI,EAAKqB,OAAOS,6BACZ0M,EAAEgC,KAAK/B,GAAKzO,EAAKqB,OAAOS,6BACxB0M,EAAEoB,SAASnB,GAAKzO,EAAKqB,OAAOS,8BAE5BgM,GAAwB,EACxB9N,EAAKiD,OAAOsG,KAAZ,wDAAAqH,OAAyElQ,EAAQ,EAAjF,QAAAkQ,OAAyF/C,EAAalN,SACtGX,EAAKiD,OAAOC,IAAI,uGAxEb,yBAAAJ,EAAAY,SAAA2S,EAAA9Z,QAAA2R,EAQmBL,EAAagB,UARhCC,OAAAC,YAAA,WAAAhB,GAAAI,EAAAD,EAAAlL,QAAAgM,KAAA,CAAA7K,EAAAnB,KAAA,gBAAAmB,EAAAiT,cAAAf,IAAA,+BAAAgB,EAAAf,EAAAnS,EAAAE,IAAA,CAAAF,EAAAnB,KAAA,gBAAAmB,EAAAK,OAAA,SAAA8R,EAAAK,GAAA,QAAA5I,GAAA,EAAA5J,EAAAnB,KAAA,iBAAAmB,EAAAnB,KAAA,iBAAAmB,EAAApB,KAAA,GAAAoB,EAAAmT,GAAAnT,EAAA,UAAA6J,GAAA,EAAAC,EAAA9J,EAAAmT,GAAA,QAAAnT,EAAApB,KAAA,GAAAoB,EAAApB,KAAA,GAAAgL,GAAA,MAAAG,EAAA2C,QAAA3C,EAAA2C,SAAA,WAAA1M,EAAApB,KAAA,IAAAiL,EAAA,CAAA7J,EAAAnB,KAAA,eAAAiL,EAAA,eAAA9J,EAAA2M,OAAA,mBAAA3M,EAAA2M,OAAA,6BAAA3M,EAAAT,SAAAf,EAAApG,KAAA,gCAlHb,gBAAAoH,GAAA,OAAAnJ,EAAAoJ,MAAArH,KAAAsH,YAAA,IAAAtB,EAAA0J,IAAAjM,IAAA,sBAAA8D,EAAAtB,EAAAC,mBAAAC,KA+LY,SAAAqB,EACVgN,EACAC,GAFU,IAAAC,EAAAC,EAAAvP,EAAAmQ,EAAAC,EAAAwF,EAAAC,EAAAC,EAAAC,EAAAC,EAAAC,EAAAC,EAAA,OAAApV,mBAAAI,KAAA,SAAAiV,GAAA,cAAAA,EAAA/U,KAAA+U,EAAA9U,MAAA,OAIJiO,EAAUD,GAAkChR,EAAK4R,qBACjDV,EAAgC,CAAE5M,QAAS,EAAGC,MAAO,GACrD5C,EAAY3B,EAAKqB,OAAOM,UACxBmQ,GAAQ,EAAAhG,EAAA4C,OAAMqC,EAAae,MAAOnQ,GAClCoQ,GAAQ,EAAAjG,EAAA4C,OAAMqC,EAAagB,MAAOpQ,GAClC4V,GAAS,EAAAzL,EAAA4C,OAAMqC,EAAawG,OAAQ5V,GAThC6V,GAAA,EAAAC,GAAA,EAAAC,OAAAzO,EAAA6O,EAAA/U,KAAA,EAAA4U,EAAAlV,mBAAAC,KAAA,SAAAiV,IAAA,IAAAI,EAAA9K,EAAAvI,EAAAsT,EAAAvD,EAAA9C,EAAA,OAAAlP,mBAAAI,KAAA,SAAAiC,GAAA,cAAAA,EAAA/B,KAAA+B,EAAA9B,MAAA,cAAA+U,EAAA9I,EAAA4I,EAAA3I,MAAA,GAUEjC,EAVF8K,EAAA,GAUKrT,EAVLqT,EAAA,GAWAC,EAAkBjG,EAAM9E,GACxBwH,EAAgB/P,EAAUrE,IAC5B,SAACgN,EAAGC,GAAJ,MACK,CACGC,WAAY,EACZC,OAAQxN,EAAKuD,cAAcyJ,QAAQgL,EAAgB1K,IACnD1N,SAAUyN,KAGhBsE,EAAc3R,EAAKuV,cAAc7Q,EAAW+P,GAAepU,IAAI,SAAAT,GAAQ,OAAIA,EAASS,IAAI,SAAAsI,GAAC,OAAIA,EAAEwM,iBACrGlE,EAAQvM,EAAW6S,EAAOtK,GAAI0E,EAAaT,GArBrCpM,EAAA9B,KAAA,EAsBAsE,EAAGkI,YAtBH,wBAAA1K,EAAApB,SAAAiU,EAAApb,QAAAqb,EAUmB9F,EAAMjD,UAVzBC,OAAAC,YAAA,WAAAyI,GAAAK,EAAAD,EAAA5U,QAAAgM,KAAA,CAAA8I,EAAA9U,KAAA,gBAAA8U,EAAAV,cAAAO,IAAA,iBAAAH,GAAA,EAAAM,EAAA9U,KAAA,iBAAA8U,EAAA9U,KAAA,iBAAA8U,EAAA/U,KAAA,GAAA+U,EAAAR,GAAAQ,EAAA,SAAAL,GAAA,EAAAC,EAAAI,EAAAR,GAAA,QAAAQ,EAAA/U,KAAA,GAAA+U,EAAA/U,KAAA,GAAAyU,GAAA,MAAAI,EAAA/G,QAAA+G,EAAA/G,SAAA,WAAAiH,EAAA/U,KAAA,IAAA0U,EAAA,CAAAK,EAAA9U,KAAA,eAAA0U,EAAA,eAAAI,EAAAhH,OAAA,mBAAAgH,EAAAhH,OAAA,mBAAAgH,EAAAtT,OAAA,SAwBH0M,GAxBG,yBAAA4G,EAAApU,SAAAK,EAAAxH,KAAA,+BA/LZ,gBAAAkI,EAAAY,GAAA,OAAAvB,EAAAF,MAAArH,KAAAsH,YAAA,IAAAtB,EAAA0J,IAAAjM,IAAA,uBA0N6B,SAC3BmR,EACA8G,EACAjG,EACAd,GAmBA,OAjBAC,EAAErI,QAAQ,SAACH,EAAGuP,GACV,IAAMC,EAAeF,EAAGC,GAClBE,EAAgBpG,EAAEkG,GACpB5T,GAAU,EACd6T,EAAarP,QAAQ,SAACuP,EAAK/K,GACnB8K,EAAc9K,KAAS+K,GAAO/T,IAC9BA,GAAU,KAGdA,GACA4M,EAAM5M,UACNtE,EAAKiD,OAAOgP,MAAZ,aAAArB,OAA+BjI,EAA/B,eAAAiI,OAA8CuH,EAA9C,iBAAAvH,OAA0EwH,MAE1ElH,EAAM3M,QACNvE,EAAKiD,OAAOiP,MAAZ,WAAAtB,OAA6BjI,EAA7B,eAAAiI,OAA4CuH,EAA5C,iBAAAvH,OAAwEwH,OAGzElH,IA/OPlR,EAAKqB,OAASA,EACdrB,EAAKuD,cAAgBA,EACrBvD,EAAKmF,gBAAkBA,EACvBnF,EAAK2H,MAAQ+E,GAAoChG,EAASyL,MAAMnS,EAAKqB,OAAQrB,EAAKuD,eAClFvD,EAAKiD,OAASA,EACdjD,EAAKmG,qBAAuBA,EAP9BnG,8OA5EgCyM,EAAM2F,kDACnB/Q,EAA2DkC,GAC5E,IAAMkE,EAAWlE,EAAcwC,oBACvB7D,EAA4Db,EAA5Da,aAAcL,EAA8CR,EAA9CQ,oBAAqBH,EAAyBL,EAAzBK,WAAYS,EAAad,EAAbc,SACjDmW,EAAejV,OAAOC,KAAKC,EAAcC,WAAW7C,OAIpD2R,EAAYhL,EAAGlE,MAAMmP,KAHL,MACF,MACA,IAGdgG,EAAyBjR,EAAGO,MAAM,CACpCE,MAAO,UACP1M,MAAO,CAACoM,EAAU5F,GAClB2Q,KAAM,mBAEJC,EAAanL,EAAGQ,OACjB4K,OAAO,CACJC,WAAY,OACZC,QAASlR,EAAW,GACpBsJ,WAAY,CAACvD,EAAU5F,GACvBgR,kBAAmB,eACnBC,WAAY,EACZN,KAAM,WACNlV,QAAS,UAEZsG,MAAM2U,GACLrF,EAAa5L,EAAGQ,OACjB4K,OAAO,CACJC,WAAY,OACZC,QAASlR,EAAW,GACpBmR,kBAAmB,eACnBC,WAAY,EACZN,KAAM,WACNlV,QAAS,UAEZsG,MAAM6O,GAEL+F,EAAkBlR,EAAGO,MAAM,CAC7BE,MAAO,UACP1M,MAAO,CAACkI,EAAcyJ,QAAQrM,QAC9B6R,KAAM,oBAEJiG,EAAqBnR,EAAGQ,OAAO4Q,aAAa,CAAEC,IAAe/U,MAAM4U,GACnEI,EAAWtR,EAAGQ,OAAOyL,cAAc3P,MAAM,CAAC6U,EAAoBF,EAAwBrF,IACtF2F,EAASvR,EAAGQ,OACbgR,cAAc,CACX/N,MAAOzD,EAAGQ,OAAOiR,KAAK,CAAE7E,MAAO/R,EAAU6W,iBAAiB,EAAMxG,KAAM,mBAEzE5O,MAAMgV,GACPK,EAAwC,KAC5C,GAAI/W,EAAc,CACd,IAAMgX,EAAgB,IAAIC,sBAAoB,CAAE3G,KAAM,qBAAsB5O,MAAMiV,EAAO,IACzFI,EAAc3R,EAAGQ,OAAOyL,cAAc3P,MAAM,CAACsV,EAAeL,EAAO,GAAIA,EAAO,UAE9EI,EAAc3R,EAAGQ,OAAOyL,cAAc3P,MAAM,CAACiV,EAAO,GAAIA,EAAO,KAEnE,IAAM3Q,EAAUZ,EAAGQ,OAAOmM,MAAM,CAAEtB,WAAY,UAAWuB,MAAOoE,IAAgB1U,MAAMqV,GAChFtR,EAAQL,EAAGK,MAAM,CAAEM,OAAQ,CAACuQ,EAAiBD,GAAyBrQ,YAE5E,OADAP,EAAMwM,QAAQ,CAAE3D,KAAM,0BAA2B4D,QAAS,CAAC,YAAa9B,cACjE3K,2KCjEf,IAAAL,ySAAA7L,CAAAtC,EAAA,KACAigB,EAAAjgB,EAAA,o2BAOaggB,cAMT,SAAAA,EAAY9X,GAAc,IAAArB,MAAA,mGAAAsC,CAAA/F,KAAA4c,KACtB5c,KAAAyD,MAAA+L,EAAAoN,GAAAnN,KAAAzP,KAAM8E,GAAU,mDADMkB,EAAA0J,IAAAjM,IAAA,YAJPmZ,EAAoB5b,WAIbgF,EAAA0J,IAAAjM,IAAA,QAFa,MAInCA,EAAKqZ,UAAY,CAAC,IAAIC,YAAU,CAAEC,KAAM,KACxCvZ,EAAKwZ,iBAAkB,EAHDxZ,wPANWsH,EAAGQ,OAAOqD,2CAYlCH,GACT,IAAMyO,EAAazO,EAAW,GACxB0O,EAAQpS,EAAG4C,WAAW,CAAEsI,KAAM,kBACpCkH,EAAM7O,IACFvD,EAAGQ,OAAOmM,MAAM,CACZjJ,WAAY,CAACyO,GACb5G,kBAAmB,QACnBqB,MAAOuF,EACP9G,WAAY,UACZH,KAAM,gBAGdkH,EAAM7O,IAAIvD,EAAGQ,OAAOmM,MAAM,CAAEC,MAAOuF,EAAY5G,kBAAmB,eAAgBF,WAAY,OAAQH,KAAM,gBAC5GjW,KAAKmd,MAAQpS,EAAGQ,OAAOgD,gBAAgB,CAAEC,MAAO2O,EAAOlH,KAAM,WAC7DjW,KAAKmd,MAAMC,MAAM3O,GACjBzO,KAAKqd,iBAAmBrd,KAAKmd,MAAME,iBACnCrd,KAAKsd,oBAAsBtd,KAAKmd,MAAMG,oBACtCtd,KAAKud,OAAQ,+BAGL7R,EAAqBmD,GAAa,IAAAC,EAAA9O,KAC1C,IAAKA,KAAKud,QAAUvd,KAAKmd,MACrB,MAAM,IAAItb,MAAM,oEAEpB,OAAOkJ,EAAGa,KAAK,WACXkD,EAAKC,eAAerD,EAAQmD,GAC5B,IAAM2O,EAAW1O,EAAKqO,MAA0B9V,MAAMqE,GAChD+R,EAAW1S,EAAGQ,OAAOmS,QAAQ,CAAEC,KAAM,CAAC,EAAG,KAAMtW,MAAMmW,GAGrDI,EAAiB7S,EAAG8S,QAAQnS,EAAO,IACnCoS,EAAoB/S,EAAG8S,QAAQJ,GAC/BM,EAAWH,EAAe9Z,IAAI,SAACka,EAAItN,GAAL,OAAW3F,EAAGkT,IAAID,EAAIF,EAAkBpN,MACtEwN,EAAanT,EAAG6P,MAAMmD,GACtBI,EAAYpT,EAAGqT,QAAQF,GACvBG,EAAoBtT,EAAGQ,OAAOmS,QAAQ,CAAEC,KAAM,CAAC,EAAG,KAAMtW,MAAM8W,GAE9DG,EADqBvT,EAAG8S,QAAQQ,GACKva,IAAI,SAACya,EAAI7N,GAAL,OAAW3F,EAAGkT,IAAIM,EAAIX,EAAelN,MAEpF,OADe3F,EAAG6P,MAAM0D,gDAKN7P,GACtB,OAAOA,iEAxDFmO,cACiB,uBA2D9B7R,EAAGoE,cAAcC,iBAAiBC,SAASuN,mMCnE3Cre,ySAAAW,CAAAtC,EAAA,k5BAYA,IAAM4hB,GAAc,EAXpB5gB,EAAAhB,EAAA,KAWoBc,SAAO+gB,EAAA/gB,QAAMghB,QAAhB1hB,WAAA,CAAAC,YAAA,wCAAAC,YAAA,eAAG,CAAH,6CAKIyhB,kVACuB,CACpCC,UAAU,EACVC,eAAgB,2BAgCF,SAAC1W,GACf,IAAMiN,EAAc3R,EAAKxD,MAAM6e,SAAS1W,QAAQ,CAACD,IACjD,OAAOrB,OAAOgD,OAAO,GAAIsL,EAAYrQ,eAAe,GAAIqQ,EAAY1P,IAAI,+BAGrD,SAACiN,GACpB,IAAKA,IAAUA,EAAMrP,OACjB,OAAO,KAEXG,EAAKsb,SAAS,CAAEH,UAAU,GAAQ,WAC9B,IAAMC,EAAiBG,KAAKC,UAAUxb,EAAK2E,QAAQuK,GAAQ,KAAM,GACjElP,EAAKsb,SAAS,CAAEF,kBAAkB,WAC9Bpb,EAAKsb,SAAS,CAAEH,UAAU,IAC1B,IAAMM,EAAcC,SAASC,eAAe,iBACxCF,GAAeA,EAAYvM,QAC3BuM,EAAYvM,MAAQ,kQAlDcpU,EAAMsB,kDAOpD,OACItB,EAAAC,cAAA,WACID,EAAAC,cAAA6gB,EAAA3hB,QAAA,CAAK8D,KAAK,OAAO8d,QAAQ,UACrB/gB,EAAAC,cAAA+gB,EAAA7hB,QAAA,CAAK8hB,KAAM,GAAIC,GAAI,CAAED,KAAM,KACvBjhB,EAAAC,cAAAkhB,EAAAhiB,QAAA,CAAMgE,MAAM,yBAAyBrB,MAAO,CAAEC,UAAW,SACrD/B,EAAAC,cAAA,SACID,EAAAC,cAACggB,EAAD,CACImB,YAAY,iCACZC,YAAY,OACZ3R,KAAK,QACL4R,SAAU7f,KAAK8f,aACfC,GAAG,gBACHnB,SAAU5e,KAAKggB,MAAMpB,YAG5B5e,KAAKC,MAAM2B,UAAY,OAGhCrD,EAAAC,cAAA+gB,EAAA7hB,QAAA,CAAK8hB,KAAM,GAAIC,GAAI,CAAED,KAAM,KACvBjhB,EAAAC,cAAAkhB,EAAAhiB,QAAA,CAAMgE,MAAM,mBAAmBrB,MAAO,CAAEC,UAAW,SAC/C/B,EAAAC,cAAA,OAAK6B,MAAO,CAAE4f,UAAW,QAAUjgB,KAAKggB,MAAMnB,gBAAkB,oKC5C5F,IAAAtgB,ySAAAW,CAAAtC,EAAA,IACAsjB,EAAAtiB,EAAAhB,EAAA,MACAujB,EAAAviB,EAAAhB,EAAA,MACAwjB,EAAAxiB,EAAAhB,EAAA,o2BAGqByjB,gaAAkB9hB,EAAMsB,kDACzB,IAAAygB,EACetgB,KAAKC,MAAxBqC,EADIge,EACJhe,KAAMpE,EADFoiB,EACEpiB,SACVqiB,EAA8B,GAQlC,OAPIje,IAASA,EAAKke,UAAYle,EAAKme,QAAQC,QACvCH,EAAeje,EAAKme,QAAQC,MAAM5c,IAAI,SAAC6c,GAAD,MAAgB,CAClDjf,MAAK,GAAA2S,OAAKsM,EAAKC,KAAK3K,KAAf,KAAA5B,OAAuBsM,EAAKC,KAAKC,WACtClO,MAAOgO,EAAKC,KAAKE,OAAOC,eAK5BxiB,EAAAC,cAAC2hB,EAAAziB,QAAD,CAAQQ,SAAUA,EAAUiC,YAAU,GAClC5B,EAAAC,cAAC0hB,EAAAxiB,QAAD,CAAQsjB,KAAMT,GAAe,SAACU,GAAD,OAAkC1iB,EAAAC,cAAC4hB,EAAA1iB,QAAsBujB,+DAMpF,oLC1BlBC,EAAAtjB,EAAAhB,EAAA,MAEA2B,EAAAW,EAAAtC,EAAA,IACAukB,EAAAjiB,EAAAtC,EAAA,MACAwkB,EAAAliB,EAAAtC,EAAA,MACAykB,EAAAniB,EAAAtC,EAAA,MACAqC,EAAArC,EAAA,IACA2S,EAAA3S,EAAA,KACA0kB,EAAA1kB,EAAA,KACA2kB,EAAAriB,EAAAtC,EAAA,MAEA4kB,EAAA5kB,EAAA,KACA4H,EAAA5G,EAAAhB,EAAA,8tEA2BA,IAAI6kB,EAAiB,KACjBC,EAAiB,KACjB,oBAAOC,SAEPF,EAAYhkB,EAAQ,KAAaC,QAEjCgkB,EAAYjkB,EAAQ,KAAmBC,aAGtBkkB,kVACY,CACzBjM,MAAO,KACPkM,QAAS,KACTC,eAAgB,EAChBC,gBAAiB,GACjBC,eAAgB,GAChBC,YAAY,EACZC,YAAY,EACZC,eAAe,EACfC,iBAAkB,8BAEE7jB,EAAM8jB,mCACL,0CACS,8BACT,qBACK,2CAEM,EAAA9S,EAAA+S,UAAS,WACzC,GAAK7e,EAAK8e,eAAene,OAAzB,CAMA,IAAMoe,EAAa/e,EAAKgf,iBAAiBhf,EAAK8e,gBAC1CG,EAAW,GAEXA,EADAF,GAAcA,EAAW7M,MACd,CAAEA,MAAO6M,EAAW7M,MAAOkM,QAAS,MACxCW,GAAcA,EAAWX,QACrB,CAAElM,MAAO,KAAMkM,QAASW,EAAWX,SAEnC,CAAElM,MAAO,KAAMkM,QAAS,MAEvCpe,EAAKsb,SAAS2D,EAAU,WACpBjf,EAAKkf,4BAfDlf,EAAKuc,MAAMrK,OAASlS,EAAKuc,MAAM6B,UAC/Bpe,EAAKsb,SAAS,CAAEpJ,MAAO,KAAMkM,QAAS,QAgB/C,yCAmG8B,WAC7B,OAAKH,EAGE,CACHnjB,EAAAC,cAAA,OAAK6B,MAAQ,CAAEU,QAAS,mBAAoBD,UAAW,UAAaQ,IAAI,cACpE/C,EAAAC,cAAAokB,EAAAllB,QAAA,CAAU8D,KAAK,SAASqhB,QAAUpf,EAAKuc,MAAMoC,iBAAmB/hB,MAAQ,CAAEyiB,aAAc,GAAIC,WAAY,MACxGxkB,EAAAC,cAAA,WACAD,EAAAC,cAAAwkB,EAAAtlB,QAAA,CAAQ8D,KAAK,UAAUyhB,QAAUxf,EAAKyf,uBAAyBtE,SAAWnb,EAAKuc,MAAMmC,eACjF5jB,EAAAC,cAAA+C,EAAA7D,QAAA,CAAM8D,KAAK,cAAcZ,MAAM,aADnC,mBAIArC,EAAAC,cAAC+iB,EAAG4B,WAAJ,4NAKJ5kB,EAAAC,cAAC+iB,EAAG6B,aAAJ,CAAiB9hB,IAAI,iBACjB/C,EAAAC,cAAC+iB,EAAG8B,kBAAJ,+CACA9kB,EAAAC,cAACkjB,EAAD,CACIrhB,MAAQ,CAAEU,QAAS,IACnBuiB,IAAM7f,EAAKuc,MAAM+B,gBACjBnhB,MAAM,QACN2iB,UAAU,SACVC,iBAAkB,EAClBC,kBAAmB,EACnBxN,MAAO,EACPyN,UAAY,MAzBb,mCA+BW,SAACC,EAAgBjT,GACvC,IACMkT,EAAangB,EAAKogB,UAAUnT,GAClC,OACInS,EAAAC,cAAC+iB,EAAGuC,UAAJ,CAAcC,OAAStgB,EAAKuc,MAAM8B,iBAAmBpR,EAAIpP,IAAG,OAAA+S,OAAU3D,GAAMuS,QAH9D,kBAAMxf,EAAKugB,UAAUtT,KAI7BiT,EAAEjiB,MACJnD,EAAAC,cAAA+C,EAAA7D,QAAA,CAAM8D,KAAK,QAAQZ,MAAM,WAAWqiB,QAAUW,iCAKlC,WACpBngB,EAAKsb,SAAS,CAAEkD,YAAY,EAAOF,gBAAiB,GAAIC,eAAgB,6BAGxD,WAChB,IAAIiC,EAAW,UACO,oBAAXtC,QAA0BA,OAAOuC,SACxCD,EAAWC,OAAO,2CAA4CD,IAAa,IAE3EA,IACAxgB,EAAKud,KAAK3T,KAAK,CAAE3L,MAAK,GAAA2S,OAAK4P,EAAL,YAAyBtR,MAAO,KACtDlP,EAAKugB,UAAUvgB,EAAKud,KAAK5c,OAAS,EAAG,WAC5BX,EAAK0gB,eAAkB1gB,EAAK0gB,cAAc7K,SAG/C7V,EAAK0gB,cAAc7K,QAAQ8K,SAAS,CAChCC,KAAM5gB,EAAK0gB,cAAc7K,QAAQgL,YACjCC,SAAU,2CAMD,WACrB,IAAK9gB,EAAKuc,MAAMiC,WAAY,CACxB,IAAIuC,GAAoB,EACxB,IACIA,EAAoB/gB,EAAKghB,uBAC3B,MAAOC,GACL,OAEAF,EACA/gB,EAAKsb,SAAS,CAAEkD,YAAY,EAAOC,YAAY,GAAQ,WAEnDyC,WAAWlhB,EAAKmhB,gBAAiB,OAGf,oBAAXjD,QAA0BA,OAAOkD,OACxClD,OAAOkD,MAAM,0FAMA,WACrB,oBAAOlD,QAA0BmD,cACjCA,aAAaC,QAAQ,OAAQ/F,KAAKC,UAAUxb,EAAKud,4CAGxB,SAAC1f,EAAa0jB,GAC3C,GAAI,oBAAOrD,QAA0BmD,aACjC,IACI,IAAMG,EAAOH,aAAaI,QAAQ5jB,GAClC,IAAK0jB,EACD,OAAOC,EAEX,GAAIA,EACA,IACI,OAAOjG,KAAKmG,MAAMF,GACpB,MAAOP,KAIf,MAAOA,GAEL3X,QAAQ4I,MAAM+O,uCAIK,WAC3B,GAAI,oBAAO/C,QAA0BmD,aAAc,CAC/C,IAAMM,EAAY3hB,EAAK4hB,uBAAuB,QAAQ,GACtD5hB,EAAKud,KAAOoE,GAAwB3hB,EAAKxD,MAAM+gB,UAE/Cvd,EAAKud,KAAOvd,EAAKxD,MAAM+gB,6BAGX,SAACtQ,EAAW4U,GACvB7hB,EAAK8hB,WAGV9hB,EAAKsb,SAAS,CAAE+C,eAAgBpR,GAAK,WACjCjN,EAAK8hB,UAAUC,WAAW/hB,EAAKud,KAAKvd,EAAKuc,MAAM8B,gBAAgBnP,OAC/DlP,EAAK8hB,UAAUE,gBACXH,GACAX,WAAWW,EAAI,+BAIP,SAAC5U,GACjB,OAAO,SAACgU,GAIJ,GAHIA,GACAA,EAAEgB,mBAEFjiB,EAAKud,KAAKtQ,GAAGiC,OACRgP,OAAOgE,QAAP,iCAAAtR,OAAgD5Q,EAAKud,KAAKtQ,GAAGhP,MAA7D,OADT,CAKA,IAAMkkB,EAAMniB,EAAKuc,MAAM8B,eACnB+D,EAAoBpiB,EAAKuc,MAAM8B,eAC/B8D,IAAQlV,GAAKkV,EAAM,IACnBC,EAAoBD,EAAM,GAE9BniB,EAAKud,KAALlR,EAAgBrM,EAAKud,KAAKpQ,MAAM,EAAGF,IAAnC2D,OAAAvE,EAA0CrM,EAAKud,KAAKpQ,MAAMF,EAAI,KACzDjN,EAAKud,KAAK5c,SACXX,EAAKud,KAAK3T,KAAK,CAAE3L,MAAO,kBAAmBiR,MAAO,KAClDkT,EAAoB,GAExBpiB,EAAKkf,qBACLlf,EAAKugB,UAAU6B,oCAGI,SAACC,GACxB,IACI,IACMC,EADM5E,EAAQ6E,cAAcF,GACF9hB,OAAO,SAAAiiB,GAAM,MAAoB,qBAAhBA,EAAOzkB,MAA+C,OAAhBykB,EAAOC,OAC9F,OAAIH,EAAoB3hB,OACb,CACHyd,QAAO,wEAAAxN,OACH0R,EAAoB,GAAGzkB,IADpB,yBAKR,KACT,MAAOojB,GAKL,MAAO,CAAE/O,MAHL+O,EAAEyB,cAAgBtkB,MACZ6iB,EAAE0B,WADR,GAAA/R,OAESqQ,EAAEzO,KAFX,MAAA5B,OAEoBqQ,EAAE2B,QAFtB,WAAAhS,OAEuCqQ,EAAExmB,SAASooB,MAAMC,KAFxD,cAAAlS,OAEyEqQ,EAAExmB,SAASooB,MAAME,6CAKvE,WAC3B,OAAQ/iB,EAAKud,KAAKyF,KAAK,SAACC,EAAKhW,GACzB,GAAIgW,EAAI/T,OAEe,OADAlP,EAAKgf,iBAAiBiE,EAAI/T,OAGzC,OADAlP,EAAKugB,UAAUtT,IACR,EAGf,OAAO,4DAIW,SAAAtK,IAAA,IAAA2b,EAAAC,EAAAxQ,EAAAC,EAAAC,EAAAoI,EAAAnI,EAAAC,EAAAmI,EAAA,OAAA7T,mBAAAI,KAAA,SAAAsB,GAAA,cAAAA,EAAApB,KAAAoB,EAAAnB,MAAA,OAClBsb,EAA8C,GAC9CC,EAA6C,GAF3BxQ,GAAA,EAAAC,GAAA,EAAAC,OAAAhF,EAAA9E,EAAApB,KAAA,EAAAsT,EAAA5T,mBAAAC,KAAA,SAAA2T,IAAA,IAAAjI,EAAAnB,EAAAgW,EAAAnf,EAAAof,EAAAC,EAAA,OAAA1gB,mBAAAI,KAAA,SAAAC,GAAA,cAAAA,EAAAC,KAAAD,EAAAE,MAAA,cAAAoL,EAAAa,EAAAd,EAAAe,MAAA,GAGVjC,EAHUmB,EAAA,GAGP6U,EAHO7U,EAAA,GAAAtL,EAAAC,KAAA,EAAAD,EAAAE,KAAA,EAKsB2a,EAAWyF,QAAQH,EAAI/T,MAAOoP,GALpD,OAAAxa,EAAAhB,EAAAsB,KAKNif,EALMvf,EAKNuf,SAAUC,EALJxf,EAKIwf,QAClBhF,EAAkB+E,EAClBzF,EAAM2F,UAAUhF,EAAgB+E,GAPlBxgB,EAAAE,KAAA,wBAAAF,EAAAC,KAAA,GAAAD,EAAAuB,GAAAvB,EAAA,SASd9C,EAAKsb,SAAS,CAAEgD,gBAAiB,GAAIC,eAAgB,GAAIC,YAAY,EAAOC,YAAY,GAAS,WAC7Fze,EAAKugB,UAAUtT,EAAG,kBACdjN,EAAKsb,SAAS,CAAEpJ,MAAOpP,EAAAuB,GAAEue,SAAW,WACV,oBAAX1E,QAA0BA,OAAOkD,OACxClD,OAAOkD,MAAP,qBAAAxQ,OAAkC9N,EAAAuB,GAAEue,gBAbtC9f,EAAA0B,OAAA,UAAAmS,OAAA,6BAAA7T,EAAAY,SAAA2S,EAAA9Z,KAAA,YAAA2R,EAGClO,EAAKud,KAAK1O,UAHXC,OAAAC,YAAA,UAAAhB,GAAAI,EAAAD,EAAAlL,QAAAgM,KAAA,CAAA7K,EAAAnB,KAAA,gBAAAmB,EAAAiT,cAAAf,IAAA,+BAAAgB,EAAAf,EAAAnS,EAAAE,IAAA,CAAAF,EAAAnB,KAAA,gBAAAmB,EAAAK,OAAA,SAAA8R,EAAAK,GAAA,QAAA5I,GAAA,EAAA5J,EAAAnB,KAAA,gBAAAmB,EAAAnB,KAAA,iBAAAmB,EAAApB,KAAA,GAAAoB,EAAAmT,GAAAnT,EAAA,SAAA6J,GAAA,EAAAC,EAAA9J,EAAAmT,GAAA,QAAAnT,EAAApB,KAAA,GAAAoB,EAAApB,KAAA,GAAAgL,GAAA,MAAAG,EAAA2C,QAAA3C,EAAA2C,SAAA,WAAA1M,EAAApB,KAAA,IAAAiL,EAAA,CAAA7J,EAAAnB,KAAA,eAAAiL,EAAA,eAAA9J,EAAA2M,OAAA,mBAAA3M,EAAA2M,OAAA,YAqBtB9Q,EAAKsb,SAAS,CAAEgD,kBAAiBC,iBAAgBE,YAAY,EAAOD,YAAY,IArB1D,yBAAAra,EAAAT,SAAAf,EAAApG,KAAA,qGAwBF,SAAAwH,EAAOyf,GAAP,IAAAC,EAAAC,EAAAC,EAAA,OAAAlhB,mBAAAI,KAAA,SAAAiC,GAAA,cAAAA,EAAA/B,KAAA+B,EAAA9B,MAAA,cAChBygB,EAAQ,EACRC,EAAW,EACf1jB,EAAKsb,SAAS,CAAEoD,eAAe,EAAMC,iBAAkB,IAHnC7Z,EAAA9B,KAAA,EAII4gB,QAAQC,IAC5BL,EAAMnjB,IAAI,SAAAyjB,GAAI,OACVC,UAAMC,IAAIF,EAAM,CACZG,mBAAoB,SAAAC,GAChB,IAAMC,EAAcD,EAAcE,iBAC5BF,EAAcT,MACdS,EAAcG,OAAOC,kBAAkB,mBACzCJ,EAAcG,OAAOC,kBAAkB,iCACvB,OAAhBH,IACAV,GAASU,EACTT,GAAYa,KAAKC,MAA8B,IAAvBN,EAAcO,OAAgBhB,IAE1DzjB,EAAKsb,SAAS,CAAEqD,iBAAkB+E,UAhB9B,cAIdC,EAJc7e,EAAAV,KAqBpBpE,EAAKsb,SAAS,CAAEoD,eAAe,EAAOC,iBAAkB,MArBpC7Z,EAAAN,OAAA,SAsBbmf,GAtBa,wBAAA7e,EAAApB,SAAAK,EAAAxH,6HAyBS,SAAAsI,IAAA,IAAA2e,EAAAkB,EAAAze,EAAAH,EAAA6e,EAAAC,EAAA7e,EAAAvC,EAAAwJ,EAAA6X,EAAA/S,EAAAC,EAAAwF,EAAA3I,EAAAD,EAAA4H,EAAA/Q,EAAA0L,EAAA4T,EAAAC,EAAAxhB,EAAA,OAAAd,mBAAAI,KAAA,SAAAiV,GAAA,cAAAA,EAAA/U,KAAA+U,EAAA9U,MAAA,cACvBwgB,EAAQ,EAAC,EAAAhoB,EAAAwpB,YAAW,4BAA4B,EAAAxpB,EAAAwpB,YAAW,wCADpClN,EAAA9U,KAAA,EAELhD,EAAKilB,cAAczB,GAFd,OAEvBkB,EAFuB5M,EAAA1T,KAGvB6B,EAAyB,IAAIgE,IAA0Bya,EAAU,GAAG7lB,MACpEiH,EAAsB4e,EAAU,GAAG7lB,KAJZ8lB,GAoBzB,EAAA5G,EAAAmH,yBAAwBllB,EAAKuc,MAAM+B,gBAAiBte,EAAKuc,MAAMgC,eAAgB7Y,UAAkB,MApBxEkf,EAAAD,EAMzBQ,WACIpf,EAPqB6e,EAOrB7e,oBACAvC,EARqBohB,EAQrBphB,UACAwJ,EATqB4X,EASrB5X,QACA6X,EAVqBD,EAUrBC,iBACA/S,EAXqB8S,EAWrB9S,MACAC,EAZqB6S,EAYrB7S,MACAwF,EAbqBqN,EAarBrN,OACA3I,EAdqBgW,EAcrBhW,OACAD,EAfqBiW,EAerBjW,OACA4H,EAhBqBqO,EAgBrBrO,QACA/Q,EAjBqBof,EAiBrBpf,SAEJ0L,EAnByByT,EAmBzBzT,MAEE4T,EAAkC,CAAElW,SAAQD,SAAQ4H,WACpDwO,EAAgC,CAAEjT,QAAOC,QAAOwF,UAChDhU,EAAgB,CAClBiC,WACAwH,UACA6X,mBACA9e,sBACAvC,aAGJ0d,WAAW,WACPlhB,EAAKsb,SAAS,CACVwJ,iBACAC,gBACAxhB,gBACA6hB,oCAAoC,EACpCtf,sBACAG,yBACAof,aAAcnU,KAEnB,KAzC0B,yBAAA4G,EAAApU,SAAAmB,EAAAtI,kQAxXDzB,EAAMsB,8DAwClCG,KAAK+oB,mEAGkB,IAAAja,EAAA9O,KACvB,GAAI,oBAAO2hB,QAA2BF,EAAtC,CACA,IAAMuH,EAAQ,IAAIvH,EAAU,kBAAmB,CAC3CxY,SAAU,UACVggB,aAAa,IAEjBD,EAAME,YAAY,UAAWC,gBACzBnpB,KAAKghB,MAAQhhB,KAAKghB,KAAKhhB,KAAKggB,MAAM8B,iBAClCkH,EAAMxD,WAAWxlB,KAAKghB,KAAKhhB,KAAKggB,MAAM8B,gBAAgBnP,OAE1DqW,EAAMI,SAAS,SAACC,GACPva,EAAKkS,MAASlS,EAAKkS,KAAKlS,EAAKkR,MAAM8B,kBAGxChT,EAAKyT,eAAiB8G,EACtBva,EAAKkS,KAAKlS,EAAKkR,MAAM8B,gBAAgBnP,MAAQ0W,EAEzCva,EAAKwa,wBAA0B,EAC/Bxa,EAAKwa,2BAELxa,EAAKiQ,SAAS,CAAEgD,gBAAiB,GAAIC,eAAgB,KACrDlT,EAAKya,gCAGbP,EAAMvD,gBACNzlB,KAAKulB,UAAYyD,oCAIjB,IAAM5c,EAAIpM,KAAKggB,MACf,IACK5T,EAAE8V,aACF9V,EAAE+V,eACH/V,EAAEyc,oCACFzc,EAAEpF,eACFoF,EAAEmc,gBACFnc,EAAEoc,eACFpc,EAAE0c,cACF9oB,KAAKC,MAAM2B,UACXwK,EAAE7C,qBACF6C,EAAE1C,uBAEF,OAAO1J,KAAKC,MAAM2B,SAAS,CACvBoF,cAAeoF,EAAEpF,cACjBX,aAAc+F,EAAEmc,eAChB9gB,YAAa2E,EAAEoc,cACfjf,oBAAqB6C,EAAE7C,oBACvBG,uBAAwB0C,EAAE1C,uBAC1Bof,aAAc1c,EAAE0c,eAGxB,IAAMU,EAAepd,EAAEuJ,MAAQ,QAAYvJ,EAAEyV,QAAU,UAAY,UAC7D4H,EAAUrd,EAAE8V,WAAa3jB,EAAAC,cAAA+C,EAAA7D,QAAA,CAAM8D,KAAK,UAAUZ,MAAM,aAAgB,KAC1E,OACIrC,EAAAC,cAAA,WACID,EAAAC,cAAA,sCACAD,EAAAC,cAAA,SACID,EAAAC,cAAA,KAAGiD,KAAK,6DAA6DqmB,OAAO,SAASpmB,MAAM,oBAA3F,WADJ,4RAQAnD,EAAAC,cAAC+iB,EAAGmI,cAAJ,KACInrB,EAAAC,cAAC+iB,EAAGoI,aAAJ,CAAiBtpB,MAAQ,CAAEupB,QAAS,QAAS9oB,UAAW,QAASC,QAAS,KACtExC,EAAAC,cAAAwkB,EAAAtlB,QAAA,CAAQulB,QAAUjjB,KAAK6pB,UAAYxpB,MAAQ,CAAEypB,YAAa,IAAOtoB,KAAK,UAClEjD,EAAAC,cAAA+C,EAAA7D,QAAA,CAAM8D,KAAK,OAAOZ,MAAM,aAD5B,gBAIArC,EAAAC,cAAAwkB,EAAAtlB,QAAA,CAAQ8D,KAAK,UAAUyhB,QAAUjjB,KAAK+pB,eAAiBnL,WAAaxS,EAAEuJ,OAClEpX,EAAAC,cAAA+C,EAAA7D,QAAA,CAAM8D,KAAK,cAAcZ,MAAM,aADnC,qBAKJrC,EAAAC,cAAC+iB,EAAGoI,aAAJ,KACIprB,EAAAC,cAAC+iB,EAAGyI,SAAJ,CAAaC,SAAWjqB,KAAKmkB,eAAkBnkB,KAAKghB,KAAKld,IAAI9D,KAAKkqB,mBAEtE3rB,EAAAC,cAAC+iB,EAAG4I,WAAJ,CAAepK,GAAG,mBAClBxhB,EAAAC,cAAC+iB,EAAG6I,kBAAJ,CAAsBpK,MAAQwJ,GAA9B,IAA8Cpd,EAAEuJ,OAASvJ,EAAEyV,SAAb,mBAC9CtjB,EAAAC,cAAC+iB,EAAG8I,cAAJ,CAAkBpH,QAAUjjB,KAAKsqB,cAAgBrI,WAAa7V,EAAE6V,YAAc7V,EAAE8V,YAC1EuH,EACFlrB,EAAAC,cAAC+iB,EAAGgJ,OAAJ,CAAWtH,QAAU,SAAAyB,GAAC,OAAIA,EAAEgB,mBAAoBzD,WAAa7V,EAAE6V,YAC3D1jB,EAAAC,cAAA+C,EAAA7D,QAAA,CAAM8D,KAAK,QAAQZ,MAAM,WAAWqiB,QAAUjjB,KAAKsqB,gBACjDtqB,KAAKwqB,sMC1KP,CACxBC,SAAU,CAAC,CAAEC,QAAS,UAAWC,QAAQ,GAAQ,CAAED,QAAS,qBAAsBC,QAAQ,IAC1FC,iBAAkB,CACd,CACIF,QAAS,0BACTG,OAAQ,CAAEC,gBAAiB,iBAE/B,CACIJ,QAAS,qCACTG,OAAQ,CAAEC,gBAAiB,kBAGnCC,eAAgB,CACZ,CACIL,QAAS,2BACTG,OAAQ,CAAEG,cAAe,iBAE7B,CACIN,QAAS,sCACTG,OAAQ,CAAEG,cAAe,kBAGjCC,KAAM,CAAEP,QAAS,oBAAqBC,QAAQ,GAC9CO,MAAO,CAAER,QAAS,mBAAoBC,QAAQ,GAC9CjtB,QAAS,CAAEgtB,QAAS,WAAYC,QAAQ,8PC7B5C,MAAAxrB,KAAAvC,EAAA,iCAEO,IAAMwtB,EAAoBttB,UAAOC,IAAVC,WAAA,CAAAC,YAAA,kCAAAC,YAAA,YAAGJ,CAAH,uIAEN,SAAAmB,GAAA,IAAG+hB,EAAH/hB,EAAG+hB,MAAH,MACN,UAAVA,EAAoB,UAAsB,YAAVA,EAAsB,UAAY,kCAW1E,IAGamK,EAAartB,UAAOC,IAAVC,WAAA,CAAAC,YAAA,2BAAAC,YAAA,YAAGJ,CAAH,w+BAFA,8BADC,oCA2EjB,IAAMgnB,EAAYhnB,UAAOC,IAAVC,WAAA,CAAAC,YAAA,0BAAAC,YAAA,YAAGJ,CAAH,kWAGE,SAAAyK,GAAA,OAAAA,EAAGwc,OA7EJ,UADC,0BAoGjB,IAAM4F,EAAe7sB,UAAOC,IAAVC,WAAA,CAAAC,YAAA,6BAAAC,YAAA,YAAGJ,CAAH,uHApGD,4BA8GjB,IAAMktB,EAAWltB,UAAOC,IAAVC,WAAA,CAAAC,YAAA,yBAAAC,YAAA,YAAGJ,CAAH,maAuBd,IAAM4sB,EAAgB5sB,UAAOC,IAAVC,WAAA,CAAAC,YAAA,8BAAAC,YAAA,YAAGJ,CAAH,+EAOnB,IAAMqmB,EAAarmB,UAAOgU,EAAV9T,WAAA,CAAAC,YAAA,2BAAAC,YAAA,YAAGJ,CAAH,8JAQhB,IAAMytB,EAASztB,UAAOC,IAAVC,WAAA,CAAAC,YAAA,uBAAAC,YAAA,YAAGJ,CAAH,6hBAWN,SAAAuL,GAAA,OAAAA,EAAG4Z,WAAqD,0BAqB9D,IAAMoI,EAAgBvtB,UAAOC,IAAVC,WAAA,CAAAC,YAAA,8BAAAC,YAAA,YAAGJ,CAAH,sVAQR,SAAAquB,GAAA,OAAAA,EAAGlJ,WAAwD,UAAY,6BAelF,IAAMmB,EAAetmB,UAAOC,IAAVC,WAAA,CAAAC,YAAA,6BAAAC,YAAA,YAAGJ,CAAH,wPAWlB,IAAMumB,EAAoBvmB,UAAOC,IAAVC,WAAA,CAAAC,YAAA,kCAAAC,YAAA,aAAGJ,CAAH,uQChOvB,SACHgqB,EACAC,EACA/d,EACAC,GAEA,IAAMmiB,EAAgC,CAClC3a,QAAS3J,OAAOC,KAAK+f,GACrBwB,iBAAkB,GAClBrf,WACAO,oBAAqB,EACrBvC,UAAW,CAAEokB,EAAG,GAChB9V,MAAO,GACPC,MAAO,GACPwF,OAAQ,GACR3I,OAAQ,GACRD,OAAQ,GACR4H,QAAS,IAEPsR,EAAgC,IAAIlT,MAAMgT,EAAI3a,QAAQrM,QAAQiU,KAAK,GACnEkT,EAA+B,IAAInT,MAAMgT,EAAI3a,QAAQrM,QAAQiU,KAAK,GACpEmT,EAAuG,GACvGC,EAAsG,GACpGC,EAAsB,IAAIC,IAChCP,EAAI3a,QAAQlE,QAAQ,SAAA0E,GAChB,IAAI2a,GAAgB,EACdC,EAAuB,SAACC,GAC1B,IAAMC,EAAqBD,EAAehoB,IAAI,SAAAkoB,GAC1C,IAAM1nB,EAAQ0E,EAAUqD,qBAAqB2f,EAAMrZ,OAC7CsZ,EAAgC,IAAI7T,MAAM9T,EAAMF,QAAQiU,KAAK,GAC7D6T,EAAWF,EAAMf,KACvB,GAAIiB,EAAU,CACVR,EAAoBpd,IAAI2C,GACnB2a,IACDA,GAAgB,GAEpB,IAAMO,EAAcD,OACexf,IAA/B0e,EAAInkB,UAAUklB,KACdf,EAAInkB,UAAUklB,GAAerlB,OAAOC,KAAKqkB,EAAInkB,WAAW7C,QAE5DE,EAAMiI,QAAQ,SAACxI,EAAG2M,GACdub,EAAoBvb,GAAK0a,EAAInkB,UAAUklB,KAG/C,OAAOF,IAELG,GAAkB,EAAA7c,EAAA4H,SAAQ4U,GAC1B1oB,EAAWyoB,EAAehoB,IAAI,SAAAyM,GAAC,OAAIA,EAAEoC,QAAOpO,KAAK,IACjD2U,EAAgBlQ,EAAUqD,qBAAqBhJ,GACjD+nB,EAAI5hB,oBAAsB0P,EAAc9U,SACxCgnB,EAAI5hB,oBAAsB0P,EAAc9U,QAE5C,IAAMioB,EAAWjB,EAAI3a,QAAQ6b,UAAU,SAAAlf,GAAC,OAAIA,IAAM6D,IAClD,MAAO,CAAE5N,WAAUgpB,WAAUD,oBAE3BG,EAA2BzF,EAAS7V,GAAQnN,IAAI+nB,GACtDL,EAA6BA,EAA2BnX,OAAOkY,GAC/D,IAAMC,EAAkCzF,EAAQ9V,GAAQnN,IAAI+nB,GAC5DJ,EAA4BA,EAA0BpX,OAAOmY,KAEjEpB,EAAI9C,iBAAJxY,EAA2B4b,IAC3B,EAAAnc,EAAA2D,SAAQsY,GAA4Bjf,QAAQ,SAAAH,GACxCkf,EAAoBlf,EAAEigB,YACtBjB,EAAI/Y,OAAOhF,KAAKjB,EAAE/I,UAClB+nB,EAAIhZ,OAAO/E,KAAKjB,EAAEigB,UAClBjB,EAAIpR,QAAQ3M,KAAKjB,EAAEggB,oBAEvB,EAAA7c,EAAA2D,SAAQuY,GAA2Blf,QAAQ,SAAAH,GACvCmf,EAAmBnf,EAAEigB,YACrBjB,EAAI7V,MAAMlI,KAAKjB,EAAE/I,UACjB+nB,EAAI5V,MAAMnI,KAAKjB,EAAEigB,UACjBjB,EAAIpQ,OAAO3N,KAAKjB,EAAEggB,mBAEtB,IAAMzX,EAAQyW,EAAI3a,QAAQ3M,IAAI,SAAC2oB,EAAWtoB,GAAZ,MAAuB,CACjD8M,OAAQwb,EACR1F,QAASwE,EAAmBpnB,GAC5B2iB,SAAUwE,EAAoBnnB,MAElC,MAAO,CAAEykB,WAAYwC,EAAKzW,4BAGvB,SAAyB+X,GAC5B,IAAMC,EAAkB,CACpBC,gBAAiB,GACjBC,WAAY,IAAInf,KAMpB,OAJAif,EAAgBE,WAAa,IAAInf,IAAIgf,GACrC5c,EAAI6c,EAAgBE,WAAW9lB,QAAQwF,QAAQ,SAACtI,EAAM8b,GAClD4M,EAAgBC,gBAAgB3oB,GAAQ8b,IAErC4M,GA9FX,IAAApd,EAAA3S,EAAA,kjBCFAmO,EAAA7L,EAAAtC,EAAA,KAEA2B,EAAAW,EAAAtC,EAAA,IACAuC,EAAAvB,EAAAhB,EAAA,KACAkwB,EAAAlwB,EAAA,KAEAmwB,EAAAnvB,EAAAhB,EAAA,MACAowB,EAAApvB,EAAAhB,EAAA,u/CAEA,IAAMqwB,EAAsB,CACxB,qGACA,kHACA,oIACA,qHACA,sGAEEC,EAAapwB,UAAOC,IAAVC,WAAA,CAAAC,YAAA,gCAAAC,YAAA,YAAGJ,CAAH,yNAgCKqwB,kVACuB,CACpCC,YAAa,EACbC,gBAAiB,EACjBC,0BAA0B,EAC1BC,MAAM,EACNC,qBAAsB,GACtBC,UAAW,0BAEyB,2CACsB,8BACX,8CAmCX,WACpC,OAA+B,IAA3BhqB,EAAKuc,MAAMoN,aAAqB3pB,EAAKqb,SAEjCvgB,EAAAC,cAAAD,EAAAmvB,SAAA,KACInvB,EAAAC,cAAAmvB,EAAAjwB,QAAA,CACI2oB,QAAQ,oFACR7kB,KAAK,UACLnB,MAAO,CAAEyiB,aAAc,MAE3BvkB,EAAAC,cAACwuB,EAAAtvB,QAAD,CAA0BohB,SAAUrb,EAAKqb,UACrCvgB,EAAAC,cAAA,wEACAD,EAAAC,cAAA,UACKiF,EAAKxD,MAAM6oB,aAAahlB,IAAI,SAACsI,EAAGsE,GAAJ,OACzBnS,EAAAC,cAAA,MAAI8C,IAAG,OAAA+S,OAAS3D,IACZnS,EAAAC,cAAA,cAAS4N,EAAE6E,QADf,KACkC7E,EAAE0a,SADpC,iBAC4D1a,EAAE2a,QAD9D,4BASjB,+BAGW,SAACrlB,EAAeqO,EAA8B4E,GAChE,IAAKA,EAAMvQ,SAAW2L,EAAO3L,OACzB,OACI7F,EAAAC,cAAAkhB,EAAAhiB,QAAA,CAAMgE,MAAOA,EAAOrB,MAAO,CAAEC,UAAW,SACpC/B,EAAAC,cAAA,wBAIZ,IAAMovB,EAAS,SAAAvZ,OAAYtE,EAAO3L,OAAnB,QAAAiQ,OAAgCM,EAAMA,EAAMvQ,OAAS,GAAGyP,cACjEga,EAAYpqB,EAAKuc,MAAMuN,KAAOhvB,EAAAC,cAACuuB,EAAArvB,QAAD,CAAWowB,WAAY/d,IAAa,KACxE,OACIxR,EAAAC,cAAAkhB,EAAAhiB,QAAA,CAAMgE,MAAOA,EAAOqsB,MAAOH,EAAWvtB,MAAO,CAAEC,UAAW,SACrDutB,EACDtvB,EAAAC,cAAA,KAAG6B,MAAO,CAAEsB,SAAU,SAClBpD,EAAAC,cAAA,iCADJ,IACsCmW,EAAMA,EAAMvQ,OAAS,GAAG0P,iBAC1DvV,EAAAC,cAAA,WACAD,EAAAC,cAAA,sCAHJ,IAG2CmW,EAAMA,EAAMvQ,OAAS,GAAG8P,mBAC/D3V,EAAAC,cAAA,WACAD,EAAAC,cAAA,6BALJ,IAKkCmW,EAAMA,EAAMvQ,OAAS,GAAG4P,aACtDzV,EAAAC,cAAA,WACAD,EAAAC,cAAA,kCAPJ,IAOuCmW,EAAMA,EAAMvQ,OAAS,GAAGgQ,eAC3D7V,EAAAC,cAAA,6CAMY,iBAAiC,CACzDuG,eAAgB,SAAA4P,GACZlR,EAAKuqB,yBAAyB3gB,KAAKsH,GADd,IAEbrB,EAAwCqB,EAAxCrB,MAAOc,EAAiCO,EAAjCP,eAAgBJ,EAAiBW,EAAjBX,aAC/BvQ,EAAKsb,SAAS,SAAA3S,GAAC,MAAK,CAChBohB,qBAAsBphB,EAAEohB,qBAAqBnZ,OAAO,CAAC,CAAEf,QAAOc,iBAAgBJ,sBAGtFtO,IAAK,SAAAiP,GACDlR,EAAKwqB,cAAc5gB,KAAKsH,GADd,IAEFrB,EAAwCqB,EAAxCrB,MAAOc,EAAiCO,EAAjCP,eAAgBJ,EAAiBW,EAAjBX,aAC/BvQ,EAAKsb,SAAS,SAAA3S,GAAC,MAAK,CAChBghB,YAA+B,IAAlBhhB,EAAEghB,YAAoB,EAAIhhB,EAAEghB,YACzCK,UAAWrhB,EAAEqhB,UAAUpZ,OAAO,CAAC,CAAEf,QAAOc,iBAAgBJ,sFAKnC,SAAA5N,IAAA,IAAA8nB,EAAAxnB,EAAAiO,EAAA,OAAAzO,mBAAAI,KAAA,SAAAC,GAAA,cAAAA,EAAAC,KAAAD,EAAAE,MAAA,UAKvBC,EAAqC,CACvCgP,MAAO,kBAAM,MACbC,MANEuY,EAAa,WACfjB,EAAU5f,KAAVhG,MAAA4lB,EAAS3lB,WACT7D,EAAKsb,SAAS,SAAAvY,GAAI,MAAK,CAAE6mB,gBAAiB7mB,EAAK6mB,gBAAkB,MAKjE1mB,IAAKunB,EACLlhB,KAAMkhB,GAEVzqB,EAAKqb,SAAW,IAAIjZ,eAAa,CAC7BmB,cAAevD,EAAKxD,MAAM+G,cAC1BN,SACA6C,oBAAqB9F,EAAKxD,MAAMsJ,oBAChCG,uBAAwBjG,EAAKxD,MAAMyJ,uBACnCK,kBAAmBtG,EAAKsG,sBAEvBtG,EAAKqb,SAlBmB,CAAAvY,EAAAE,KAAA,eAAAF,EAAA0B,OAAA,SAmBlB,MAnBkB,cAAA1B,EAAAE,KAAA,EAqBvBhD,EAAKqb,SAASjY,MAAMpD,EAAKxD,MAAMoG,cArBR,cAsB7B5C,EAAKsb,SAAS,CAAEqO,YAAa,IAtBA7mB,EAAAE,KAAA,GAuBThD,EAAKqb,SAAS/e,KAAK0D,EAAKxD,MAAMwH,aAvBrB,eAuBvBkN,EAvBuBpO,EAAAsB,KAwB7BpE,EAAKsb,SAAS,CAAEqO,YAAa,IAC7B1mB,EAAOC,IAAI,sGACXD,EAAOC,IAAI,uBACXD,EAAOC,IAAIqY,KAAKC,UAAUtK,EAAO,KAAM,IACvCjO,EAAOC,IAAI,sGA5BkBJ,EAAAE,KAAA,GA6BvBhD,EAAKqb,SAASrW,KAAK,CACrBC,mBAAoB,6BACpBG,cAAe,wBACfF,QAAS,oBAhCgB,QAkC7BlF,EAAKsb,SAAS,CAAEwO,MAAM,IAlCO,yBAAAhnB,EAAAY,SAAAf,EAAApG,kQApHUzB,EAAMsB,6DAc7CG,KAAKkjB,wEAILljB,KAAK8e,SAAW,KAEhB/R,QAAQpG,IAAIoE,EAAG4I,UACf5I,EAAGojB,oDAIH,OACI5vB,EAAAC,cAAA,WACID,EAAAC,cAAA4vB,EAAA1wB,QAAA,CAAOuQ,KAAK,QAAQqL,QAAStZ,KAAKggB,MAAMoN,YAAa/sB,MAAO,CAAEyiB,aAAc,SACxEvkB,EAAAC,cAAA4vB,EAAA1wB,QAAO2wB,KAAP,CAAY3sB,MAAM,+BAClBnD,EAAAC,cAAA4vB,EAAA1wB,QAAO2wB,KAAP,CAAY3sB,MAAM,oBAClBnD,EAAAC,cAAA4vB,EAAA1wB,QAAO2wB,KAAP,CAAY3sB,MAAM,gBAClBnD,EAAAC,cAAA4vB,EAAA1wB,QAAO2wB,KAAP,CAAY3sB,MAAM,sBAErB1B,KAAKsuB,gCACN/vB,EAAAC,cAAA,MAAI6B,MAAO,CAAE4f,UAAW,SAAxB,kBACA1hB,EAAAC,cAAC0uB,EAAD,KAAaD,EAAU1oB,KAAK,OAC5BhG,EAAAC,cAAA6gB,EAAA3hB,QAAA,CAAK8D,KAAK,OAAO8d,QAAQ,UACrB/gB,EAAAC,cAAA+gB,EAAA7hB,QAAA,CAAK8hB,KAAM,IACNxf,KAAKuuB,YAAY,6BAA8BvuB,KAAKggB,MAAMwN,qBAAsBxtB,KAAKguB,2BAE1FzvB,EAAAC,cAAA+gB,EAAA7hB,QAAA,CAAK8hB,KAAM,IAAKxf,KAAKuuB,YAAY,kBAAmBvuB,KAAKggB,MAAMyN,UAAWztB,KAAKiuB,uKCxFnG,IAAA1vB,ySAAAW,CAAAtC,EAAA,+yBAIqB4xB,gaAAsBjwB,EAAMsB,kDAEzC,OAAOtB,EAAAC,cAAA","file":"component---web-pages-train-tsx-29d7133bf7b6f2818f06.js","sourcesContent":["import '@babel/polyfill';\nimport styled from 'styled-components';\n\n// The Button from the last section without the interpolations\nexport const ColoredText = styled.div`\n    font-weight: bold;\n    color: #1890ff;\n    display: inline-block;\n    text-decoration: none;\n    background-image: linear-gradient(to right, #1890ff 25%, #c4ce35 50%, #ac24e2 75%, #1890ff 100%);\n    -webkit-text-fill-color: transparent;\n    -webkit-background-clip: text;\n    background-clip: text;\n    background-size: 300% auto;\n    &.static {\n        background-position: -215% center;\n    }\n    &.animated {\n        @keyframes text-gradient {\n            to {\n                background-position: -300% center;\n            }\n        }\n        animation: text-gradient 16s ease-in-out infinite;\n    }\n`;\n\nexport const Logo = ColoredText.extend`\n    text-align: center;\n    font-size: 19px;\n`;\n","const preferDefault = m => (m && m.default) || m\n\nif (process.env.BUILD_STAGE === `develop`) {\n  module.exports = preferDefault(require(`./public-page-renderer-dev`))\n} else if (process.env.BUILD_STAGE === `build-javascript`) {\n  module.exports = preferDefault(require(`./public-page-renderer-prod`))\n} else {\n  module.exports = () => null\n}\n","import React from \"react\"\nimport PropTypes from \"prop-types\"\n\nimport InternalPageRenderer from \"./page-renderer\"\nimport loader from \"./loader\"\n\nconst ProdPageRenderer = ({ location }) => {\n  const pageResources = loader.getResourcesForPathnameSync(location.pathname)\n  return React.createElement(InternalPageRenderer, {\n    location,\n    pageResources,\n    ...pageResources.json,\n  })\n}\n\nProdPageRenderer.propTypes = {\n  location: PropTypes.shape({\n    pathname: PropTypes.string.isRequired,\n  }).isRequired,\n}\n\nexport default ProdPageRenderer\n","import '@babel/polyfill';\nimport { Icon, Layout, Menu } from 'antd';\nimport Link from 'gatsby-link';\nimport * as React from 'react';\nimport styled from 'styled-components';\nimport { Logo } from './Logo';\n\nconst { Content, Footer } = Layout;\n\nconst overviewRouteRE = /^\\/overview(\\/.*)?$/i;\nconst demoRouteRE = /^\\/demo(\\/.*)?$/i;\nconst trainRouteRE = /^\\/train(\\/.*)?$/i;\n\nexport const InnerContent = styled(Content)`\n    > p {\n        text-align: justify;\n    }\n    background: #fcfcfc;\n    min-height: '95vh';\n`;\n\nexport const InnerPaddedContent = styled(InnerContent)`\n    padding: 28px 28px 28px 52px;\n`;\n\nexport default class MainLayout extends React.Component<{ location: { pathname: string }; addPadding?: boolean }, {}> {\n    public render() {\n        let defaultSelectedKeys = '-1';\n        if (overviewRouteRE.test(this.props.location.pathname)) {\n            defaultSelectedKeys = '0';\n        } else if (trainRouteRE.test(this.props.location.pathname)) {\n            defaultSelectedKeys = '1';\n        } else if (demoRouteRE.test(this.props.location.pathname)) {\n            defaultSelectedKeys = '2';\n        }\n        const IC = this.props.addPadding ? InnerPaddedContent : InnerContent;\n        return (\n            <Layout style={{ minHeight: '100vh' }}>\n                <Layout style={{ flexDirection: 'row' }}>\n                    <Layout.Sider width={200} breakpoint=\"lg\" collapsedWidth=\"0\" theme=\"light\" style={{ backgroundColor: '#fcfcfc' }}>\n                        <Logo style={{ textAlign: 'center', width: 200, padding: 24 }} className=\"static\">\n                            <Link to=\"/\">{`< Aida />`}</Link>\n                        </Logo>\n                        <Menu theme=\"light\" mode=\"inline\" defaultSelectedKeys={[defaultSelectedKeys]} style={{ background: '#fcfcfc' }}>\n                            <Menu.Item key=\"0\">\n                                <Link to=\"/overview\">\n                                    <Icon type=\"right-circle-o\" />\n                                    Overview\n                                </Link>\n                            </Menu.Item>\n                            <Menu.Item key=\"1\">\n                                <Link to=\"/train\">\n                                    <Icon type=\"right-circle-o\" />\n                                    Train assistant\n                                </Link>\n                            </Menu.Item>\n                            <Menu.Item key=\"2\">\n                                <Link to=\"/demo\">\n                                    <Icon type=\"right-circle-o\" />\n                                    Demo\n                                </Link>\n                            </Menu.Item>\n                        </Menu>\n                        <div style={{ padding: '24px', textAlign: 'center' }}>\n                            <a href=\"https://github.com/rodrigopivi/aida\" title=\"Aida\" style={{ fontSize: 26 }}>\n                                <Icon type=\"github\" />\n                            </a>\n                        </div>\n                    </Layout.Sider>\n                    <Layout style={{ padding: '24px 0 0 24px' }}>\n                        <IC>{this.props.children}</IC>\n                        <Footer style={{ textAlign: 'center' }}>Aida © 2018 Rodrigo Pimentel</Footer>\n                    </Layout>\n                </Layout>\n            </Layout>\n        );\n    }\n}\n","import React from \"react\"\nimport PropTypes from \"prop-types\"\nimport Link, {\n  withPrefix,\n  navigate,\n  push,\n  replace,\n  navigateTo,\n} from \"gatsby-link\"\nimport { waitForRouteChange } from \"./wait-for-route-change\"\nimport PageRenderer from \"./public-page-renderer\"\nimport parsePath from \"./parse-path\"\n\nconst StaticQueryContext = React.createContext({})\n\nconst StaticQuery = props => (\n  <StaticQueryContext.Consumer>\n    {staticQueryData => {\n      if (\n        props.data ||\n        (staticQueryData[props.query] && staticQueryData[props.query].data)\n      ) {\n        return (props.render || props.children)(\n          props.data ? props.data.data : staticQueryData[props.query].data\n        )\n      } else {\n        return <div>Loading (StaticQuery)</div>\n      }\n    }}\n  </StaticQueryContext.Consumer>\n)\n\nStaticQuery.propTypes = {\n  data: PropTypes.object,\n  query: PropTypes.string.isRequired,\n  render: PropTypes.func,\n  children: PropTypes.func,\n}\n\nfunction graphql() {\n  throw new Error(\n    `It appears like Gatsby is misconfigured. Gatsby related \\`graphql\\` calls ` +\n      `are supposed to only be evaluated at compile time, and then compiled away,. ` +\n      `Unfortunately, something went wrong and the query was left in the compiled code.\\n\\n.` +\n      `Unless your site has a complex or custom babel/Gatsby configuration this is likely a bug in Gatsby.`\n  )\n}\n\nexport {\n  Link,\n  withPrefix,\n  graphql,\n  parsePath,\n  navigate,\n  push, // TODO replace for v3\n  replace, // TODO remove replace for v3\n  navigateTo, // TODO: remove navigateTo for v3\n  StaticQueryContext,\n  StaticQuery,\n  PageRenderer,\n  waitForRouteChange,\n}\n","import { IAidaTokenizer } from '../../types';\n\n// TODO: use fancier tokenizer, better split of words, better joining of sentences\nclass EnglishTokenizer implements IAidaTokenizer {\n    // list of valid characters used for the dictionary, any word with a char that is not listed here will be skip\n    public FILTER_CHARS_REGEXP = /[^a-z0-9\\.,\\?\\'\"!@#\\$%\\^&\\*\\(\\)-_=\\+;:<>\\/\\\\\\|\\}\\{\\[\\]`~ ]/gi;\n    // list of charaters that act as word splitters (includes space)\n    public WORD_SEPARATORS_REGEXP = /([\\ \\.\\,\\%\\*\\-\\=\\+\\;\\|\\`\\~])/g;\n    // regexp that detect if a word contains non alphanumeric characters\n    public NON_ALPHANUMERIC_REGEXP = /[^a-z0-9]/gi;\n    // when ngram is unkown, replace it with this string listed at the dictionary\n    public UNKNOWN_NGRAM_KEY = '__';\n    // fastText doesn't contain numbers, so we use the sane embeddings for the number words\n    public NUMBERS_MAP: { [key: string]: string } = {\n        // tslint:disable:object-literal-sort-keys\n        zero: '0',\n        one: '1',\n        two: '2',\n        three: '3',\n        four: '4',\n        five: '5',\n        six: '6',\n        seven: '7',\n        eight: '8',\n        nine: '9'\n        // tslint:enable:object-literal-sort-keys\n    };\n\n    public sanitizeSentence = (sentence: string): string => {\n        return sentence\n            .trim()\n            .toLowerCase()\n            .replace(this.FILTER_CHARS_REGEXP, '');\n    };\n\n    public splitSentenceToWords = (sentence: string): string[] => {\n        return this.sanitizeSentence(sentence)\n            .split(this.WORD_SEPARATORS_REGEXP)\n            .map(w => w.trim())\n            .filter(w => !!w);\n    };\n\n    public splitWordToBiGrams = (word: string): string[] => {\n        const ngram = 2;\n        const grams: string[] = [];\n        let index = word.length - ngram + 1;\n        if (index < 1) {\n            return grams;\n        }\n        while (index--) {\n            grams[index] = word.substr(index, ngram);\n        }\n        return grams;\n    };\n    public joinWordsToSentence = (words: string[]): string => words.join(' ');\n}\nexport default new EnglishTokenizer();\n","import * as tf from '@tensorflow/tfjs';\nimport englishTokenizer from '../../languages/en/EnglishTokenizer';\nimport spanishTokenizer from '../../languages/es/SpanishTokenizer';\nimport * as types from '../../types';\nimport { EmbeddingsModel } from './embeddings/EmbeddingsModel';\nimport ClassificationModel from './models/classification';\nimport NerModel from './models/ner';\n\nfunction getTokenizer(language: 'en' | 'es') {\n    const lang = language ? language.toLowerCase() : language;\n    if (lang === 'en') {\n        return englishTokenizer;\n    } else if (lang === 'es') {\n        return spanishTokenizer;\n    }\n    throw new Error(\"Invalid config language. Only 'en' and 'es' are supported.\");\n}\n\nexport const defaultPipelineDefinition: types.IPipelineDefinition = {\n    config: {\n        classification: {\n            epochs: 5,\n            filterSizes: [2, 4, 8],\n            lowConfidenceThreshold: 0.3,\n            numFilters: 128\n        },\n        default: {\n            // NOTE Using batch size of 50 because on windows higher batch sizes tend to exit with\n            // lost context error, on a Mac a batchSize of 70-100 works just fine and trains faster.\n            // Reference TF.js issue: https://github.com/tensorflow/tfjs/issues/263\n            batchSize: 120,\n            drop: 0.5,\n            embeddingDimensions: 300,\n            lossThresholdToStopTraining: 1e-6,\n            maxNgrams: 20,\n            trainingValidationSplit: 0.3\n        },\n        ner: {\n            addAttention: true,\n            epochs: 5,\n            lowConfidenceThreshold: 0.2,\n            numFilters: [128, 128],\n            rnnUnits: 100\n        }\n    }\n};\nexport class AidaPipeline {\n    private pipelineDefinition: types.IPipelineDefinition = defaultPipelineDefinition;\n    private datasetParams: types.IDatasetParams;\n    private embeddingsModel: EmbeddingsModel;\n    private classificationModel: ClassificationModel;\n    private nerModel: NerModel;\n    private logger: types.IPipelineModelLogger;\n    private tokenizer: types.IAidaTokenizer;\n\n    constructor(cfg: {\n        datasetParams: types.IDatasetParams;\n        logger: types.IPipelineModelLogger;\n        ngramToIdDictionary: { [key: string]: number };\n        trainStatsHandler?: types.ITrainStatsHandler;\n        pipelineDefinition?: types.IPipelineDefinition;\n        pretrainedClassifier?: tf.Model;\n        pretrainedNer?: tf.Model;\n        pretrainedEmbedding?: tf.Model;\n        pretrainedNGramVectors?: types.PretrainedDict;\n    }) {\n        if (cfg.pipelineDefinition) {\n            this.pipelineDefinition = cfg.pipelineDefinition;\n        }\n        this.datasetParams = cfg.datasetParams;\n        this.logger = cfg.logger;\n        this.tokenizer = getTokenizer(this.datasetParams.language);\n        this.embeddingsModel = new EmbeddingsModel(\n            cfg.ngramToIdDictionary,\n            cfg.datasetParams.maxWordsPerSentence,\n            this.pipelineDefinition.config.default.maxNgrams,\n            this.pipelineDefinition.config.default.embeddingDimensions,\n            this.tokenizer,\n            cfg.pretrainedEmbedding,\n            cfg.pretrainedNGramVectors\n        );\n        const classificationCfg = Object.assign({}, this.pipelineDefinition.config.default, this.pipelineDefinition.config.classification);\n        let classificationTrainStatsHandler;\n        let nerTrainStatsHandler;\n        if (cfg.trainStatsHandler) {\n            classificationTrainStatsHandler = cfg.trainStatsHandler.classification;\n            nerTrainStatsHandler = cfg.trainStatsHandler.ner;\n        }\n        this.classificationModel = new ClassificationModel(\n            classificationCfg,\n            this.datasetParams,\n            this.embeddingsModel,\n            this.logger,\n            cfg.pretrainedClassifier,\n            classificationTrainStatsHandler\n        );\n        const nerCfg = Object.assign({}, this.pipelineDefinition.config.default, this.pipelineDefinition.config.ner);\n        this.nerModel = new NerModel(\n            nerCfg,\n            this.datasetParams,\n            this.embeddingsModel,\n            this.logger,\n            cfg.pretrainedNer,\n            nerTrainStatsHandler\n        );\n    }\n\n    public train = async (trainDataset: types.ITrainingParams): Promise<void> => {\n        this.logger.log('START TRAINING PIPELINE MODELS!');\n        this.logger.log('==================================================================================================');\n        await this.classificationModel.train(trainDataset);\n        const slotsLength = Object.keys(this.datasetParams.slotsToId).length;\n        // NOTE: only train the ner model if there are slots in the training params\n        if (slotsLength >= 2) {\n            await this.nerModel.train(trainDataset);\n        } else {\n            this.logger.log('SKIPPING NER MODEL (NO SLOTS FOUND)!');\n            this.logger.log('==================================================================================================');\n        }\n        this.logger.log('FINISHED TRAINING PIPELINE MODELS!');\n        this.logger.log('==================================================================================================');\n    };\n\n    public test = async (testDataset: types.ITestingParams) => {\n        this.logger.log('START TESTING PIPELINE MODELS!');\n        this.logger.log('==================================================================================================');\n        const classificationStats = await this.classificationModel.test(testDataset);\n        // NOTE: only use the ner model if there are slots in the training params\n        const slotsLength = Object.keys(this.datasetParams.slotsToId).length;\n        const nerStats = slotsLength >= 2 ? await this.nerModel.test(testDataset) : { correct: 0, wrong: 0 };\n        return { classificationStats, nerStats };\n    };\n\n    public predict = (sentences: string[]) => {\n        const classification = this.classificationModel.predict(sentences);\n        // NOTE: only use the ner model if there are slots in the training params\n        const slotsLength = Object.keys(this.datasetParams.slotsToId).length;\n        const ner: types.ISlotReducer[] = slotsLength >= 2 ? this.nerModel.predict(sentences, classification) : [];\n        return { classification, ner };\n    };\n\n    public save = async (cfg: { classificationPath: string; nerPath: string; embeddingPath: string }) => {\n        this.logger.log('SAVING PIPELINE MODELS!');\n        this.logger.log('==================================================================================================');\n        await this.classificationModel.tfModel().save(cfg.classificationPath);\n        const slotsLength = Object.keys(this.datasetParams.slotsToId).length;\n        // NOTE: only download the ner model if there are slots\n        if (slotsLength >= 2) {\n            await this.nerModel.tfModel().save(cfg.nerPath);\n        }\n        await this.embeddingsModel.tfModel().save(cfg.embeddingPath);\n    };\n}\n","import { IAidaTokenizer } from '../../types';\n\n// TODO: use fancier tokenizer, better split of words, better joining of sentences\nclass SpanishTokenizer implements IAidaTokenizer {\n    // list of valid characters used for the dictionary, any word with a char that is not listed here will be skip\n    public FILTER_CHARS_REGEXP = /[^aábcdeéfghijklmnñoópqrstuúüvwxyzAÁBCDEÉFGHIJKLMNÑOÓPQRSTUÚÜVWXYZ 0-9\\.,\\?\\'\"!@#\\$%\\^&\\*\\(\\)-_=\\+;:<>\\/\\\\\\|\\}\\{\\[\\]`~]/g;\n    // list of charaters that act as word splitters (includes space)\n    public WORD_SEPARATORS_REGEXP = /([\\ \\.\\,\\%\\*\\-\\=\\+\\;\\|\\`\\~])/g;\n    // regexp that detect if a word contains non alphanumeric characters\n    public NON_ALPHANUMERIC_REGEXP = /[^aábcdeéfghijklmnñoópqrstuúüvwxyzAÁBCDEÉFGHIJKLMNÑOÓPQRSTUÚÜVWXYZ0-9]/g;\n    // when ngram is unkown, replace it with this string listed at the dictionary\n    public UNKNOWN_NGRAM_KEY = '__';\n    // fastText doesn't contain numbers, so we use the sane embeddings for the number words\n    public NUMBERS_MAP: { [key: string]: string } = {\n        // tslint:disable:object-literal-sort-keys\n        cero: '0',\n        uno: '1',\n        dos: '2',\n        tres: '3',\n        cuatro: '4',\n        cinco: '5',\n        seis: '6',\n        siete: '7',\n        ocho: '8',\n        nueve: '9'\n        // tslint:enable:object-literal-sort-keys\n    };\n\n    public sanitizeSentence = (sentence: string): string => {\n        return sentence\n            .trim()\n            .toLowerCase()\n            .replace(this.FILTER_CHARS_REGEXP, '');\n    };\n\n    public splitSentenceToWords = (sentence: string): string[] => {\n        return this.sanitizeSentence(sentence)\n            .split(this.WORD_SEPARATORS_REGEXP)\n            .map(w => w.trim())\n            .filter(w => !!w);\n    };\n\n    public splitWordToBiGrams = (word: string): string[] => {\n        const ngram = 3;\n        const grams: string[] = [];\n        let index = word.length - ngram + 1;\n        if (index < 1) {\n            return grams;\n        }\n        while (index--) {\n            grams[index] = word.substr(index, ngram);\n        }\n        return grams;\n    };\n\n    public joinWordsToSentence = (words: string[]): string => words.join(' ');\n}\n\nexport default new SpanishTokenizer();\n","import * as tf from '@tensorflow/tfjs';\nimport * as types from '../../../types';\nimport { CombineNgramsLayer } from './CombineNgramsLayer';\nimport { PreSavedEmbeddingsInitializer } from './PreSavedEmbeddingsInitializer';\n\nexport class EmbeddingsModel {\n    public static setupModel(\n        pretrainedNGramVectors: types.PretrainedDict,\n        maxWords: number,\n        maxNgrams: number,\n        embeddingDimensions: number\n    ) {\n        const model = tf.sequential();\n        const embedLayer = tf.layers.embedding({\n            embeddingsInitializer: new PreSavedEmbeddingsInitializer({\n                embeddingDimensions,\n                pretrainedNGramVectors\n            }),\n            inputDim: pretrainedNGramVectors.size,\n            inputLength: [maxNgrams],\n            maskZero: true,\n            outputDim: embeddingDimensions,\n            trainable: false\n        });\n        model.add(tf.layers.timeDistributed({ layer: embedLayer, inputShape: [maxWords, maxNgrams] }));\n        model.add(new CombineNgramsLayer({}));\n        return model;\n    }\n\n    public tokenizer: types.IAidaTokenizer;\n\n    private ngramToIdDictionary: { [key: string]: number };\n    private maxWords: number;\n    private maxNgrams: number;\n    private embeddingDimensions: number;\n    private model: tf.Model;\n    private inputModel: tf.Model | null = null;\n\n    constructor(\n        ngramToIdDictionary: { [key: string]: number },\n        maxWords: number,\n        maxNgrams: number,\n        embeddingDimensions: number,\n        tokenizer: types.IAidaTokenizer,\n        pretrainedEmbeddingModel?: tf.Model,\n        pretrainedNGramVectors?: types.PretrainedDict\n    ) {\n        this.ngramToIdDictionary = ngramToIdDictionary;\n        this.maxWords = maxWords;\n        this.maxNgrams = maxNgrams;\n        this.embeddingDimensions = embeddingDimensions;\n        this.model = pretrainedEmbeddingModel\n            ? pretrainedEmbeddingModel\n            : EmbeddingsModel.setupModel(pretrainedNGramVectors || new Map(), this.maxWords, this.maxNgrams, this.embeddingDimensions);\n        this.tokenizer = tokenizer;\n    }\n\n    public tfModel = () => this.model;\n\n    public modelInput = () => {\n        if (!this.inputModel) {\n            const input = tf.layers.input({ shape: [this.maxWords, this.maxNgrams], dtype: 'int32' });\n            const embedded = this.model.apply(input) as tf.SymbolicTensor;\n            this.inputModel = tf.model({ inputs: input, outputs: embedded });\n        }\n        return this.inputModel;\n    };\n\n    // Embeds by word bigrams\n    public embed = (sentences: string[]) => {\n        return tf.tidy(() => {\n            const sentencesTensor = this.sentencesToWordIds(sentences);\n            const output = this.modelInput().predictOnBatch(sentencesTensor) as tf.Tensor<tf.Rank.R3>;\n            sentencesTensor.dispose();\n            return output;\n        });\n    };\n\n    public dictionary = () => this.ngramToIdDictionary;\n\n    private sentencesToWordIds = (sentences: string[]) => {\n        return tf.tidy(() => {\n            const sentencesSplittedByWords = sentences.map(s => this.tokenizer.splitSentenceToWords(s));\n            const buffer = tf.buffer([sentences.length, this.maxWords, this.maxNgrams], 'int32') as tf.TensorBuffer<tf.Rank.R3>;\n            sentencesSplittedByWords.forEach((s, sentenceIndex) => {\n                s.forEach((w: string, wordIndex: number) => {\n                    if (this.ngramToIdDictionary[w] !== undefined) {\n                        // use the word dictionary\n                        buffer.set(this.ngramToIdDictionary[w], sentenceIndex, wordIndex, 0);\n                    } else if (w.length) {\n                        this.generateWordIdsFromNGrams(w).forEach((gram, gramIndex) => {\n                            if (gramIndex > this.maxNgrams) {\n                                // tslint:disable-next-line:no-console\n                                console.warn('Word exceeding max n grams per word: ', w);\n                                return;\n                            }\n                            buffer.set(gram, sentenceIndex, wordIndex, gramIndex);\n                        });\n                    }\n                });\n            });\n            return buffer.toTensor();\n        });\n    };\n\n    private generateWordIdsFromNGrams = (word: string): number[] => {\n        let vecIds: number[] = [];\n        const addToVecsIfNotUndefined = (k: string) => {\n            if (this.ngramToIdDictionary[k] === undefined) {\n                return false;\n            }\n            vecIds.push(this.ngramToIdDictionary[k]);\n            return true;\n        };\n        // first try using ngrams to reconstruct the word vector\n        if (word.length > 2) {\n            let allNgramsFound = true;\n            const wordNgrams = this.tokenizer.splitWordToBiGrams(word);\n            wordNgrams.forEach(wt => {\n                if (!addToVecsIfNotUndefined(wt) && allNgramsFound) {\n                    allNgramsFound = false;\n                }\n            });\n            if (allNgramsFound) {\n                return vecIds;\n            }\n        }\n        // if not by ngrams use characters to construct the word vector\n        vecIds = [];\n        // TODO: use characters to construct ngrams, not the word\n        word.split('').forEach(addToVecsIfNotUndefined);\n        return vecIds;\n    };\n}\n","import * as tf from '@tensorflow/tfjs';\nimport { l2Normalize } from '@tensorflow/tfjs-layers/dist/losses';\n\n// given an imput composed of max_ngrams x 300d this layer will sum\n// and normalize all the max_ngrams to get a unique 300d vector representation\nexport class CombineNgramsLayer extends tf.layers.Layer {\n    public static className = 'CombineNgramsLayer';\n    public className = CombineNgramsLayer.className;\n    // The output shape removes the ngram dimension\n    public computeOutputShape(inputShape: number[]) {\n        return [inputShape[0], inputShape[1], inputShape[inputShape.length - 1]];\n    }\n    public call(inputs: tf.Tensor, kwargs: any) {\n        return tf.tidy(() => {\n            this.invokeCallHook(inputs, kwargs);\n            const combined = tf.sum(inputs, 2);\n            const output = l2Normalize(combined, 2);\n            combined.dispose();\n            return output;\n        });\n    }\n}\n\ntf.serialization.SerializationMap.register(CombineNgramsLayer);\n","import * as tf from '@tensorflow/tfjs';\nimport * as initializers from '@tensorflow/tfjs-layers/dist/initializers';\nimport { flatMapDeep } from 'lodash';\n\nexport interface IEmbeddingsModelConfig {\n    pretrainedNGramVectors: Map<string, Float32Array>;\n    embeddingDimensions: number;\n}\n\nexport class PreSavedEmbeddingsInitializer extends initializers.Initializer {\n    public static className = 'PreSavedEmbeddingsInitializer';\n    public config: IEmbeddingsModelConfig;\n    public className = PreSavedEmbeddingsInitializer.className;\n    constructor(config: IEmbeddingsModelConfig) {\n        super();\n        this.config = config;\n    }\n    public apply(shape: tf.Shape, dtype: tf.DataType): tf.Tensor {\n        if (!this.config || !this.config.pretrainedNGramVectors) {\n            return tf.zeros(shape, dtype);\n        }\n        return tf.tidy(() => {\n            const flatMat = flatMapDeep([...this.config.pretrainedNGramVectors.values()]);\n            return tf.tensor2d(flatMat, [this.config.pretrainedNGramVectors.size, this.config.embeddingDimensions], 'float32');\n        });\n    }\n\n    public getConfig() {\n        return this.config.pretrainedNGramVectors as any;\n    }\n}\ntf.serialization.SerializationMap.register(PreSavedEmbeddingsInitializer);\n","import * as tf from '@tensorflow/tfjs';\nimport { chunk } from 'lodash';\nimport * as types from '../../../types';\nimport { EmbeddingsModel } from '../embeddings/EmbeddingsModel';\n\nexport default class ClassificationModel extends types.PipelineModel implements types.IPipelineModel {\n    private static setup(\n        config: types.IClassificationModelParams & types.IDefaultModelParams,\n        { maxWordsPerSentence: maxWords, intents }: types.IDatasetParams\n    ) {\n        const numClasses = intents.length;\n        const LEARNING_RATE = 0.0066; // use 1e-4 as default as alternative starting point\n        const ADAM_BETA_1 = 0.0025;\n        const ADAM_BETA_2 = 0.1;\n        const optimizer = tf.train.adam(LEARNING_RATE, ADAM_BETA_1, ADAM_BETA_2);\n        // Layer 1: Convolution + max pool\n        const input = tf.input({\n            dtype: 'float32',\n            shape: [maxWords, config.embeddingDimensions],\n            name: 'embedded_words'\n        });\n        const convLayer1 = tf.layers\n            .conv1d({\n                activation: 'relu',\n                filters: config.numFilters,\n                inputShape: [maxWords, config.embeddingDimensions],\n                kernelInitializer: 'randomNormal',\n                kernelSize: [config.filterSizes[0]],\n                name: 'classConv1',\n                padding: 'valid'\n            })\n            .apply(input);\n        const maxpool1 = tf.layers\n            .maxPooling1d({\n                padding: 'valid',\n                poolSize: maxWords - config.filterSizes[0] + 1\n            })\n            .apply(convLayer1) as tf.SymbolicTensor;\n        // Layer 2: Convolution + max pool\n        const convLayer2 = tf.layers\n            .conv1d({\n                activation: 'relu',\n                filters: config.numFilters,\n                inputShape: [maxWords, config.embeddingDimensions],\n                kernelInitializer: 'randomNormal',\n                kernelSize: [config.filterSizes[1]],\n                name: 'classConv2',\n                padding: 'valid'\n            })\n            .apply(input);\n        const maxpool2 = tf.layers\n            .maxPooling1d({\n                padding: 'valid',\n                poolSize: maxWords - config.filterSizes[1] + 1\n            })\n            .apply(convLayer2) as tf.SymbolicTensor;\n        // Layer 3: Convolution + max pool\n        const convLayer3 = tf.layers\n            .conv1d({\n                activation: 'relu',\n                filters: config.numFilters,\n                inputShape: [maxWords, config.embeddingDimensions],\n                kernelInitializer: 'randomNormal',\n                kernelSize: [config.filterSizes[2]],\n                name: 'classConv3',\n                padding: 'valid'\n            })\n            .apply(input);\n        const maxpool3 = tf.layers\n            .maxPooling1d({\n                padding: 'valid',\n                poolSize: maxWords - config.filterSizes[2] + 1\n            })\n            .apply(convLayer3) as tf.SymbolicTensor;\n        // Concatenation of all CNN layers on different levels and apply a fully connected dense layer\n        const concatLayer = tf.layers.concatenate({ axis: 1 }).apply([maxpool1, maxpool2, maxpool3]);\n        const flat = tf.layers.flatten().apply(concatLayer);\n        const dropOut = tf.layers.dropout({ rate: config.drop }).apply(flat) as tf.SymbolicTensor;\n        const flatPool1 = tf.layers.flatten().apply(maxpool1) as tf.SymbolicTensor;\n        const concatForClassification = tf.layers.concatenate({ axis: 1 }).apply([dropOut, flatPool1]);\n        const outClassification = tf.layers\n            .dense({\n                activation: 'softmax',\n                units: numClasses\n            })\n            .apply(concatForClassification) as tf.SymbolicTensor;\n        const model = tf.model({ inputs: input, outputs: outClassification });\n        model.compile({\n            loss: 'categoricalCrossentropy',\n            metrics: ['accuracy'],\n            optimizer\n        });\n        return model;\n    }\n\n    private config: types.IClassificationModelParams & types.IDefaultModelParams;\n    private datasetParams: types.IDatasetParams;\n    private model: tf.Model;\n    private embeddingsModel: EmbeddingsModel;\n    private logger: types.IPipelineModelLogger;\n    private classificationTrainStatsHandler: types.ITrainStatsHandler['classification'] | undefined;\n    constructor(\n        config: types.IClassificationModelParams & types.IDefaultModelParams,\n        datasetParams: types.IDatasetParams,\n        embeddingsModel: EmbeddingsModel,\n        logger: types.IPipelineModelLogger,\n        pretrainedModel?: tf.Model,\n        classificationTrainStatsHandler?: types.ITrainStatsHandler['classification']\n    ) {\n        super();\n        this.config = config;\n        this.datasetParams = datasetParams;\n        this.embeddingsModel = embeddingsModel;\n        this.model = pretrainedModel ? pretrainedModel : ClassificationModel.setup(this.config, this.datasetParams);\n        this.logger = logger;\n        this.classificationTrainStatsHandler = classificationTrainStatsHandler;\n    }\n\n    public tfModel = () => this.model;\n\n    public predict = (sentences: string[]): types.IClassificationPred[] => {\n        const prediction = [] as types.IClassificationPred[];\n        tf.tidy(() => {\n            const embeddedSentences = this.embeddingsModel.embed(sentences);\n            const output = this.model.predict(embeddedSentences) as tf.Tensor<tf.Rank>;\n            const d = output.dataSync() as Float32Array;\n            output.dispose();\n            embeddedSentences.dispose();\n            const intents = this.datasetParams.intents;\n            sentences.forEach((s, i) => {\n                const preds = d.slice(i * intents.length, i * intents.length + intents.length);\n                const sentencePreds: types.IClassificationPred[] = [];\n                preds.forEach((p, idx) =>\n                    sentencePreds.push({\n                        confidence: p,\n                        intent: intents[idx],\n                        sentence: s\n                    })\n                );\n                sentencePreds.sort((a, b) => (a.confidence > b.confidence ? -1 : 1));\n                prediction.push(sentencePreds[0]);\n            });\n        });\n        return prediction;\n    };\n\n    public train = async (trainDataset: types.ITrainingParams): Promise<void> => {\n        const trainYChunks = chunk(trainDataset.trainY, this.config.batchSize);\n        const trainXChunks = chunk(trainDataset.trainX, this.config.batchSize);\n        this.logger.log('Start training classification model!');\n        const m = this.model;\n        let enoughAccuracyReached = false;\n        for (const [index, xChunk] of trainXChunks.entries()) {\n            if (enoughAccuracyReached) {\n                return;\n            }\n            const embeddedSentences = this.embeddingsModel.embed(xChunk);\n            const dataLabels = tf.tensor1d(trainYChunks[index], 'int32');\n            const hotEncodedLabels = tf.oneHot(dataLabels, this.datasetParams.intents.length);\n            await m.fit(embeddedSentences, hotEncodedLabels, {\n                // batchSize: this.config.batchSize,\n                callbacks: { onBatchEnd: tf.nextFrame },\n                epochs: this.config.epochs,\n                shuffle: true,\n                validationSplit: this.config.trainingValidationSplit\n            });\n            dataLabels.dispose();\n            embeddedSentences.dispose();\n            hotEncodedLabels.dispose();\n            const h = m.history.history;\n            const c = h.val_loss.length - 1;\n            if (this.classificationTrainStatsHandler) {\n                this.classificationTrainStatsHandler({\n                    batch: index + 1,\n                    batchEpochs: m.history.epoch.length,\n                    currentBatchSize: trainXChunks[index].length,\n                    tensorsInMemory: tf.memory().numTensors,\n                    totalBatches: trainXChunks.length,\n                    trainingAccuracy: h.acc[c],\n                    trainingLoss: h.loss[c],\n                    validationAccuracy: h.val_acc[c],\n                    validationLoss: h.val_loss[c]\n                });\n            }\n            this.logger.log(`Trained ${m.history.epoch.length} epochs on batch ${index + 1} of ${trainXChunks.length}`);\n            this.logger.log(`Training Loss: ${h.loss[c]} | Training Accuracy: ${h.acc[c]}`);\n            this.logger.log(`Validation Loss: ${h.val_loss[c]} | Validation Accuracy: ${h.val_acc[c]}`);\n            this.logger.warn(`(Memory) Number of tensors in memory at the end of batch: ${tf.memory().numTensors}`);\n            this.logger.log('==================================================================================================');\n            if (\n                this.config.lossThresholdToStopTraining &&\n                h.loss[c] < this.config.lossThresholdToStopTraining &&\n                h.val_loss[c] < this.config.lossThresholdToStopTraining\n            ) {\n                enoughAccuracyReached = true;\n                this.logger.warn(`Enough accuracy reached! Ending training after batch ${index + 1} of ${trainXChunks.length}`);\n                this.logger.log('==================================================================================================');\n            }\n        }\n    };\n\n    public test = async (\n        testExamples: types.ITestingParams,\n        resultsHandler?: types.ITestPredictionsHandler\n    ): Promise<types.IPredictionStats> => {\n        const handler = resultsHandler ? resultsHandler : this.defaultResultsLogger;\n        const stats: types.IPredictionStats = { correct: 0, wrong: 0, lowConfidence: 0 };\n        const x = chunk(testExamples.testX, this.config.batchSize);\n        const y = chunk(testExamples.testY, this.config.batchSize);\n        for (const [i, sentences] of x.entries()) {\n            const predictions = this.predict(sentences);\n            handler(sentences, y[i], predictions, stats);\n        }\n        return stats;\n    };\n\n    private defaultResultsLogger = (\n        x: types.ITestingParams['testX'],\n        y: types.ITestingParams['testY'],\n        o: types.IClassificationPred[],\n        stats: types.IPredictionStats\n    ): types.IPredictionStats => {\n        x.forEach((s, i) => {\n            const intent = this.datasetParams.intents[y[i]];\n            const correct = o[i].intent === intent;\n            if (o[i].confidence < this.config.lowConfidenceThreshold) {\n                if (stats.lowConfidence === undefined) {\n                    return;\n                }\n                stats.lowConfidence++;\n                this.logger.warn(`LOW CONFIDENCE (intent: ${o[i].intent}, confidence: ${o[i].confidence}) - ${s}`);\n            } else if (correct) {\n                stats.correct++;\n                this.logger.debug(`CORRECT (intent: ${o[i].intent}, confidence: ${o[i].confidence}) - ${s}`);\n            } else {\n                stats.wrong++;\n                this.logger.error(`WRONG (intent: ${o[i].intent}, confidence: ${o[i].confidence}) - ${s}`);\n            }\n        });\n        return stats;\n    };\n}\n","import * as tf from '@tensorflow/tfjs';\nimport { chunk, flatMapDeep } from 'lodash';\nimport * as types from '../../../types';\nimport { EmbeddingsModel } from '../embeddings/EmbeddingsModel';\nimport { TimeSeriesAttention } from '../TimeSeriesAttention';\n\nexport default class NerModel extends types.PipelineModel implements types.IPipelineModel {\n    private static setup(config: types.INerModelParams & types.IDefaultModelParams, datasetParams: types.IDatasetParams) {\n        const maxWords = datasetParams.maxWordsPerSentence;\n        const { addAttention, embeddingDimensions, numFilters, rnnUnits } = config;\n        const numSlotTypes = Object.keys(datasetParams.slotsToId).length;\n        const LEARNING_RATE = 0.0066; // use 1e-4 as default as alternative starting point\n        const ADAM_BETA_1 = 0.0025;\n        const ADAM_BETA_2 = 0.1;\n        const optimizer = tf.train.adam(LEARNING_RATE, ADAM_BETA_1, ADAM_BETA_2);\n        // WORD-NGRAMS LEVEL EMBEDDINGS\n        const embeddedSentencesInput = tf.input({\n            dtype: 'float32',\n            shape: [maxWords, embeddingDimensions],\n            name: 'embedded_words'\n        });\n        const convLayer1 = tf.layers\n            .conv1d({\n                activation: 'relu',\n                filters: numFilters[0],\n                inputShape: [maxWords, embeddingDimensions],\n                kernelInitializer: 'randomNormal',\n                kernelSize: 1,\n                name: 'nerConv1',\n                padding: 'valid'\n            })\n            .apply(embeddedSentencesInput) as tf.SymbolicTensor;\n        const convLayer2 = tf.layers\n            .conv1d({\n                activation: 'relu',\n                filters: numFilters[0],\n                kernelInitializer: 'randomNormal',\n                kernelSize: 1,\n                name: 'nerConv2',\n                padding: 'valid'\n            })\n            .apply(convLayer1) as tf.SymbolicTensor;\n        // CONCATENATE BOTH CNN ENCODERS (WORD AND CHAR) WITH THE INPUT AND THE CHAR CNN LAYER 1\n        const classLabelInput = tf.input({\n            dtype: 'float32',\n            shape: [datasetParams.intents.length],\n            name: 'embedded_intent'\n        });\n        const classLabelRepeated = tf.layers.repeatVector({ n: maxWords }).apply(classLabelInput) as tf.SymbolicTensor;\n        const concated = tf.layers.concatenate().apply([classLabelRepeated, embeddedSentencesInput, convLayer2]);\n        const biLstm = tf.layers\n            .bidirectional({\n                layer: tf.layers.lstm({ units: rnnUnits, returnSequences: true, name: 'bidi_encoder' }) as tf.RNN\n            })\n            .apply(concated) as tf.SymbolicTensor[];\n        let finalHidden: tf.SymbolicTensor | null = null;\n        if (addAttention) {\n            const timeAttention = new TimeSeriesAttention({ name: 'attention_weight' }).apply(biLstm[0]) as tf.SymbolicTensor;\n            finalHidden = tf.layers.concatenate().apply([timeAttention, biLstm[0], biLstm[1]]) as tf.SymbolicTensor;\n        } else {\n            finalHidden = tf.layers.concatenate().apply([biLstm[0], biLstm[1]]) as tf.SymbolicTensor;\n        }\n        const outputs = tf.layers.dense({ activation: 'softmax', units: numSlotTypes }).apply(finalHidden) as tf.SymbolicTensor;\n        const model = tf.model({ inputs: [classLabelInput, embeddedSentencesInput], outputs });\n        model.compile({ loss: 'categoricalCrossentropy', metrics: ['accuracy'], optimizer });\n        return model;\n    }\n\n    private config: types.INerModelParams & types.IDefaultModelParams;\n    private datasetParams: types.IDatasetParams;\n    private model: tf.Model;\n    private embeddingsModel: EmbeddingsModel;\n    private logger: types.IPipelineModelLogger;\n    private nerTrainStatsHandler: types.ITrainStatsHandler['ner'] | undefined;\n\n    constructor(\n        config: types.INerModelParams & types.IDefaultModelParams,\n        datasetParams: types.IDatasetParams,\n        embeddingsModel: EmbeddingsModel,\n        logger: types.IPipelineModelLogger,\n        pretrainedModel?: tf.Model,\n        nerTrainStatsHandler?: types.ITrainStatsHandler['ner']\n    ) {\n        super();\n        this.config = config;\n        this.datasetParams = datasetParams;\n        this.embeddingsModel = embeddingsModel;\n        this.model = pretrainedModel ? pretrainedModel : NerModel.setup(this.config, this.datasetParams);\n        this.logger = logger;\n        this.nerTrainStatsHandler = nerTrainStatsHandler;\n    }\n\n    public tfModel = () => this.model;\n\n    public rawPrediction = (sentences: string[], classificationPred: types.IClassificationPred[]) => {\n        return tf.tidy(() => {\n            const { maxWordsPerSentence: maxWords, slotsToId } = this.datasetParams;\n            const slotTypesLength = Object.keys(slotsToId).length;\n            const embeddedSentences = this.embeddingsModel.embed(sentences);\n            const encodedIntent = classificationPred.map(p => {\n                const intentEncoded = new Array(this.datasetParams.intents.length).fill(0) as number[];\n                const idx = this.datasetParams.intents.indexOf(p.intent);\n                if (idx !== -1) {\n                    intentEncoded[idx] = 1;\n                }\n                return intentEncoded;\n            });\n            const intentsFlat = flatMapDeep(encodedIntent);\n            const classLabel = tf.tensor2d(intentsFlat, [encodedIntent.length, this.datasetParams.intents.length]);\n            const output = this.model.predict([classLabel, embeddedSentences]) as tf.Tensor<tf.Rank>;\n            const flattenedPredictions = output.dataSync() as Float32Array;\n            output.dispose();\n            classLabel.dispose();\n            embeddedSentences.dispose();\n            // word predictions for each sentence in the form [sentence, word, slots scores]\n            const chunks = chunk(flattenedPredictions, maxWords * slotTypesLength).map(sp => chunk(sp, slotTypesLength));\n            return chunks.map(sentencePreds => {\n                return sentencePreds.map(wordTagPredictions => {\n                    let highestIndex = 0;\n                    let confidence = wordTagPredictions.length ? wordTagPredictions[highestIndex] : 0;\n                    wordTagPredictions.forEach((tp, ti) => {\n                        if (wordTagPredictions[highestIndex] < tp) {\n                            highestIndex = ti;\n                            confidence = tp;\n                        }\n                    });\n                    return { highestIndex, confidence };\n                });\n            });\n        });\n    };\n\n    public predict = (sentences: string[], classificationPred: types.IClassificationPred[]) => {\n        const { lowConfidenceThreshold } = this.config;\n        const { slotsToId } = this.datasetParams;\n        const wordPredictionsChunk = this.rawPrediction(sentences, classificationPred);\n        return sentences.map((s, i) => {\n            const sentenceWordPredictionIds = wordPredictionsChunk[i];\n            const sentenceWords = this.embeddingsModel.tokenizer.splitSentenceToWords(s);\n            return sentenceWords.reduce(\n                (accumulator: types.ISlotReducer, w: string, currentIndex) => {\n                    if (accumulator.current && accumulator.current.confidence === 0) {\n                        accumulator.current.confidence = sentenceWordPredictionIds[currentIndex].confidence;\n                    }\n                    const currentSlotKey = Object.keys(slotsToId).find(\n                        slotKey =>\n                            sentenceWordPredictionIds[currentIndex] &&\n                            slotsToId[slotKey] === sentenceWordPredictionIds[currentIndex].highestIndex\n                    );\n                    if (!currentSlotKey || !accumulator.current) {\n                        return accumulator;\n                    }\n                    if (accumulator.current.key !== currentSlotKey) {\n                        if (\n                            accumulator.current.key &&\n                            accumulator.current.key !== 'O' &&\n                            accumulator.current.confidence >= lowConfidenceThreshold\n                        ) {\n                            if (!accumulator.slots[accumulator.current.key]) {\n                                accumulator.slots[accumulator.current.key] = [];\n                            }\n                            accumulator.slots[accumulator.current.key].push({\n                                confidence: accumulator.current.confidence,\n                                value: accumulator.current.value\n                            });\n                        }\n                        accumulator.current = {\n                            confidence: sentenceWordPredictionIds[currentIndex].confidence,\n                            key: currentSlotKey,\n                            value: w\n                        };\n                    } else {\n                        // todo: add a join words handler for languages that tokenize differently\n                        accumulator.current.value += ` ${w}`;\n                        accumulator.current.confidence =\n                            (sentenceWordPredictionIds[currentIndex].confidence + accumulator.current.confidence) / 2;\n                    }\n                    if (currentIndex + 1 === sentenceWords.length) {\n                        if (accumulator.current.key !== 'O' && accumulator.current.confidence >= lowConfidenceThreshold) {\n                            if (!accumulator.slots[accumulator.current.key]) {\n                                accumulator.slots[accumulator.current.key] = [];\n                            }\n                            accumulator.slots[accumulator.current.key].push({\n                                confidence: accumulator.current.confidence,\n                                value: accumulator.current.value\n                            });\n                        }\n                        return { sentence: s, slots: accumulator.slots };\n                    }\n                    return accumulator;\n                },\n                { current: { key: '', value: '', confidence: 0 }, slots: {}, sentence: '' }\n            );\n        });\n    };\n\n    public train = async (trainDataset: types.ITrainingParams) => {\n        const trainY2Chunks = chunk(trainDataset.trainY2, this.config.batchSize);\n        const trainYChunks = chunk(trainDataset.trainY, this.config.batchSize);\n        const trainXChunks = chunk(trainDataset.trainX, this.config.batchSize);\n        const { epochs, trainingValidationSplit: validationSplit } = this.config;\n        const slotsLength = Object.keys(this.datasetParams.slotsToId).length;\n        this.logger.log('Start training NER model!');\n        let enoughAccuracyReached = false;\n        for (const [index, xChunk] of trainXChunks.entries()) {\n            if (enoughAccuracyReached) {\n                return;\n            }\n            // classification hot encoded labels as input\n            const intentLabels = tf.tidy(() =>\n                tf.oneHot(tf.tensor1d(trainYChunks[index], 'int32'), this.datasetParams.intents.length).asType('float32')\n            );\n            const embeddedSentenceWords = this.embeddingsModel.embed(xChunk);\n            // convert sentence-word-slots from the highest index format like [0,0,0,0,4,4,0,0,3,3] for a sentence\n            // to one hot encoded sentences with correct maxWords and batch sizes tensor sizes\n            const slotTags: tf.Tensor3D = tf.tidy(() => {\n                const y2sentences: tf.Tensor2D[] = [];\n                for (const wordsSlotId of trainY2Chunks[index]) {\n                    const slotIds = tf\n                        .tensor1d(wordsSlotId, 'int32')\n                        .pad([[0, this.datasetParams.maxWordsPerSentence - wordsSlotId.length]]);\n                    const ohe = tf.oneHot(slotIds, slotsLength).asType('float32');\n                    slotIds.dispose();\n                    y2sentences.push(ohe);\n                }\n                const stack = tf.stack(y2sentences) as tf.Tensor3D;\n                y2sentences.forEach(s => s.dispose());\n                return stack;\n            });\n            await this.model.fit([intentLabels, embeddedSentenceWords], slotTags, {\n                // batchSize: this.config.batchSize,\n                callbacks: { onBatchEnd: tf.nextFrame },\n                epochs,\n                shuffle: true,\n                validationSplit\n            });\n            intentLabels.dispose();\n            embeddedSentenceWords.dispose();\n            await tf.nextFrame();\n            const h = this.model.history.history;\n            const c = h.val_loss.length - 1;\n            const epoch = this.model.history.epoch;\n            if (this.nerTrainStatsHandler) {\n                this.nerTrainStatsHandler({\n                    batch: index + 1,\n                    batchEpochs: epoch.length,\n                    currentBatchSize: trainXChunks[index].length,\n                    tensorsInMemory: tf.memory().numTensors,\n                    totalBatches: trainXChunks.length,\n                    trainingAccuracy: h.acc[c],\n                    trainingLoss: h.loss[c],\n                    validationAccuracy: h.val_acc[c],\n                    validationLoss: h.val_loss[c]\n                });\n            }\n            this.logger.log(`Trained ${epoch.length} epochs on batch ${index + 1} of ${trainXChunks.length}`);\n            this.logger.log(`Training Loss: ${h.loss[c]} | Training Accuracy: ${h.acc[c]}`);\n            this.logger.log(`Validation Loss: ${h.val_loss[c]} | Validation Accuracy: ${h.val_acc[c]}`);\n            this.logger.warn(`(Memory) Number of tensors in memory at the end of batch: ${tf.memory().numTensors}`);\n            this.logger.log('==================================================================================================');\n            slotTags.dispose();\n            if (\n                this.config.lossThresholdToStopTraining &&\n                h.loss[c] < this.config.lossThresholdToStopTraining &&\n                h.val_loss[c] < this.config.lossThresholdToStopTraining\n            ) {\n                enoughAccuracyReached = true;\n                this.logger.warn(`Enough accuracy reached! Ending training after batch ${index + 1} of ${trainXChunks.length}`);\n                this.logger.log('==================================================================================================');\n            }\n        }\n    };\n\n    public test = async (\n        testExamples: types.ITestingParams,\n        resultsHandler?: types.ITestPredictionsHandler\n    ): Promise<types.IPredictionStats> => {\n        const handler = resultsHandler ? resultsHandler : this.defaultResultsLogger;\n        const stats: types.IPredictionStats = { correct: 0, wrong: 0 };\n        const batchSize = this.config.batchSize; // this.config.batchSize\n        const testX = chunk(testExamples.testX, batchSize);\n        const testY = chunk(testExamples.testY, batchSize);\n        const testY2 = chunk(testExamples.testY2, batchSize);\n        for (const [i, sentences] of testX.entries()) {\n            const classifications = testY[i];\n            const encodedIntent = sentences.map(\n                (p, idx) =>\n                    ({\n                        confidence: 1,\n                        intent: this.datasetParams.intents[classifications[idx]],\n                        sentence: p\n                    } as types.IClassificationPred)\n            );\n            const predictions = this.rawPrediction(sentences, encodedIntent).map(sentence => sentence.map(s => s.highestIndex));\n            handler(sentences, testY2[i], predictions, stats);\n            await tf.nextFrame();\n        }\n        return stats;\n    };\n\n    private defaultResultsLogger = (\n        x: types.ITestingParams['testX'],\n        y2: types.ITestingParams['testY2'],\n        o: types.ITestingParams['testY2'],\n        stats: types.IPredictionStats\n    ): types.IPredictionStats => {\n        x.forEach((s, sentenceIdx) => {\n            const expectedTags = y2[sentenceIdx];\n            const predictedTags = o[sentenceIdx];\n            let correct = true;\n            expectedTags.forEach((tag, idx) => {\n                if (predictedTags[idx] !== tag && correct) {\n                    correct = false;\n                }\n            });\n            if (correct) {\n                stats.correct++;\n                this.logger.debug(`CORRECT - ${s} expected: ${expectedTags}, predicted: ${predictedTags}`);\n            } else {\n                stats.wrong++;\n                this.logger.error(`WRONG - ${s} expected: ${expectedTags}, predicted: ${predictedTags}`);\n            }\n        });\n        return stats;\n    };\n}\n","import * as tf from '@tensorflow/tfjs';\nimport { InputSpec } from '@tensorflow/tfjs-layers/dist/engine/topology';\n\n// NOTE:\n// Attention of multi dimensional time series following the implementation\n// from the great NLP course \"Tensorflow Solutions for Text\" by Will Ballard.\n// References:\n// https://www.safaribooksonline.com/library/view/tensorflow-solutions-for/9781788399180/\nexport class TimeSeriesAttention extends tf.layers.Layer {\n    public static className = 'TimeSeriesAttention';\n    public className = TimeSeriesAttention.className;\n\n    public timed: tf.layers.Layer | null = null;\n\n    constructor(config?: any) {\n        super(config || {});\n        this.inputSpec = [new InputSpec({ ndim: 3 })];\n        this.supportsMasking = true;\n    }\n\n    public build(inputShape: tf.Shape): void {\n        const dimensions = inputShape[2];\n        const timed = tf.sequential({ name: 'per_time_step' });\n        timed.add(\n            tf.layers.dense({\n                inputShape: [dimensions],\n                kernelInitializer: 'zeros',\n                units: dimensions,\n                activation: 'softmax',\n                name: 'att_dense1'\n            })\n        );\n        timed.add(tf.layers.dense({ units: dimensions, kernelInitializer: 'glorotNormal', activation: 'tanh', name: 'att_dense2' }));\n        this.timed = tf.layers.timeDistributed({ layer: timed, name: 'att_td' });\n        this.timed.build(inputShape);\n        this.trainableWeights = this.timed.trainableWeights;\n        this.nonTrainableWeights = this.timed.nonTrainableWeights;\n        this.built = true;\n    }\n\n    public call(inputs: tf.Tensor[], kwargs: any) {\n        if (!this.built || !this.timed) {\n            throw new Error('Calling TimeSeriesAttention layer before it was built correctly.');\n        }\n        return tf.tidy(() => {\n            this.invokeCallHook(inputs, kwargs);\n            const encoded = (this.timed as tf.layers.Layer).apply(inputs) as tf.Tensor;\n            const permuted = tf.layers.permute({ dims: [2, 1] }).apply(encoded) as tf.Tensor;\n            // NOTE: this code is a workaround the absense of batch_dot in tfjs\n            // TODO: replace this with batch_dot\n            const unstackedInput = tf.unstack(inputs[0]);\n            const unstackedPermuted = tf.unstack(permuted);\n            const dotProds = unstackedInput.map((ui, i) => tf.dot(ui, unstackedPermuted[i]));\n            const selfAttend = tf.stack(dotProds);\n            const attention = tf.softmax(selfAttend);\n            const attentionPermuted = tf.layers.permute({ dims: [2, 1] }).apply(attention) as tf.Tensor;\n            const unstackedAttention = tf.unstack(attentionPermuted);\n            const unstackedOutput = unstackedAttention.map((ua, i) => tf.dot(ua, unstackedInput[i]));\n            const output = tf.stack(unstackedOutput);\n            return output;\n        });\n    }\n\n    public computeOutputShape(inputShape: tf.Shape) {\n        return inputShape;\n    }\n}\n\ntf.serialization.SerializationMap.register(TimeSeriesAttention);\n\n// WIP!\n// export function batchDot(x: tf.Tensor, y: tf.Tensor): tf.Tensor {\n//     const xNdim = x.shape.length;\n//     const yNdim = y.shape.length;\n//     const axes = [xNdim - 1, yNdim - 2];\n//     let reshapedX = x;\n//     let reshapedY = y;\n//     let diff = 0;\n//     if (xNdim > yNdim) {\n//         diff = xNdim - yNdim;\n//         reshapedY = tf.reshape(y, y.shape.concat(Array(diff).fill(1)));\n//     } else if (yNdim > xNdim) {\n//         diff = yNdim - xNdim;\n//         reshapedX = tf.reshape(x, x.shape.concat(Array(diff).fill(1)));\n//     }\n//     let out;\n//     if (xNdim === 2 && yNdim === 2) {\n//         if (axes[0] === axes[1]) {\n//             out = tf.sum(tf.mul(reshapedX, reshapedY), axes[0]);\n//         } else {\n//             out = tf.sum(tf.mul(tf.transpose(reshapedX, [1, 0]), reshapedY), axes[1]);\n//         }\n//     } else {\n//         const adjX = axes[0] !== xNdim - 1;\n//         const adjY = axes[1] === yNdim - 1;\n//         out = tf.matMul(reshapedX, reshapedY, adjX, adjY);\n//     }\n//     if (diff) {\n//         const idx = xNdim > yNdim - 3 ? xNdim + yNdim - 3 : xNdim - 1;\n//         out = tf.squeeze(out, Array.from(Array(idx + diff).keys()));\n//     }\n//     if (out.shape.length === 1) {\n//         out = tf.expandDims(out, 1);\n//     }\n//     return out;\n// }\n","import { Card, Col, Input, Row } from 'antd';\nimport * as React from 'react';\nimport styled from 'styled-components';\nimport { AidaPipeline } from '../../src/pipelines/zebraWings/pipeline';\n\ninterface IPipelineTestInputProps {\n    pipeline: AidaPipeline;\n}\ninterface IPipelineTestInputState {\n    disabled: boolean;\n    outTextContent: string | null;\n}\n// ant bug: patch input styling of search text input to avoid being hidden by the button\nconst SearchInput = styled(Input.Search)`\n    > input {\n        padding-right: 70px !important;\n    }\n`;\nexport default class TrainedPipelineTestInput extends React.Component<IPipelineTestInputProps, IPipelineTestInputState> {\n    public state: IPipelineTestInputState = {\n        disabled: false,\n        outTextContent: null\n    };\n\n    public render() {\n        return (\n            <div>\n                <Row type=\"flex\" justify=\"center\">\n                    <Col span={24} sm={{ span: 12 }}>\n                        <Card title=\"Test trained pipeline:\" style={{ minHeight: '100%' }}>\n                            <p>\n                                <SearchInput\n                                    placeholder=\"Enter some input to process...\"\n                                    enterButton=\"Send\"\n                                    size=\"large\"\n                                    onSearch={this.handleSubmit}\n                                    id=\"__inputSearch\"\n                                    disabled={this.state.disabled}\n                                />\n                            </p>\n                            {this.props.children || null}\n                        </Card>\n                    </Col>\n                    <Col span={24} sm={{ span: 12 }}>\n                        <Card title=\"Pipeline output:\" style={{ minHeight: '100%' }}>\n                            <pre style={{ marginTop: '2em' }}>{this.state.outTextContent || ''}</pre>\n                        </Card>\n                    </Col>\n                </Row>\n            </div>\n        );\n    }\n\n    private predict = (sentences: string) => {\n        const predictions = this.props.pipeline.predict([sentences]);\n        return Object.assign({}, predictions.classification[0], predictions.ner[0]);\n    };\n\n    private handleSubmit = (value: string) => {\n        if (!value || !value.trim()) {\n            return null;\n        }\n        this.setState({ disabled: true }, () => {\n            const outTextContent = JSON.stringify(this.predict(value), null, 2);\n            this.setState({ outTextContent }, () => {\n                this.setState({ disabled: false });\n                const inputSearch = document.getElementById('__inputSearch') as HTMLInputElement;\n                if (inputSearch && inputSearch.value) {\n                    inputSearch.value = '';\n                }\n            });\n        });\n    };\n}\n","import { graphql } from 'gatsby';\nimport * as React from 'react';\nimport Editor from '../components/Editor/Editor';\nimport Layout from '../components/Layout';\nimport TrainingDashboard, { ITrainingDashboardProps } from '../components/TrainingDashboard';\nimport { IEditorTabs } from '../components/Editor/editorConfig';\n\nexport default class AidaTrain extends React.Component<any, {}> {\n    public render() {\n        const { data, location } = this.props;\n        let chatitoFiles: IEditorTabs[] = [];\n        if (data && !data.allFiles && data.allFile.edges) {\n            chatitoFiles = data.allFile.edges.map((edge: any) => ({\n                title: `${edge.node.name}.${edge.node.extension}`,\n                value: edge.node.fields.chatitoDSL\n            }));\n        }\n\n        return (\n            <Layout location={location} addPadding>\n                <Editor tabs={chatitoFiles}>{(tdp: ITrainingDashboardProps) => <TrainingDashboard {...tdp} />}</Editor>\n            </Layout>\n        );\n    }\n}\n\nexport const query = graphql`\n    query ChatitoFiles {\n        allFile(filter: { extension: { eq: \"chatito\" }, relativePath: { glob: \"typescript/examples/en/intents/*.chatito\" } }) {\n            edges {\n                node {\n                    name\n                    extension\n                    dir\n                    relativePath\n                    fields {\n                        chatitoDSL\n                    }\n                }\n            }\n        }\n    }\n`;\n","import axios from 'axios';\nimport { Button, Icon, Progress } from 'antd';\nimport * as React from 'react';\nimport * as chatito from 'chatito';\nimport * as webAdapter from 'chatito/dist/adapters/web';\nimport * as utils from 'chatito/dist/utils';\nimport { withPrefix } from 'gatsby-link';\nimport { debounce } from 'lodash';\nimport { chatitoPrism, IEditorTabs } from './editorConfig';\nimport * as es from './editorStyles';\nimport { ITrainingDashboardProps } from '../TrainingDashboard';\nimport { dictionariesFromDataset } from '../../../src/utils/dictionaryUtils';\nimport englishTokenizer from '../../../src/languages/en/EnglishTokenizer';\nimport { IDatasetParams, IPretrainedDictionary, ITrainingParams, ITestingParams } from '../../../src/types';\n\ninterface IEditorProps {\n    tabs: IEditorTabs[];\n    children: (p: ITrainingDashboardProps) => any;\n}\ninterface IEditorState {\n    error: null | string;\n    warning: null | string;\n    activeTabIndex: number;\n    trainingDataset: webAdapter.IDefaultDataset;\n    testingDataset: webAdapter.IDefaultDataset;\n    showDrawer: boolean;\n    generating: boolean;\n    isDownloading: boolean;\n    downloadProgress: number;\n    datasetParams?: IDatasetParams;\n    trainingParams?: ITrainingParams;\n    testingParams?: ITestingParams;\n    embeddingsAndTrainingDatasetLoaded?: boolean;\n    ngramToIdDictionary?: IPretrainedDictionary['NGRAM_TO_ID_MAP'];\n    pretrainedNGramVectors?: IPretrainedDictionary['PRETRAINED'];\n    datasetStats?: { intent: string; training: number; testing: number }[];\n}\n\n// NOTE: for SSR, wrap the require in check for window (since it's pre rendered by gatsbyjs)\nlet CodeFlask: any = null;\nlet ReactJson: any = null;\nif (typeof window !== `undefined`) {\n    // tslint:disable-next-line:no-var-requires\n    CodeFlask = require('codeflask').default;\n    // tslint:disable-next-line:no-var-requires\n    ReactJson = require('react-json-view').default;\n}\n\nexport default class Editor extends React.Component<IEditorProps, IEditorState> {\n    public state: IEditorState = {\n        error: null,\n        warning: null,\n        activeTabIndex: 0,\n        trainingDataset: {},\n        testingDataset: {},\n        showDrawer: false,\n        generating: false,\n        isDownloading: false,\n        downloadProgress: 0\n    };\n    private tabsContainer = React.createRef() as React.RefObject<HTMLDivElement>;\n    private codeflask: any = null;\n    private editorUpdatesSetupCount = 0;\n    private codeInputValue = '';\n    private tabs: IEditorTabs[] = [];\n\n    private debouncedTabDSLValidation = debounce(() => {\n        if (!this.codeInputValue.length) {\n            if (this.state.error || this.state.warning) {\n                this.setState({ error: null, warning: null });\n            }\n            return;\n        }\n        const validation = this.getDSLValidation(this.codeInputValue);\n        let newState = {};\n        if (validation && validation.error) {\n            newState = { error: validation.error, warning: null };\n        } else if (validation && validation.warning) {\n            newState = { error: null, warning: validation.warning };\n        } else {\n            newState = { error: null, warning: null };\n        }\n        this.setState(newState, () => {\n            this.saveToLocalStorage();\n        });\n    }, 300);\n\n    public componentWillMount() {\n        this.loadFromLocalStorage();\n    }\n\n    public componentDidMount() {\n        if (typeof window === `undefined` || !CodeFlask) { return; }\n        const flask = new CodeFlask('#my-code-editor', {\n            language: 'chatito',\n            lineNumbers: true\n        });\n        flask.addLanguage('chatito', chatitoPrism);\n        if (this.tabs && this.tabs[this.state.activeTabIndex]) {\n            flask.updateCode(this.tabs[this.state.activeTabIndex].value);\n        }\n        flask.onUpdate((code: string) => {\n            if (!this.tabs || !this.tabs[this.state.activeTabIndex]) {\n                return;\n            }\n            this.codeInputValue = code;\n            this.tabs[this.state.activeTabIndex].value = code;\n            // NOTE: ugly hack to know when codeflask is mounted (it makes 2 calls to update on mount)\n            if (this.editorUpdatesSetupCount < 2) {\n                this.editorUpdatesSetupCount++;\n            } else {\n                this.setState({ trainingDataset: {}, testingDataset: {} });\n                this.debouncedTabDSLValidation();\n            }\n        });\n        flask.setLineNumber();\n        this.codeflask = flask;\n    }\n\n    public render() {\n        const s = this.state;\n        if (\n            !s.generating &&\n            !s.isDownloading &&\n            s.embeddingsAndTrainingDatasetLoaded &&\n            s.datasetParams &&\n            s.trainingParams &&\n            s.testingParams &&\n            s.datasetStats &&\n            this.props.children &&\n            s.ngramToIdDictionary &&\n            s.pretrainedNGramVectors\n        ) {\n            return this.props.children({\n                datasetParams: s.datasetParams,\n                trainDataset: s.trainingParams,\n                testDataset: s.testingParams,\n                ngramToIdDictionary: s.ngramToIdDictionary,\n                pretrainedNGramVectors: s.pretrainedNGramVectors,\n                datasetStats: s.datasetStats\n            });\n        }\n        const alertState = !!s.error ? 'error' : !!s.warning ? 'warning' : 'success';\n        const loading = s.generating ? <Icon type=\"loading\" theme=\"outlined\" /> : null;\n        return (\n            <div>\n                <h2>Train a custom assistant</h2>\n                <p>\n                    <a href=\"https://github.com/rodrigopivi/Chatito/blob/master/spec.md\" target=\"_blank\" title=\"Chatito DSL docs\">\n                        Chatito\n                    </a>\n                    &nbsp; is a language that helps create and maintain datasets. You can improve and customize the assistant accuracy and\n                    knowledge by extending intents, slots and sentences to build a cloud of possible combinations and only pull the\n                    examples needed. Click 'Generate dataset' to continue.\n                </p>\n                <es.EditorWrapper>\n                    <es.EditorHeader style={ { display: 'block', textAlign: 'right', padding: 16 } }>\n                        <Button onClick={ this.onAddFile } style={ { marginRight: 32 } } type=\"dashed\">\n                            <Icon type=\"plus\" theme=\"outlined\" />\n                            Add new file\n                        </Button>\n                        <Button type=\"primary\" onClick={ this.onToggleDrawer } disabled={ !!s.error }>\n                            <Icon type=\"play-circle\" theme=\"outlined\" />\n                            Generate dataset\n                        </Button>\n                    </es.EditorHeader>\n                    <es.EditorHeader>\n                        <es.TabsArea innerRef={ this.tabsContainer }>{ this.tabs.map(this.renderTabButton) }</es.TabsArea>\n                    </es.EditorHeader>\n                    <es.CodeStyles id=\"my-code-editor\" />\n                    <es.AlertNotification state={ alertState }> { s.error || s.warning || `Correct syntax!` }</es.AlertNotification>\n                    <es.EditorOverlay onClick={ this.onCloseDrawer } showDrawer={ s.showDrawer || s.generating }>\n                        { loading }\n                        <es.Drawer onClick={ e => e.stopPropagation() } showDrawer={ s.showDrawer }>\n                            <Icon type=\"close\" theme=\"outlined\" onClick={ this.onCloseDrawer } />\n                            { this.renderDatasetPreviewer() }\n                        </es.Drawer>\n                    </es.EditorOverlay>\n                </es.EditorWrapper>\n            </div>\n        );\n    }\n\n    /* ================== Renderers ================== */\n    private renderDatasetPreviewer = () => {\n        if (!ReactJson) {\n            return null;\n        }\n        return [\n            <div style={ { padding: '20px 20px 0 20px', textAlign: 'center' } } key=\"top_drawer\">\n                <Progress type=\"circle\" percent={ this.state.downloadProgress } style={ { marginBottom: 20, marginLeft: 30 } } />\n                <br />\n                <Button type=\"primary\" onClick={ this.trainTestAndSaveModels } disabled={ this.state.isDownloading }>\n                    <Icon type=\"play-circle\" theme=\"outlined\" />\n                    Start training!\n                </Button>\n                <es.StrokeText>\n                    Will download the embeddings dictionary (about 1mb) then train and test the models with your dataset. This process may\n                    take some time to complete depending on your hardware, please don't change the browser tab.\n                </es.StrokeText>\n            </div>,\n            <es.BlockWrapper key=\"bottom_drawer\">\n                <es.BlockWrapperTitle>Review the generated training dataset:</es.BlockWrapperTitle>\n                <ReactJson\n                    style={ { padding: 20 } }\n                    src={ this.state.trainingDataset }\n                    theme=\"chalk\"\n                    iconStyle=\"square\"\n                    enableClipboard={ false }\n                    displayDataTypes={ false }\n                    name={ false }\n                    collapsed={ 1 }\n                />\n            </es.BlockWrapper>\n        ];\n    };\n\n    private renderTabButton = (t: IEditorTabs, i: number) => {\n        const changeTab = () => this.changeTab(i);\n        const onCloseTab = this.closerTab(i);\n        return (\n            <es.TabButton active={ this.state.activeTabIndex === i } key={ `tab-${i}` } onClick={ changeTab }>\n                { t.title }\n                <Icon type=\"close\" theme=\"outlined\" onClick={ onCloseTab } />\n            </es.TabButton>\n        );\n    };\n    /* ================== Event Handlers ================== */\n    private onCloseDrawer = () => {\n        this.setState({ showDrawer: false, trainingDataset: {}, testingDataset: {} })\n    };\n\n    private onAddFile = () => {\n        let filename = 'newFile';\n        if (typeof window !== 'undefined' && window.prompt) {\n            filename = prompt('Please enter the new .chatito file name:', filename) || '';\n        }\n        if (filename) {\n            this.tabs.push({ title: `${filename}.chatito`, value: '' });\n            this.changeTab(this.tabs.length - 1, () => {\n                if (!this.tabsContainer || !this.tabsContainer.current) {\n                    return;\n                }\n                this.tabsContainer.current.scrollTo({\n                    left: this.tabsContainer.current.scrollWidth,\n                    behavior: 'smooth'\n                });\n            });\n        }\n    };\n\n    private onToggleDrawer = () => {\n        if (!this.state.showDrawer) {\n            let validChatitoFiles = false;\n            try {\n                validChatitoFiles = this.validateChatitoFiles();\n            } catch (e) {\n                return;\n            }\n            if (validChatitoFiles) {\n                this.setState({ showDrawer: false, generating: true }, () => {\n                    // NOTE: using setTimeout to render a loading state before dataset generation may block the ui\n                    setTimeout(this.generateDataset, 600);\n                });\n            } else {\n                if (typeof window !== 'undefined' && window.alert) {\n                    window.alert('Please fix the errors or warnings found in the code.');\n                }\n            }\n        }\n    };\n    /* ================== Utils ================== */\n    private saveToLocalStorage = () => {\n        if (typeof window !== `undefined` && localStorage) {\n            localStorage.setItem('tabs', JSON.stringify(this.tabs));\n        }\n    };\n    private loadFromLocalIfPresent = (key: string, parseAsJSON: boolean) => {\n        if (typeof window !== `undefined` && localStorage) {\n            try {\n                const item = localStorage.getItem(key);\n                if (!parseAsJSON) {\n                    return item;\n                }\n                if (item) {\n                    try {\n                        return JSON.parse(item);\n                    } catch (e) {\n                        // just catch the error\n                    }\n                }\n            } catch (e) {\n                // tslint:disable-next-line:no-console\n                console.error(e);\n            }\n        }\n    };\n    private loadFromLocalStorage = () => {\n        if (typeof window !== `undefined` && localStorage) {\n            const localTabs = this.loadFromLocalIfPresent('tabs', true);\n            this.tabs = localTabs ? localTabs : this.props.tabs;\n        } else {\n            this.tabs = this.props.tabs;\n        }\n    };\n    private changeTab = (i: number, cb?: () => void) => {\n        if (!this.codeflask) {\n            return;\n        }\n        this.setState({ activeTabIndex: i }, () => {\n            this.codeflask.updateCode(this.tabs[this.state.activeTabIndex].value);\n            this.codeflask.setLineNumber();\n            if (cb) {\n                setTimeout(cb, 600); // note; hack using setTimeout because codeflask uses a timeout on update code\n            }\n        });\n    };\n    private closerTab = (i: number) => {\n        return (e: React.SyntheticEvent) => {\n            if (e) {\n                e.stopPropagation();\n            }\n            if (this.tabs[i].value) {\n                if (!window.confirm(`Do you really want to remove '${this.tabs[i].title}'?`)) {\n                    return;\n                }\n            }\n            const ati = this.state.activeTabIndex;\n            let newActiveTabIndex = this.state.activeTabIndex;\n            if (ati === i && ati > 0) {\n                newActiveTabIndex = ati - 1;\n            }\n            this.tabs = [...this.tabs.slice(0, i), ...this.tabs.slice(i + 1)];\n            if (!this.tabs.length) {\n                this.tabs.push({ title: 'newFile.chatito', value: '' });\n                newActiveTabIndex = 0;\n            }\n            this.saveToLocalStorage();\n            this.changeTab(newActiveTabIndex);\n        };\n    };\n    private getDSLValidation = (dsl: string): null | { error?: string; warning?: string } => {\n        try {\n            const ast = chatito.astFromString(dsl);\n            const intentsWithoutLimit = ast.filter(entity => entity.type === 'IntentDefinition' && entity.args === null);\n            if (intentsWithoutLimit.length) {\n                return {\n                    warning: `Warning: Limit the number of generated examples for intents. E.g.: %[${\n                        intentsWithoutLimit[0].key\n                        }]('training': '100')`\n                };\n            }\n            return null;\n        } catch (e) {\n            const error =\n                e.constructor === Error\n                    ? e.toString()\n                    : `${e.name}: ${e.message} Line: ${e.location.start.line}, Column: ${e.location.start.column}`;\n            return { error };\n        }\n    };\n\n    private validateChatitoFiles = () => {\n        return !this.tabs.some((tab, i) => {\n            if (tab.value) {\n                const validation = this.getDSLValidation(tab.value);\n                if (validation !== null) {\n                    this.changeTab(i);\n                    return true;\n                }\n            }\n            return false;\n        });\n    };\n\n    private generateDataset = async () => {\n        let trainingDataset: webAdapter.IDefaultDataset = {};\n        let testingDataset: webAdapter.IDefaultDataset = {};\n        for (const [i, tab] of this.tabs.entries()) {\n            try {\n                const { training, testing } = await webAdapter.adapter(tab.value, trainingDataset);\n                trainingDataset = training;\n                utils.mergeDeep(testingDataset, testing);\n            } catch (e) {\n                this.setState({ trainingDataset: {}, testingDataset: {}, showDrawer: false, generating: false }, () => {\n                    this.changeTab(i, () =>\n                        this.setState({ error: e.message }, () => {\n                            if (typeof window !== 'undefined' && window.alert) {\n                                window.alert(`Please fix error: ${e.message}`);\n                            }\n                        })\n                    );\n                });\n                return;\n            }\n        }\n        this.setState({ trainingDataset, testingDataset, generating: false, showDrawer: true });\n    };\n\n    private downloadFiles = async (files: string[]) => {\n        let total = 0;\n        let progress = 0;\n        this.setState({ isDownloading: true, downloadProgress: 0 });\n        const downloads = await Promise.all(\n            files.map(file =>\n                axios.get(file, {\n                    onDownloadProgress: progressEvent => {\n                        const totalLength = progressEvent.lengthComputable\n                            ? progressEvent.total\n                            : progressEvent.target.getResponseHeader('content-length') ||\n                            progressEvent.target.getResponseHeader('x-decompressed-content-length');\n                        if (totalLength !== null) {\n                            total += totalLength;\n                            progress += Math.round((progressEvent.loaded * 100) / total);\n                        }\n                        this.setState({ downloadProgress: progress });\n                    }\n                })\n            )\n        );\n        this.setState({ isDownloading: false, downloadProgress: 100 });\n        return downloads;\n    };\n\n    private trainTestAndSaveModels = async () => {\n        const files = [withPrefix('/models/dictionary.json'), withPrefix('/models/ngram_to_id_dictionary.json')];\n        const jsonFiles = await this.downloadFiles(files);\n        const pretrainedNGramVectors = new Map<string, Float32Array>(jsonFiles[0].data);\n        const ngramToIdDictionary = jsonFiles[1].data;\n        const {\n            dictionary: {\n                maxWordsPerSentence,\n                slotsToId,\n                intents,\n                intentsWithSlots,\n                testX,\n                testY,\n                testY2,\n                trainX,\n                trainY,\n                trainY2,\n                language\n            },\n            stats\n        } = dictionariesFromDataset(this.state.trainingDataset, this.state.testingDataset, englishTokenizer, 'en');\n        const trainingParams: ITrainingParams = { trainX, trainY, trainY2 };\n        const testingParams: ITestingParams = { testX, testY, testY2 };\n        const datasetParams = {\n            language,\n            intents,\n            intentsWithSlots,\n            maxWordsPerSentence,\n            slotsToId\n        } as IDatasetParams;\n        // NOTE: using setTimeout to render a loading state before dataset generation may block the ui\n        setTimeout(() => {\n            this.setState({\n                trainingParams,\n                testingParams,\n                datasetParams,\n                embeddingsAndTrainingDatasetLoaded: true,\n                ngramToIdDictionary,\n                pretrainedNGramVectors,\n                datasetStats: stats\n            });\n        }, 200);\n    };\n}\n","export interface IEditorTabs {\n    title: string;\n    value: string;\n}\n\nexport const chatitoPrism = {\n    comments: [{ pattern: /^\\/\\/.*/, greedy: true }, { pattern: /((\\n|\\r\\n)+)\\/\\/.*/, greedy: true }],\n    intentDefinition: [\n        {\n            pattern: /^%\\[[^\\]]+\\]((\\(.+\\))?)/,\n            inside: { intentArguments: /((\\(.+\\))?)$/ }\n        },\n        {\n            pattern: /((\\n|\\r\\n)+)%\\[[^\\]]+\\]((\\(.+\\))?)/,\n            inside: { intentArguments: /((\\(.+\\))?)$/ }\n        }\n    ],\n    slotDefinition: [\n        {\n            pattern: /^\\@\\[[^\\]]+\\]((\\(.+\\))?)/,\n            inside: { slotArguments: /((\\(.+\\))?)$/ }\n        },\n        {\n            pattern: /((\\n|\\r\\n)+)\\@\\[[^\\]]+\\]((\\(.+\\))?)/,\n            inside: { slotArguments: /((\\(.+\\))?)$/ }\n        }\n    ],\n    slot: { pattern: /\\@\\[[^\\]]+(\\?)?\\]/, greedy: true },\n    alias: { pattern: /~\\[[^\\]]+(\\?)?\\]/, greedy: true },\n    default: { pattern: /[^\\r\\n]/i, greedy: true }\n};\n","import styled from 'styled-components';\n\nexport const AlertNotification = styled.div`\n    width: 100%;\n    background-color: ${({ state }: { state: 'error' | 'warning' | 'success' }) =>\n        state === 'error' ? '#c80000' : state === 'warning' ? '#7f8000' : '#008800'};\n    bottom: 0;\n    margin: auto;\n    right: 0;\n    text-align: center;\n    padding: 12px;\n    color: white;\n    z-index: 99;\n    font-size: 14px;\n`;\n\nconst editorOutsideBg = '#D0D2D5';\nconst editorInsideBg = '#f8f8f8';\n\nexport const CodeStyles = styled.div`\n    white-space: pre-wrap;\n    position: relative;\n    margin: auto;\n    width: inherit;\n    height: calc(100vh - 330px) !important;\n    min-height: 350px;\n    background-color: ${editorInsideBg};\n    > .codeflask {\n        background-color: ${editorInsideBg};\n        > textarea.codeflask__textarea {\n            color: ${editorInsideBg};\n            caret-color: #343434;\n        }\n        &.codeflask--has-line-numbers {\n            :before {\n                background-color: ${editorOutsideBg};\n            }\n            > pre {\n                width: auto !important;\n            }\n            div.codeflask__lines {\n                z-index: 3;\n                height: auto !important;\n                padding: 10px 4px 0 0;\n                > .codeflask__lines__line {\n                    color: #6473a0;\n                    background-color: ${editorOutsideBg};\n                }\n            }\n        }\n        *::-webkit-scrollbar {\n            width: 10px;\n            height: 10px;\n        }\n        *::-webkit-scrollbar-thumb {\n            background-color: #7c7c9c;\n            box-shadow: inset 0 0 2px rgba(0, 0, 0, 0.8);\n        }\n        *::-webkit-scrollbar-track {\n            box-shadow: inset 0 0 2px rgba(0, 0, 0, 0.8);\n        }\n        *::-webkit-scrollbar-corner {\n            background-color: transparent;\n        }\n    }\n    .token.comments {\n        color: #999;\n    }\n    .token.intentDefinition {\n        color: #ff3636;\n    }\n    .token.slotDefinition {\n        color: #d000ff;\n    }\n    .token.slot {\n        color: #c000ee;\n    }\n    .token.alias {\n        color: #3f51b5;\n    }\n    .token.default {\n        color: #343434;\n    }\n    .token.intentArguments {\n        color: #ff9191;\n    }\n    .token.slotArguments {\n        color: #da7cee;\n    }\n`;\n\nexport const TabButton = styled.div`\n    cursor: pointer;\n    display: inline-block;\n    background-color: ${({ active }: { active: boolean }) => (active ? editorInsideBg : editorOutsideBg)};\n    font-size: 12px;\n    color: #454545;\n    padding: 13px 3px 13px 13px;\n    border-right: 1px solid #d5d5d5;\n    zoom: 1;\n    -webkit-touch-callout: none;\n    -webkit-user-select: none;\n    -moz-user-select: none;\n    -ms-user-select: none;\n    user-select: none;\n\n    i.anticon-close {\n        margin-left: 8px;\n        margin-right: 8px;\n        color: #5c5c5c;\n        :hover {\n            color: #ff66ff;\n        }\n    }\n`;\n\nexport const EditorHeader = styled.div`\n    display: flex;\n    flex-direction: row;\n    width: 100%;\n    max-width: 100%;\n    background-color: ${editorOutsideBg};\n    padding-left: 40px;\n    padding-top: 10px;\n`;\n\nexport const TabsArea = styled.div`\n    width: auto;\n    max-width: 100%;\n    white-space: nowrap;\n    position: relative;\n    overflow-x: auto;\n    overflow-y: hidden;\n    -webkit-overflow-scrolling: touch;\n    &::-webkit-scrollbar {\n        height: 6px;\n    }\n    &::-webkit-scrollbar-thumb {\n        background-color: #7c7c9c;\n        -webkit-box-shadow: inset 0 0 2px rgba(0, 0, 0, 0.8);\n    }\n    &::-webkit-scrollbar-track {\n        -webkit-box-shadow: inset 0 0 2px rgba(0, 0, 0, 0.8);\n    }\n    *::-webkit-scrollbar-corner {\n        background-color: transparent;\n    }\n`;\n\nexport const EditorWrapper = styled.div`\n    width: auto;\n    overflow: auto;\n    margin: auto;\n    position: relative;\n`;\n\nexport const StrokeText = styled.p`\n    text-align: center;\n    font-size: 12px;\n    padding-top: 20px;\n    color: #fff;\n    text-shadow: -1px -1px 0 #444, 1px -1px 0 #444, -1px 1px 0 #444, 1px 1px 0 #444;\n`;\n\nexport const Drawer = styled.div`\n    z-index: 99;\n    position: absolute;\n    background-color: rgba(40, 40, 40, 0.8);\n    -webkit-box-shadow: -5px 0px 5px -5px rgba(0, 0, 0, 0.55);\n    -moz-box-shadow: -5px 0px 5px -5px rgba(0, 0, 0, 0.55);\n    box-shadow: -5px 0px 5px -5px rgba(0, 0, 0, 0.55);\n    top: 0;\n    right: 0;\n    max-width: 700px;\n    height: 100%;\n    width: ${({ showDrawer }: { showDrawer: boolean }) => (showDrawer ? `100%` : `0px`)};\n    -webkit-transition: 0.65s ease;\n    -moz-transition: 0.65s ease;\n    -o-transition: 0.65s ease;\n    transition: 0.65s ease;\n    overflow: auto;\n\n    > i.anticon-close {\n        cursor: pointer;\n        color: white;\n        margin: 8px 20px 8px 20px;\n        float: right;\n        :hover {\n            color: #ff66ff;\n        }\n    }\n    .ant-progress .ant-progress-text {\n        color: #fff;\n    }\n`;\n\nexport const EditorOverlay = styled.div`\n    z-index: 999;\n    position: absolute;\n    top: 0;\n    left: 0;\n    width: 100%;\n    height: 100%;\n    background: rgba(0, 0, 0, 0.6);\n    visibility: ${({ showDrawer }: { showDrawer: boolean }) => (showDrawer ? 'visible' : 'hidden')};\n    -webkit-transition: 0.25s ease;\n    -moz-transition: 0.25s ease;\n    -o-transition: 0.25s ease;\n    transition: 0.25s ease;\n    > i.anticon-loading {\n        position: absolute;\n        font-size: 50px;\n        left: 50%;\n        transform: translateX(-50%);\n        top: 50%;\n        transform: translateY(-50%);\n    }\n`;\n\nexport const BlockWrapper = styled.div`\n    background-color: #e4e4e4;\n    margin: 20px;\n    overflow: auto;\n    border-radius: 8px;\n    -webkit-box-shadow: 0px 0px 50px 0px rgba(0, 0, 0, 0.4);\n    -moz-box-shadow: 0px 0px 50px 0px rgba(0, 0, 0, 0.4);\n    box-shadow: 0px 0px 50px 0px rgba(0, 0, 0, 0.4);\n    clear: both;\n`;\n\nexport const BlockWrapperTitle = styled.div`\n    background-color: #6b5a86;\n    color: #efefef;\n    font-size: 13px;\n    padding: 8px 10px;\n    border-top-left-radius: 8px;\n    border-top-right-radius: 8px;\n`;\n","import { IDefaultDataset } from 'chatito/dist/adapters/web';\nimport { ISentenceTokens } from 'chatito/dist/types';\nimport { flatten, shuffle } from 'lodash';\nimport { IAidaTokenizer, IDictionariesFromDataset, IDictJsonItem, IPretrainedDictionary } from '../types';\n\n/// Build an object that contains the max words size, the train x, y and the intents\nexport function dictionariesFromDataset(\n    training: IDefaultDataset,\n    testing: IDefaultDataset,\n    tokenizer: IAidaTokenizer,\n    language: 'en' | 'es'\n) {\n    const ret: IDictionariesFromDataset = {\n        intents: Object.keys(training),\n        intentsWithSlots: [],\n        language,\n        maxWordsPerSentence: 0,\n        slotsToId: { O: 0 },\n        testX: [],\n        testY: [],\n        testY2: [],\n        trainX: [],\n        trainY: [],\n        trainY2: []\n    };\n    const intentTrainingStats: number[] = new Array(ret.intents.length).fill(0);\n    const intentTestingStats: number[] = new Array(ret.intents.length).fill(0);\n    let processedTrainingSentences: Array<{ sentence: string; intentId: number; tagsForSentence: number[] }> = [];\n    let processedTestingSentences: Array<{ sentence: string; intentId: number; tagsForSentence: number[] }> = [];\n    const intentsWithSlotsSet = new Set();\n    ret.intents.forEach(intent => {\n        let containsSlots = false;\n        const getProcessedSentence = (sentenceTokens: ISentenceTokens[]) => {\n            const y2Tags: number[][] = sentenceTokens.map(token => {\n                const words = tokenizer.splitSentenceToWords(token.value);\n                const encodedSentenceFrag: number[] = new Array(words.length).fill(0);\n                const slotName = token.slot;\n                if (slotName) {\n                    intentsWithSlotsSet.add(intent);\n                    if (!containsSlots) {\n                        containsSlots = true;\n                    }\n                    const internalKey = slotName;\n                    if (ret.slotsToId[internalKey] === undefined) {\n                        ret.slotsToId[internalKey] = Object.keys(ret.slotsToId).length;\n                    }\n                    words.forEach((w, i) => {\n                        encodedSentenceFrag[i] = ret.slotsToId[internalKey];\n                    });\n                }\n                return encodedSentenceFrag;\n            });\n            const tagsForSentence = flatten(y2Tags);\n            const sentence = sentenceTokens.map(d => d.value).join('');\n            const sentenceWords = tokenizer.splitSentenceToWords(sentence);\n            if (ret.maxWordsPerSentence < sentenceWords.length) {\n                ret.maxWordsPerSentence = sentenceWords.length;\n            }\n            const intentId = ret.intents.findIndex(k => k === intent);\n            return { sentence, intentId, tagsForSentence };\n        };\n        const intentProcessedSentences = training[intent].map(getProcessedSentence);\n        processedTrainingSentences = processedTrainingSentences.concat(intentProcessedSentences);\n        const intentTestingProcessedSentences = testing[intent].map(getProcessedSentence);\n        processedTestingSentences = processedTestingSentences.concat(intentTestingProcessedSentences);\n    });\n    ret.intentsWithSlots = [...intentsWithSlotsSet];\n    shuffle(processedTrainingSentences).forEach(s => {\n        intentTrainingStats[s.intentId]++;\n        ret.trainX.push(s.sentence);\n        ret.trainY.push(s.intentId);\n        ret.trainY2.push(s.tagsForSentence);\n    });\n    shuffle(processedTestingSentences).forEach(s => {\n        intentTestingStats[s.intentId]++;\n        ret.testX.push(s.sentence);\n        ret.testY.push(s.intentId);\n        ret.testY2.push(s.tagsForSentence);\n    });\n    const stats = ret.intents.map((intentKey, index) => ({\n        intent: intentKey,\n        testing: intentTestingStats[index],\n        training: intentTrainingStats[index]\n    }));\n    return { dictionary: ret, stats };\n}\n\nexport function buildDictionary(dictJson: IDictJsonItem[]): IPretrainedDictionary {\n    const dictionaryCache = {\n        NGRAM_TO_ID_MAP: {},\n        PRETRAINED: new Map() // the actual pretrained word to vectors map\n    } as IPretrainedDictionary;\n    dictionaryCache.PRETRAINED = new Map(dictJson);\n    [...dictionaryCache.PRETRAINED.keys()].forEach((word, id) => {\n        dictionaryCache.NGRAM_TO_ID_MAP[word] = id;\n    });\n    return dictionaryCache;\n}\n","import * as tf from '@tensorflow/tfjs';\nimport { Alert, Card, Col, Row, Steps } from 'antd';\nimport * as React from 'react';\nimport styled from 'styled-components';\nimport { AidaPipeline } from '../../src/pipelines/zebraWings/pipeline';\nimport * as types from '../../src/types';\nimport LineChart, { ILineChartDataValues } from './LineChart';\nimport TrainedPipelineTestInput from './TrainedPipelineTestInput';\n\nconst globalLog: string[] = [\n    '==================================================================================================',\n    'WARNING: Training may take several minutes depending on your hardware, your browser may be slow while training.',\n    'NOTE: Will train classification model, then NER model, then run the test dataset on both and finally download the trained models.',\n    '        The line-plotting of the training models will show at the end to avoid extra GPU/CPU load during training.',\n    '=================================================================================================='\n];\nconst LoggerFeed = styled.div`\n    white-space: pre-wrap;\n    height: 130px;\n    overflow: scroll;\n    background-color: #ededed;\n    padding: 10px 20px;\n    margin-bottom: 20px;\n    display: flex;\n    flex-direction: column-reverse;\n    border: 1px solid #ccc;\n    font-size: 12px;\n    margin-bottom: 20px;\n`;\n\nexport interface ITrainingDashboardProps {\n    datasetParams: types.IDatasetParams;\n    trainDataset: types.ITrainingParams;\n    testDataset: types.ITestingParams;\n    ngramToIdDictionary: types.IPretrainedDictionary['NGRAM_TO_ID_MAP'];\n    pretrainedNGramVectors: types.IPretrainedDictionary['PRETRAINED'];\n    datasetStats: { intent: string; training: number; testing: number }[];\n}\n\ninterface ITrainingDashboardState {\n    currentStep: number;\n    logLinesCounter: number;\n    pipelineFinishedTraining: boolean;\n    valuesClassification: ILineChartDataValues;\n    valuesNer: ILineChartDataValues;\n    plot: boolean;\n}\n\nexport default class TrainingDashboard extends React.Component<ITrainingDashboardProps, ITrainingDashboardState> {\n    public state: ITrainingDashboardState = {\n        currentStep: 0,\n        logLinesCounter: 0,\n        pipelineFinishedTraining: false,\n        plot: false, // only plot at the end of training to make training fast (avoid any gpu external use)\n        valuesClassification: [],\n        valuesNer: []\n    };\n    private pipeline: AidaPipeline | null = null;\n    private trainStatsClassification: types.IStatsHandlerArgs[] = [];\n    private trainStatsNer: types.IStatsHandlerArgs[] = [];\n\n    public componentDidMount() {\n        this.trainTestAndSaveModels();\n    }\n\n    public componentWillUnmount() {\n        this.pipeline = null;\n        // tslint:disable-next-line:no-console\n        console.log(tf.memory());\n        tf.disposeVariables();\n    }\n\n    public render() {\n        return (\n            <div>\n                <Steps size=\"small\" current={this.state.currentStep} style={{ marginBottom: '20px' }}>\n                    <Steps.Step title=\"Train classification model\" />\n                    <Steps.Step title=\"Train NER model\" />\n                    <Steps.Step title=\"Test models\" />\n                    <Steps.Step title=\"Download and try\" />\n                </Steps>\n                {this.renderPipelineManualTestInput()}\n                <h3 style={{ marginTop: '20px' }}>Training logs:</h3>\n                <LoggerFeed>{globalLog.join('\\n')}</LoggerFeed>\n                <Row type=\"flex\" justify=\"center\">\n                    <Col span={12}>\n                        {this.renderChart('Classification model stats', this.state.valuesClassification, this.trainStatsClassification)}\n                    </Col>\n                    <Col span={12}>{this.renderChart('NER model stats', this.state.valuesNer, this.trainStatsNer)}</Col>\n                </Row>\n            </div>\n        );\n    }\n\n    private renderPipelineManualTestInput = () => {\n        if (this.state.currentStep === 3 && this.pipeline) {\n            return (\n                <>\n                    <Alert\n                        message=\"Finished training! You can test sentences manually or look at the logs for stats.\"\n                        type=\"success\"\n                        style={{ marginBottom: 20 }}\n                    />\n                    <TrainedPipelineTestInput pipeline={this.pipeline}>\n                        <p>The pipeline was trained and tested on your custom dataset:</p>\n                        <ul>\n                            {this.props.datasetStats.map((s, i) => (\n                                <li key={`int-${i}`}>\n                                    <strong>{s.intent}</strong> ({s.training} training and {s.testing} testing examples).\n                                </li>\n                            ))}\n                        </ul>\n                    </TrainedPipelineTestInput>\n                </>\n            );\n        }\n        return null;\n    };\n\n    private renderChart = (title: string, values: ILineChartDataValues, stats: types.IStatsHandlerArgs[]) => {\n        if (!stats.length || !values.length) {\n            return (\n                <Card title={title} style={{ minHeight: '100%' }}>\n                    <p>Waiting...</p>\n                </Card>\n            );\n        }\n        const batchInfo = `Batch ${values.length} of ${stats[stats.length - 1].totalBatches}`;\n        const lineChart = this.state.plot ? <LineChart dataValues={values} /> : null;\n        return (\n            <Card title={title} extra={batchInfo} style={{ minHeight: '100%' }}>\n                {lineChart}\n                <p style={{ fontSize: '12px' }}>\n                    <strong>Train Accuracy:</strong> {stats[stats.length - 1].trainingAccuracy}\n                    <br />\n                    <strong>Validation Accuracy:</strong> {stats[stats.length - 1].validationAccuracy}\n                    <br />\n                    <strong>Train Loss:</strong> {stats[stats.length - 1].trainingLoss}\n                    <br />\n                    <strong>Validation Loss:</strong> {stats[stats.length - 1].validationLoss}\n                    <br />\n                </p>\n            </Card>\n        );\n    };\n\n    private trainStatsHandler = (): types.ITrainStatsHandler => ({\n        classification: stats => {\n            this.trainStatsClassification.push(stats);\n            const { batch, validationLoss, trainingLoss } = stats;\n            this.setState(s => ({\n                valuesClassification: s.valuesClassification.concat([{ batch, validationLoss, trainingLoss }])\n            }));\n        },\n        ner: stats => {\n            this.trainStatsNer.push(stats);\n            const { batch, validationLoss, trainingLoss } = stats;\n            this.setState(s => ({\n                currentStep: s.currentStep === 0 ? 1 : s.currentStep,\n                valuesNer: s.valuesNer.concat([{ batch, validationLoss, trainingLoss }])\n            }));\n        }\n    });\n\n    private trainTestAndSaveModels = async () => {\n        const logHandler = (...args: any[]) => {\n            globalLog.push(...args);\n            this.setState(prev => ({ logLinesCounter: prev.logLinesCounter + 1 }));\n        };\n        const logger: types.IPipelineModelLogger = {\n            debug: () => null,\n            error: logHandler,\n            log: logHandler,\n            warn: logHandler\n        };\n        this.pipeline = new AidaPipeline({\n            datasetParams: this.props.datasetParams,\n            logger,\n            ngramToIdDictionary: this.props.ngramToIdDictionary,\n            pretrainedNGramVectors: this.props.pretrainedNGramVectors,\n            trainStatsHandler: this.trainStatsHandler()\n        });\n        if (!this.pipeline) {\n            return null;\n        }\n        await this.pipeline.train(this.props.trainDataset);\n        this.setState({ currentStep: 2 });\n        const stats = await this.pipeline.test(this.props.testDataset);\n        this.setState({ currentStep: 3 });\n        logger.log('==================================================================================================');\n        logger.log('Test dataset stats:');\n        logger.log(JSON.stringify(stats, null, 2));\n        logger.log('==================================================================================================');\n        await this.pipeline.save({\n            classificationPath: 'downloads://classification',\n            embeddingPath: 'downloads://embedding',\n            nerPath: 'downloads://ner'\n        });\n        this.setState({ plot: true });\n    };\n}\n","import * as React from 'react';\nexport type ILineChartDataValues = Array<{ batch: number; validationLoss: any; trainingLoss: any }>;\n\n// TODO: Print a line chart\nexport default class AidaLineChart extends React.Component<{ dataValues: ILineChartDataValues }, any> {\n    public render() {\n        return <div />;\n    }\n}\n"],"sourceRoot":""}